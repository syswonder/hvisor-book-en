<!DOCTYPE HTML>
<html lang="zh" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>hvisor Manual</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="hvisor Manual">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">hvisor Manual</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/syswonder/hvisor" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div align=center>
	<img src="chap01/img/hvisor-logo.svg"/>
</div>
<h1 id="welcome-to-hvisor"><a class="header" href="#welcome-to-hvisor">Welcome to hvisor!</a></h1>
<p>Hello~</p>
<div style="break-before: page; page-break-before: always;"></div><p>Welcome to hvisor!</p>
<p><a href="https://github.com/syswonder/hvisor">hvisor</a> is a lightweight Type-1 virtual machine monitor written in Rust, offering efficient resource management and low-overhead virtualization performance.</p>
<p>Features</p>
<ol>
<li><strong>Cross-platform support</strong>: Supports multiple architectures including AARCH64, RISC-V, and LoongArch.</li>
<li><strong>Lightweight</strong>: Focuses on core virtualization features, avoiding unnecessary complexity found in traditional virtualization solutions, suitable for resource-constrained environments.</li>
<li><strong>Efficient</strong>: Runs directly on hardware without going through an OS layer, providing near-native performance.</li>
<li><strong>Security</strong>: Rust is known for its memory safety and concurrent programming model, helping to reduce common system-level programming errors such as memory leaks and data races.</li>
<li><strong>Fast startup</strong>: Designed to be simple with a short startup time, suitable for scenarios that require rapid deployment of virtualization.</li>
</ol>
<p>Main Functions</p>
<ol>
<li><strong>Virtual Machine Management</strong>: Provides basic management functions for creating, starting, stopping, and deleting virtual machines.</li>
<li><strong>Resource Allocation and Isolation</strong>: Supports efficient allocation and management of CPU, memory, and I/O devices, using virtualization technology to ensure isolation between different virtual machines, enhancing system security and stability.</li>
</ol>
<p>Use Cases</p>
<ol>
<li><strong>Edge Computing</strong>: Suitable for running on edge devices, providing virtualization support for IoT and edge computing scenarios.</li>
<li><strong>Development and Testing</strong>: Developers can quickly create and destroy virtual machine environments for software development and testing.</li>
<li><strong>Security Research</strong>: Provides an isolated environment for security research and malware analysis.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="hvisor-currently-supported-hardware-platforms"><a class="header" href="#hvisor-currently-supported-hardware-platforms">hvisor currently supported hardware platforms</a></h1>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
QEMU</li>
</ul>
<h1 id="hvisor-upcoming-hardware-platforms"><a class="header" href="#hvisor-upcoming-hardware-platforms">hvisor upcoming hardware platforms</a></h1>
<ul>
<li><input disabled="" type="checkbox"/>
OKMX8MP-C</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-hvisor-on-qemu"><a class="header" href="#running-hvisor-on-qemu">Running hvisor on QEMU</a></h1>
<h2 id="1-install-cross-compiler-aarch64-none-linux-gnu-103"><a class="header" href="#1-install-cross-compiler-aarch64-none-linux-gnu-103">1. Install Cross Compiler aarch64-none-linux-gnu-10.3</a></h2>
<p>URL: <a href="https://developer.arm.com/downloads/-/gnu-a">https://developer.arm.com/downloads/-/gnu-a</a></p>
<p>Tool selection: AArch64 GNU/Linux target (aarch64-none-linux-gnu)</p>
<p>Download link: <a href="https://developer.arm.com/-/media/Files/downloads/gnu-a/10.3-2021.07/binrel/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz?rev=1cb9c51b94f54940bdcccd791451cec3&amp;hash=B380A59EA3DC5FDC0448CA6472BF6B512706F8EC">https://developer.arm.com/-/media/Files/downloads/gnu-a/10.3-2021.07/binrel/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz?rev=1cb9c51b94f54940bdcccd791451cec3&amp;hash=B380A59EA3DC5FDC0448CA6472BF6B512706F8EC</a></p>
<pre><code class="language-bash">wget https://armkeil.blob.core.windows.net/developer/Files/downloads/gnu-a/10.3-2021.07/binrel/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz
tar xvf gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz
ls gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin/
</code></pre>
<p>After installation, remember the path, for example, at: /home/tools/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu-, this path will be used later.</p>
<h2 id="2-compile-and-install-qemu-7212"><a class="header" href="#2-compile-and-install-qemu-7212">2. Compile and Install QEMU 7.2.12</a></h2>
<pre><code># Install required dependencies for compilation
sudo apt install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \
              gawk build-essential bison flex texinfo gperf libtool patchutils bc \
              zlib1g-dev libexpat-dev pkg-config  libglib2.0-dev libpixman-1-dev libsdl2-dev \
              git tmux python3 python3-pip ninja-build
# Download source code
wget https://download.qemu.org/qemu-7.2.12.tar.xz
# Unzip
tar xvJf qemu-7.2.12.tar.xz
cd qemu-7.2.12
# Generate configuration files
./configure --enable-kvm --enable-slirp --enable-debug --target-list=aarch64-softmmu,x86_64-softmmu
# Compile
make -j$(nproc)
</code></pre>
<p>Then edit the <code>~/.bashrc</code> file, add a few lines at the end of the file:</p>
<pre><code># Please note, the parent directory of qemu-7.2.12 can be flexibly adjusted according to your actual installation location
export PATH=$PATH:/path/to/qemu-7.2.12/build
</code></pre>
<p>Afterward, you can update the system path in the current terminal by <code>source ~/.bashrc</code>, or directly restart a new terminal. At this point, you can check the qemu version:</p>
<pre><code>qemu-system-aarch64 --version   #Check version
</code></pre>
<blockquote>
<p>Note, the above dependency packages may not be complete, for example:</p>
<ul>
<li>If you encounter <code>ERROR: pkg-config binary 'pkg-config' not found</code>, you can install the <code>pkg-config</code> package;</li>
<li>If you encounter <code>ERROR: glib-2.48 gthread-2.0 is required to compile QEMU</code>, you can install the <code>libglib2.0-dev</code> package;</li>
<li>If you encounter <code>ERROR: pixman &gt;= 0.21.8 not present</code>, you can install the <code>libpixman-1-dev</code> package.</li>
</ul>
</blockquote>
<blockquote>
<p>If you encounter an error ERROR: Dependency "slirp" not found, tried pkgconfig while generating configuration files:</p>
<p>Download <a href="https://gitlab.freedesktop.org/slirp/libslirp">https://gitlab.freedesktop.org/slirp/libslirp</a> package, and install according to the readme.</p>
</blockquote>
<h2 id="3-compile-linux-kernel-54"><a class="header" href="#3-compile-linux-kernel-54">3. Compile Linux Kernel 5.4</a></h2>
<p>Before compiling the root linux image, change the CONFIG_IPV6 and CONFIG_BRIDGE config in the .config file to y, to support creating bridges and tap devices in root linux. The specific operations are as follows:</p>
<pre><code>git clone https://github.com/torvalds/linux -b v5.4 --depth=1
cd linux
git checkout v5.4
# Modify the CROSS_COMPILE path according to the path of the cross compiler installed in the first step
make ARCH=arm64 CROSS_COMPILE=/root/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu- defconfig
# Add a line in .config
CONFIG_BLK_DEV_RAM=y
# Modify two CONFIG parameters in .config
CONFIG_IPV6=y
CONFIG_BRIDGE=y
# Compile, modify the CROSS_COMPILE path according to the path of the cross compiler installed in the first step
make ARCH=arm64 CROSS_COMPILE=/root/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu- Image -j$(nproc)
</code></pre>
<blockquote>
<p>If you encounter an error while compiling linux:</p>
<pre><code>/usr/bin/ld: scripts/dtc/dtc-parser.tab.o:(.bss+0x20): multiple definition of `yylloc'; scripts/dtc/dtc-lexer.lex.o:(.bss+0x0): first defined here
</code></pre>
<p>Then modify the file <code>scripts/dtc/dtc-lexer.lex.c</code> under the linux folder, add <code>extern</code> before <code>YYLTYPE yylloc;</code>. Compile again, if you encounter an error: openssl/bio.h: No such file or directory, then execute <code>sudo apt install libssl-dev</code></p>
</blockquote>
<p>After compilation, the kernel file is located at: arch/arm64/boot/Image. Remember the entire linux folder's path, for example: home/korwylee/lgw/hypervisor/linux, we will use this path in step 7.</p>
<h2 id="4-build-file-system-based-on-ubuntu-2004-arm64-base"><a class="header" href="#4-build-file-system-based-on-ubuntu-2004-arm64-base">4. Build File System Based on Ubuntu 20.04 arm64 Base</a></h2>
<blockquote>
<p>This section can be omitted, you can directly download the ready-made disk image for use. https://blog.syswonder.org/#/2024/20240415_Virtio_devices_tutorial</p>
</blockquote>
<p>We use Ubuntu 20.04 (22.04 is also possible) to build the root file system.</p>
<p>Download: <a href="http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.5-base-arm64.tar.gz">ubuntu-base-20.04.5-base-arm64.tar.gz</a></p>
<p>Link: <a href="http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.5-base-arm64.tar.gz">http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.5-base-arm64.tar.gz</a></p>
<pre><code class="language-bash">wget http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.5-base-arm64.tar.gz

mkdir rootfs
# Create a 1G ubuntu.img, you can modify the count to change the img size
dd if=/dev/zero of=ubuntu-20.04-rootfs_ext4.img bs=1M count=1024 oflag=direct
mkfs.ext4 ubuntu-20.04-rootfs_ext4.img
# Put ubuntu.tar.gz into the ubuntu.img that has been mounted on rootfs
sudo mount -t ext4 ubuntu-20.04-rootfs_ext4.img rootfs/
sudo tar -xzf ubuntu-base-20.04.5-base-arm64.tar.gz -C rootfs/

# Let rootfs bind and get some information and hardware of the physical machine
# qemu-path is your qemu path
sudo cp qemu-path/build/qemu-system-aarch64 rootfs/usr/bin/
sudo cp /etc/resolv.conf rootfs/etc/resolv.conf
sudo mount -t proc /proc rootfs/proc
sudo mount -t sysfs /sys rootfs/sys
sudo mount -o bind /dev rootfs/dev
sudo mount -o bind /dev/pts rootfs/dev/pts

# Executing this command may report an error, please refer to the solution below
sudo chroot rootfs
sudo apt-get install git sudo vim bash-completion \
		kmod net-tools iputils-ping resolvconf ntpdate

# The following content surrounded by # is optional
###################
adduser arm64
adduser arm64 sudo
echo "kernel-5_4" &gt;/etc/hostname
echo "127.0.0.1 localhost" &gt;/etc/hosts
echo "127.0.0.1 kernel-5_4"&gt;&gt;/etc/hosts
dpkg-reconfigure resolvconf
dpkg-reconfigure tzdata
###################
exit

sudo umount rootfs/proc
sudo umount rootfs/sys
sudo umount rootfs/dev/pts
sudo umount rootfs/dev
sudo umount rootfs
</code></pre>
<p>Finally, unmount the mount to complete the root file system production.</p>
<blockquote>
<p>When executing <code>sudo chroot .</code>, if you encounter the error <code>chroot: failed to run command ‘/bin/bash’: Exec format error</code>, you can execute the command:</p>
<pre><code>sudo apt-get install qemu-user-static
sudo update-binfmts --enable qemu-aarch64
</code></pre>
</blockquote>
<h2 id="5-rust-environment-configuration"><a class="header" href="#5-rust-environment-configuration">5. Rust Environment Configuration</a></h2>
<p>Please refer to: <a href="https://course.rs/first-try/intro.html">Rust Language Bible</a></p>
<h2 id="6-compile-and-run-hvisor"><a class="header" href="#6-compile-and-run-hvisor">6. Compile and Run hvisor</a></h2>
<p>First, pull the <a href="https://github.com/KouweiLee/hvisor">hvisor code repository</a> locally, then switch to the dev branch, and in the hvisor/images/aarch64 folder, put the previously compiled root file system and Linux kernel image respectively in the virtdisk and kernel directories, and rename them as rootfs1.ext4 and Image respectively. And in the devicetree directory, execute <code>make all</code>.</p>
<p>Then, in the hvisor directory, execute:</p>
<pre><code>make ARCH=aarch64 LOG=info FEATURES=platform_qemu run
</code></pre>
<p>Afterward, you will enter the uboot startup interface, under this interface execute:</p>
<pre><code>bootm 0x40400000 - 0x40000000
</code></pre>
<p>This startup command will boot hvisor from the physical address <code>0x40400000</code>, with the device tree address at <code>0x40000000</code>. When hvisor starts, it will automatically start root linux (used for management) and enter the shell interface of root linux, root linux is zone0, taking on management tasks.</p>
<blockquote>
<p>If missing <code>dtc</code>, you can execute the command:</p>
<pre><code>sudo apt install device-tree-compiler
</code></pre>
</blockquote>
<h2 id="7-use-hvisor-tool-to-start-zone1-linux"><a class="header" href="#7-use-hvisor-tool-to-start-zone1-linux">7. Use hvisor-tool to Start zone1-linux</a></h2>
<p>First, complete the compilation of the latest version of hvisor-tool. For details, please refer to the README of <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a> (the Chinese version is the latest version, the English README may not be updated in time). For example, if you want to compile a command line tool for arm64, and the source code of the Linux image in the Hvisor environment is located at <code>~/linux</code>, you can execute</p>
<pre><code>make all ARCH=arm64 LOG=LOG_WARN KDIR=~/linux
</code></pre>
<blockquote>
<p>Please make sure that the Linux image in Hvisor is compiled from the Linux source directory specified in the options of compiling hvisor-tool.</p>
</blockquote>
<p>After compilation, copy driver/hvisor.ko, tools/hvisor, driver/ivc.ko (if this file exists) to the directory where zone1 linux is started in the image/virtdisk/rootfs1.ext4 root file system (currently /home/arm64); then put the kernel image of zone1 (if it is the same Linux as zone0, copy a copy of image/aarch64/kernel/Image), and the device tree (image/aarch64/linux2.dtb) in the same directory (/home/arm64) of rootfs1.ext4, and rename them as Image and linux2.dtb respectively.</p>
<p>Finally, copy rootfs1.ext4 in place in image/aarch64/virtdisk, rename it as rootfs2.etx4.</p>
<p>Then, zone1-linux can be started on QEMU through root linux-zone0.</p>
<blockquote>
<p>For detailed steps to start zone1-linux, refer to the README of hvisor-tool. Here is a reference (subject to hvisor-tool), where linux2.json is the configuration file for zone1-linux:</p>
</blockquote>
<pre><code># Execute in the /home/arm64 directory:
insmod hvisor.ko
./hvisor zone start linux2.json
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-qemu"><a class="header" href="#install-qemu">Install qemu</a></h1>
<p>Install QEMU 7.2.12:</p>
<pre><code>wget https://download.qemu.org/qemu-7.2.12.tar.xz
# Unzip
tar xvJf qemu-7.2.12.tar.xz
cd qemu-7.2.12
# Configure Riscv support
./configure --target-list=riscv64-softmmu,riscv64-linux-user 
make -j$(nproc)
# Add to environment variable
export PATH=$PATH:/path/to/qemu-7.2.12/build
# Test if installation is successful
qemu-system-riscv64 --version
</code></pre>
<h1 id="install-cross-compiler"><a class="header" href="#install-cross-compiler">Install Cross Compiler</a></h1>
<p>The RISC-V cross compiler needs to be obtained and compiled from riscv-gnu-toolchain.</p>
<pre><code># Install necessary tools
sudo apt-get install autoconf automake autotools-dev curl python3 python3-pip libmpc-dev libmpfr-dev libgmp-dev gawk build-essential bison flex texinfo gperf libtool patchutils bc zlib1g-dev libexpat-dev ninja-build git cmake libglib2.0-dev libslirp-dev

git clone https://github.com/riscv/riscv-gnu-toolchain
cd riscv-gnu-toolchain
git rm qemu 
git submodule update --init --recursive
# The above operations will occupy more than 5GB of disk space
# If git reports a network error, you can execute:
git config --global http.postbuffer 524288000
</code></pre>
<p>Then start compiling the toolchain:</p>
<pre><code>cd riscv-gnu-toolchain
mkdir build
cd build
../configure --prefix=/opt/riscv64
sudo make linux -j $(nproc)
# After compilation, add the toolchain to the environment variable
echo 'export PATH=/opt/riscv64/bin:$PATH' &gt;&gt; ~/.bashrc
source ~/.bashrc
</code></pre>
<p>This will get the riscv64-unknown-linux-gnu toolchain.</p>
<h1 id="compile-linux"><a class="header" href="#compile-linux">Compile Linux</a></h1>
<pre><code>git clone https://github.com/torvalds/linux -b v6.2 --depth=1
cd linux
git checkout v6.2
make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig
make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- modules -j$(nproc)
# Start compilation
make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- Image -j$(nproc)

</code></pre>
<h1 id="create-ubuntu-root-file-system"><a class="header" href="#create-ubuntu-root-file-system">Create Ubuntu Root File System</a></h1>
<pre><code>wget http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.2-base-riscv64.tar.gz
mkdir rootfs
dd if=/dev/zero of=riscv_rootfs.img bs=1M count=1024 oflag=direct
mkfs.ext4 riscv_rootfs.img
sudo mount -t ext4 riscv_rootfs.img rootfs/
sudo tar -xzf ubuntu-base-20.04.2-base-riscv64.tar.gz -C rootfs/

sudo cp /path-to-qemu/build/qemu-system-riscv64 rootfs/usr/bin/
sudo cp /etc/resolv.conf rootfs/etc/resolv.conf
sudo mount -t proc /proc rootfs/proc
sudo mount -t sysfs /sys rootfs/sys
sudo mount -o bind /dev rootfs/dev
sudo mount -o bind /dev/pts rootfs/dev/pts
sudo chroot rootfs 
# After entering chroot, install necessary packages:
apt-get update
apt-get install git sudo vim bash-completion \
    kmod net-tools iputils-ping resolvconf ntpdate
exit

sudo umount rootfs/proc
sudo umount rootfs/sys
sudo umount rootfs/dev/pts
sudo umount rootfs/dev
sudo umount rootfs
</code></pre>
<h1 id="run-hvisor"><a class="header" href="#run-hvisor">Run hvisor</a></h1>
<p>Place the prepared root file system and Linux kernel image in the specified location under the hvisor directory, and execute <code>make run ARCH=riscv64</code> in the root directory of hvisor</p>
<p>By default, PLIC is used, execute <code>make run ARCH=riscv64 IRQ=aia</code> to enable AIA specification</p>
<h1 id="possible-issues"><a class="header" href="#possible-issues">Possible Issues</a></h1>
<p>After running Linux, the display shows <code>/bin/sh: 0: can't access tty; job control turned off</code>, type <code>bash</code> in the console.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nxp-launches-jailhouse"><a class="header" href="#nxp-launches-jailhouse">NXP Launches Jailhouse</a></h1>
<p>Date: 2024/2/25
Update Date: 2024/3/13</p>
<p>Authors: Yang Junyi, Chen Xingyu</p>
<p>Overall Approach:</p>
<ol>
<li>Boot the first Linux using an SD card, it is recommended to use Ubuntu's rootfs for this Linux and ensure it is network-enabled for easy package installation.</li>
<li>Boot the root Linux and compile the Linux kernel and Jailhouse.</li>
<li>Restart, modify the root dtb, and boot root Linux.</li>
<li>Jailhouse boots nonroot Linux, which is the Linux on the eMMC (original manufacturer's Linux), specifying rootfs as eMMC.</li>
</ol>
<h2 id="1-creating-an-ubuntu-sd-card-image"><a class="header" href="#1-creating-an-ubuntu-sd-card-image">1. Creating an Ubuntu SD Card Image</a></h2>
<pre><code class="language-shell">wget https://cdimage.ubuntu.com/ubuntu-base/releases/18.04/release/ubuntu-base-18.04.5-base-arm64.tar.gz
tar zxvf ubuntu-base-18.04.5-base-arm64.tar.gz

cd ubuntu-base-18.04.5-base-arm64

# chroot in x86
sudo apt-get install qemu
sudo cp /usr/bin/qemu-aarch64-static usr/bin/

sudo mount /sys ./sys -o bind
sudo mount /proc ./proc -o bind
sudo mount /dev ./dev -o bind

sudo mv etc/resolv.conf etc/resolv.conf.saved
sudo cp /etc/resolv.conf etc

sudo LC_ALL=C chroot . /bin/bash

# chroot in arm
sudo arch-chroot .

sudo apt-get update 
# Install necessary packages, such as vim, build-essential, python3, python3-dev, gcc, g++, git, make, kmod.
sudo apt-get install &lt;PKG_NAME&gt; 

exit

# If using arch-chroot, no need to manually umount
sudo umount ./sys
sudo umount ./proc
sudo umount ./dev

mv etc/resolv.conf.saved etc/resolv.conf

## Additionally, copy Linux and jailhouse to the SD card, change to local path here.
sudo cp -r LINUX_DEMO ubuntu-base-18.04.5-base-arm64/home # Source path see Linux kernel compilation section
sudo cp -r Jailhouse_DEMO ubuntu-base-18.04.5-base-arm64/home
# Then copy the ubuntu-base-18.04.5-base-arm64 directory to the SD card as rootfs.
# It is recommended to complete the "Compilation" section before copying, or you can compile after entering the system
sudo fdisk -l # Determine the SD card device name
sudo mount /dev/sdb1 /mnt 
sudo cp -r ubuntu-base-18.04.5-base-arm64 /mnt
</code></pre>
<h2 id="2-compile-nxp-linux-kernel"><a class="header" href="#2-compile-nxp-linux-kernel">2. Compile NXP Linux Kernel</a></h2>
<p>The source code can be obtained from the manufacturer's materials (source location: /OKMX8MP-C_Linux5.4.70+Qt5.15.0_User Data_R5 (update date: 20231012)/Linux/Source/OK8MP-linux-sdk/OK8MP-linux-kernel)</p>
<h3 id="adding-root-device-tree"><a class="header" href="#adding-root-device-tree">Adding root device tree</a></h3>
<p>Device tree storage location is arch/arm64/boot/dts/freescale, add new device tree OK8MP-C-root.dts, mainly modify to disable usdhc3 (eMMC) and uart4, and share pins between usdhc3 and usdhc2 to facilitate booting non-root-linux</p>
<p>Content:</p>
<pre><code class="language-C">// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
/*
 * Copyright 2019 NXP
 */

/dts-v1/;

#include "OK8MP-C.dts"

/ {
        interrupt-parent = &lt;&amp;gic&gt;;

        resmem: reserved-memory {
                #address-cells = &lt;2&gt;;
                #size-cells = &lt;2&gt;;
                ranges;
        };
};

&amp;cpu_pd_wait {
        /delete-property/ compatible;
};

&amp;clk {
        init-on-array = &lt;IMX8MP_CLK_USDHC3_ROOT
                         IMX8MP_CLK_NAND_USDHC_BUS
                         IMX8MP_CLK_HSIO_ROOT
                         IMX8MP_CLK_UART4_ROOT
                         IMX8MP_CLK_OCOTP_ROOT&gt;;
};

&amp;{/busfreq} {
        status = "disabled";
};

&amp;{/reserved-memory} { // Reserved jailhouse memory area
        jh_reserved: jh@fdc00000 {
                no-map;
                reg = &lt;0 0xfdc00000 0x0 0x400000&gt;;
        };

        loader_reserved: loader@fdb00000 {
                no-map;
                reg = &lt;0 0xfdb00000 0x0 0x00100000&gt;;
        };

        ivshmem_reserved: ivshmem@fda00000 {
                no-map;
                reg = &lt;0 0xfda00000 0x0 0x00100000&gt;;
        };

        ivshmem2_reserved: ivshmem2@fd900000 {
                no-map;
                reg = &lt;0 0xfd900000 0x0 0x00100000&gt;;
        };

        pci_reserved: pci@fd700000 {
                no-map;
                reg = &lt;0 0xfd700000 0x0 0x00200000&gt;;
        };

        inmate_reserved: inmate@60000000 {
                no-map;
                reg = &lt;0 0x60000000 0x0 0x10000000&gt;;
        };
};

&amp;iomuxc {
        pinctrl_uart4: uart4grp {
                fsl,pins = &lt;
                        MX8MP_IOMUXC_UART4_RXD__UART4_DCE_RX    0x49
                        MX8MP_IOMUXC_UART4_TXD__UART4_DCE_TX    0x49
                &gt;;
        };
};

&amp;usdhc3 { // eMMC: mmc2, since this eMMC is nonroot, root should not occupy it, so disable it
        status = "disabled";
};

&amp;uart4 { // This is also disabled, used for nonroot boot.
        /delete-property/ dmas;
        /delete-property/ dma-names;
        pinctrl-names = "default";
        pinctrl-0 = &lt;&amp;pinctrl_uart4&gt;;
        status = "disabled";
};

&amp;uart2 { // uart1=ttymxc0 uart4=ttymxc3 default for ttymxc1.
        /* uart4 is used by the 2nd OS, so configure pin and clk */
        pinctrl-0 = &lt;&amp;pinctrl_uart2&gt;, &lt;&amp;pinctrl_uart4&gt;;
        assigned-clocks = &lt;&amp;clk IMX8MP_CLK_UART4&gt;;
        assigned-clock-parents = &lt;&amp;clk IMX8MP_CLK_24M&gt;;
};

&amp;usdhc2 {
        pinctrl-0 = &lt;&amp;pinctrl_usdhc3&gt;, &lt;&amp;pinctrl_usdhc2&gt;, &lt;&amp;pinctrl_usdhc2_gpio&gt;;
        pinctrl-1 = &lt;&amp;pinctrl_usdhc3&gt;, &lt;&amp;pinctrl_usdhc2_100mhz&gt;, &lt;&amp;pinctrl_usdhc2_gpio&gt;;
        pinctrl-2 = &lt;&amp;pinctrl_usdhc3&gt;, &lt;&amp;pinctrl_usdhc2_200mhz&gt;, &lt;&amp;pinctrl_usdhc2_gpio&gt;;
};
</code></pre>
<h3 id="kernel-compilation"><a class="header" href="#kernel-compilation">Kernel Compilation</a></h3>
<pre><code class="language-shell"># First, refer to the previous chroot and enter the source directory
make OK8MP-C_defconfig # Configure default config
make -j$(nproc) ARCH=arm64 # Compilation takes about 15 minutes
</code></pre>
<p>If the gcc version is high, you may encounter yylloc issues, which can be resolved by lowering the version or by adding extern in front of yylloc in scripts/dtc under dtc-lexer.lex.c_shipped</p>
<p>If there are definition conflicts between jailhouse and the kernel, prioritize the kernel and modify jailhouse accordingly</p>
<h3 id="compile-jailhouse"><a class="header" href="#compile-jailhouse">Compile jailhouse</a></h3>
<p>Use jailhouse version v0.12 and manually add dts and configuration files</p>
<pre><code class="language-shell">git checkout v0.12
</code></pre>
<p>.c file addition location configs/arm64</p>
<p>.dts file addition location configs/arm64/dts</p>
<p>imx8mp.c</p>
<pre><code class="language-C">/*
 * i.MX8MM Target
 *
 * Copyright 2018 NXP
 *
 * Authors:
 *  Peng Fan &lt;peng.fan@nxp.com&gt;
 *
 * This work is licensed under the terms of the GNU GPL, version 2.  See
 * the COPYING file in the top-level directory.
 *
 * Reservation via device tree: reg = &lt;0x0 0xffaf0000 0x0 0x510000&gt;
 */

#include &lt;jailhouse/types.h&gt;
#include &lt;jailhouse/cell-config.h&gt;

struct {
        struct jailhouse_system header;
        __u64 cpus[1];
        struct jailhouse_memory mem_regions[15];
        struct jailhouse_irqchip irqchips[3];
        struct jailhouse_pci_device pci_devices[2];
} __attribute__((packed)) config = {
        .header = {
                .signature = JAILHOUSE_SYSTEM_SIGNATURE,
                .revision = JAILHOUSE_CONFIG_REVISION,
                .flags = JAILHOUSE_SYS_VIRTUAL_DEBUG_CONSOLE,
                .hypervisor_memory = {
                        .phys_start = 0xfdc00000,
                        .size =       0x00400000,
                },
                .debug_console = {
                        .address = 0x30890000,
                        .size = 0x1000,
                        .flags = JAILHOUSE_CON_TYPE_IMX |
                                 JAILHOUSE_CON_ACCESS_MMIO |
                                 JAILHOUSE_CON_REGDIST_4,
                        .type = JAILHOUSE_CON_TYPE_IMX,
                },
                .platform_info = {
                        .pci_mmconfig_base = 0xfd700000,
                        .pci_mmconfig_end_bus = 0,
                        .pci_is_virtual = 1,
                        .pci_domain = 0,

                        .arm = {
                                .gic_version = 3,
                                .gicd_base = 0x38800000,
                                .gicr_base = 0x38880000,
                                .maintenance_irq = 25,
                        },
                },
                .root_cell = {
                        .name = "imx8mp",

                        .num_pci_devices = ARRAY_SIZE(config.pci_devices),
                        .cpu_set_size = sizeof(config.cpus),
                        .num_memory_regions = ARRAY_SIZE(config.mem_regions),
                        .num_irqchips = ARRAY_SIZE(config.irqchips),
                        /* gpt5/4/3/2 not used by root cell */
                        .vpci_irq_base = 51, /* Not include 32 base */
                },
        },

        .cpus = {
                0xf,
        },

        .mem_regions = {
                /* IVHSMEM shared memory region for 00:00.0 (demo )*/ {
                        .phys_start = 0xfd900000,
                        .virt_start = 0xfd900000,
                        .size = 0x1000,
                        .flags = JAILHOUSE_MEM_READ,
                },
                {
                        .phys_start = 0xfd901000,
                        .virt_start = 0xfd901000,
                        .size = 0x9000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE ,
                },
                {
                        .phys_start = 0xfd90a000,
                        .virt_start = 0xfd90a000,
                        .size = 0x2000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE ,
                },
                {
                        .phys_start = 0xfd90c000,
                        .virt_start = 0xfd90c000,
                        .size = 0x2000,
                        .flags = JAILHOUSE_MEM_READ,
                },
                {
                        .phys_start = 0xfd90e000,
                        .virt_start = 0xfd90e000,
                        .size = 0x2000,
                        .flags = JAILHOUSE_MEM_READ,
                },
                /* IVSHMEM shared memory regions for 00:01.0 (networking) */
                JAILHOUSE_SHMEM_NET_REGIONS(0xfda00000, 0),
                /* IO */ {
                        .phys_start = 0x00000000,
                        .virt_start = 0x00000000,
                        .size =       0x40000000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE |
                                JAILHOUSE_MEM_IO,
                },
                /* RAM 00*/ {
                        .phys_start = 0x40000000,
                        .virt_start = 0x40000000,
                        .size = 0x80000000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE |
                                JAILHOUSE_MEM_EXECUTE,
                },
                /* Inmate memory */{
                        .phys_start = 0x60000000,
                        .virt_start = 0x60000000,
                        .size = 0x10000000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE |
                                JAILHOUSE_MEM_EXECUTE | JAILHOUSE_MEM_DMA,
                },
                /* Loader */{
                        .phys_start = 0xfdb00000,
                        .virt_start = 0xfdb00000,
                        .size = 0x100000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE |
                                JAILHOUSE_MEM_EXECUTE,
                },
                /* OP-TEE reserved memory?? */{
                        .phys_start = 0xfe000000,
                        .virt_start = 0xfe000000,
                        .size = 0x2000000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE,
                },
                /* RAM04 */{
                        .phys_start = 0x100000000,
                        .virt_start = 0x100000000,
                        .size = 0xC0000000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE,
                },
        },

        .irqchips = {
                /* GIC */ {
                        .address = 0x38800000,
                        .pin_base = 32,
                        .pin_bitmap = {
                                0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff,
                        },
                },
                /* GIC */ {
                        .address = 0x38800000,
                        .pin_base = 160,
                        .pin_bitmap = {
                                0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff,
                        },
                },
                /* GIC */ {
                        .address = 0x38800000,
                        .pin_base = 288,
                        .pin_bitmap = {
                                0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff,
                        },
                },
        },

        .pci_devices = {
                { /* IVSHMEM 0000:00:00.0 (demo) */
                        .type = JAILHOUSE_PCI_TYPE_IVSHMEM,
                        .domain = 0,
                        .bdf = 0 &lt;&lt; 3,
                        .bar_mask = JAILHOUSE_IVSHMEM_BAR_MASK_INTX,
                        .shmem_regions_start = 0,
                        .shmem_dev_id = 0,
                        .shmem_peers = 3,
                        .shmem_protocol = JAILHOUSE_SHMEM_PROTO_UNDEFINED,
                },
                { /* IVSHMEM 0000:00:01.0 (networking) */
                        .type = JAILHOUSE_PCI_TYPE_IVSHMEM,
                        .domain = 0,
                        .bdf = 1 &lt;&lt; 3,
                        .bar_mask = JAILHOUSE_IVSHMEM_BAR_MASK_INTX,
                        .shmem_regions_start = 5,
                        .shmem_dev_id = 0,
                        .shmem_peers = 2,
                        .shmem_protocol = JAILHOUSE_SHMEM_PROTO_VETH,
                },
        },
};
</code></pre>
<p>imx8mp-linux-demo.c</p>
<pre><code class="language-C">/*
 * iMX8MM target - linux-demo
 *
 * Copyright 2019 NXP
 *
 * Authors:
 *  Peng Fan &lt;peng.fan@nxp.com&gt;
 *
 * This work is licensed under the terms of the GNU GPL, version 2.  See
 * the COPYING file in the top-level directory.
 */

/*
 * Boot 2nd Linux cmdline:
 * export PATH=$PATH:/usr/share/jailhouse/tools/
 * jailhouse cell linux imx8mp-linux-demo.cell Image -d imx8mp-evk-inmate.dtb -c "clk_ignore_unused console=ttymxc3,115200 earlycon=ec_imx6q,0x30890000,115200  root=/dev/mmcblk2p2 rootwait rw"
 */
#include &lt;jailhouse/types.h&gt;
#include &lt;jailhouse/cell-config.h&gt;

struct {
        struct jailhouse_cell_desc cell;
        __u64 cpus[1];
        struct jailhouse_memory mem_regions[15];
        struct jailhouse_irqchip irqchips[2];
        struct jailhouse_pci_device pci_devices[2];
} __attribute__((packed)) config = {
        .cell = {
                .signature = JAILHOUSE_CELL_DESC_SIGNATURE,
                .revision = JAILHOUSE_CONFIG_REVISION,
                .name = "linux-inmate-demo",
                .flags = JAILHOUSE_CELL_PASSIVE_COMMREG,

                .cpu_set_size = sizeof(config.cpus),
                .num_memory_regions = ARRAY_SIZE(config.mem_regions),
                .num_irqchips = ARRAY_SIZE(config.irqchips),
                .num_pci_devices = ARRAY_SIZE(config.pci_devices),
                .vpci_irq_base = 154, /* Not include 32 base */
        },

        .cpus = {
                0xc,
        },

        .mem_regions = {
                /* IVHSMEM shared memory region for 00:00.0 (demo )*/ {
                        .phys_start = 0xfd900000,
                        .virt_start = 0xfd900000,
                        .size = 0x1000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_ROOTSHARED,
                },
                {
                        .phys_start = 0xfd901000,
                        .virt_start = 0xfd901000,
                        .size = 0x9000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE |
                                JAILHOUSE_MEM_ROOTSHARED,
                },
                {
                        .phys_start = 0xfd90a000,
                        .virt_start = 0xfd90a000,
                        .size = 0x2000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_ROOTSHARED,
                },
                {
                        .phys_start = 0xfd90c000,
                        .virt_start = 0xfd90c000,
                        .size = 0x2000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_ROOTSHARED,
                },
                {
                        .phys_start = 0xfd90e000,
                        .virt_start = 0xfd90e000,
                        .size = 0x2000,
                        .flags = JAILHOUSE_MEM_READ | JAILHOUSE_MEM_WRITE |
                                JAILHOUSE_MEM_ROOTSHARED,
                },
                /* IVSHMEM shared memory regions for 00:01.0 (networking) */
                JAILHOUSE_SH</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fpga-zcu102"><a class="header" href="#fpga-zcu102">FPGA zcu102</a></h1>
<p>Author: 杨竣轶 (Jerry) github.com/comet959</p>
<pre><code class="language-shell"># Before, Install vivado 2022.2 software
# Ubuntu 20.04 can work fine
sudo apt update

git clone https://github.com/U-interrupt/uintr-rocket-chip.git
cd uintr-rocket-chip
git submodule update --init --recursive
export RISCV=/opt/riscv64
git checkout 98e9e41
vim digilent-vivado-script/config.ini # Env Config

make checkout
make clean
make build

# Use vivado to open the vivado project, then change the top file, run synthesis, run implementation, generate bitstream.
# Connect the zcu102 - Jtag and Uart on your PC.
# Use dd command to flash the image include boot and rootfs part.
# Change the boot button mode to (On Off Off Off)
# Boot the power.

sudo screen /dev/ttyUSB0 115200 # Aarch64 Core Uart
sudo screen /dev/ttyUSB2 115200 # Riscv Core Uart

# On /dev/ttyUSB0
cd uintr-rocket-chip
./load-and-reset.sh

# Focus on ttyUSB2, then you will see the Riscv Linux Boot Msg.

</code></pre>
<h2 id="enable-h-extension-in-rocketchip"><a class="header" href="#enable-h-extension-in-rocketchip">Enable H extension in RocketChip</a></h2>
<pre><code class="language-shell">vim path/to/repo/common/src/main/scala/Configs.scala
</code></pre>
<pre><code class="language-scala">// change
class UintrConfig extends Config(
  new WithNBigCores(4) ++
    new WithNExtTopInterrupts(6) ++
    new WithTimebase((BigInt(10000000))) ++ // 10 MHz
    new WithDTS("freechips.rocketchip-unknown", Nil) ++
    new WithUIPI ++
    new WithCustomBootROM(0x10000, "../common/boot/bootrom/bootrom.img") ++
    new WithDefaultMemPort ++
    new WithDefaultMMIOPort ++
    new WithDefaultSlavePort ++
    new WithoutTLMonitors ++
    new WithCoherentBusTopology ++
    new BaseSubsystemConfig
)

// to

class UintrConfig extends Config(
  new WithHypervisor ++
  new WithNBigCores(4) ++
    new WithNExtTopInterrupts(6) ++
    new WithTimebase((BigInt(10000000))) ++ // 10 MHz
    new WithDTS("freechips.rocketchip-unknown", Nil) ++
    new WithUIPI ++
    new WithCustomBootROM(0x10000, "../common/boot/bootrom/bootrom.img") ++
    new WithDefaultMemPort ++
    new WithDefaultMMIOPort ++
    new WithDefaultSlavePort ++
    new WithoutTLMonitors ++
    new WithCoherentBusTopology ++
    new BaseSubsystemConfig
)

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="starting-hvisor-on-loongson-3a5000-motherboard-7a2000"><a class="header" href="#starting-hvisor-on-loongson-3a5000-motherboard-7a2000">Starting hvisor on Loongson 3A5000 motherboard (7A2000)</a></h1>
<p>Han Yulu <a href="mailto:chap02/enkerewpo@hotmail.com">enkerewpo@hotmail.com</a></p>
<p>Updated: 2024.12.4</p>
<h2 id="step-1-obtain-hvisor-source-code-and-compile"><a class="header" href="#step-1-obtain-hvisor-source-code-and-compile">Step 1: Obtain hvisor source code and compile</a></h2>
<p>Clone the code locally:</p>
<pre><code class="language-bash">git clone -b dev-loongarch https://github.com/syswonder/hvisor # dev-loongarch branch
make ARCH=loongarch64
</code></pre>
<p>After compiling, you can find the stripped hvisor.bin in the target directory (the file path will be displayed in the last line of the compilation output).</p>
<h2 id="obtain-vmlinuxbin-image"><a class="header" href="#obtain-vmlinuxbin-image">Obtain vmlinux.bin image</a></h2>
<p>Please download the latest released hvisor default Loongson Linux image from <a href="https://github.com/enkerewpo/linux-hvisor-loongarch64/releases">https://github.com/enkerewpo/linux-hvisor-loongarch64/releases</a> (including root linux kernel + root linux dtb + root linux rootfs, where root linux rootfs includes non-root linux + nonroot linux dtb + nonroot linux rootfs). If you need to compile the Linux kernel and rootfs yourself, refer to the <code>arch/loongarch</code> directory in the repository for hvisor-related device trees and the buildroot environment I ported for 3A5000 (<a href="https://github.com/enkerewpo/buildroot-loongarch64">https://github.com/enkerewpo/buildroot-loongarch64</a>). If you need to manually compile hvisor-tool, please refer to <a href="https://github.com/enkerewpo/hvisor-tool">https://github.com/enkerewpo/hvisor-tool</a>, for the compilation order and script invocation process of all environments, refer to the code within the <code>world</code> target in the <code>Makefile.1</code> file (<a href="https://github.com/enkerewpo/hvisor_uefi_packer/blob/main/Makefile.1">https://github.com/enkerewpo/hvisor_uefi_packer/blob/main/Makefile.1</a>), and compile everything by running the <code>./make_world</code> script. If you need to manually compile these, you need to modify the corresponding code path variables in Makefile.1, including:</p>
<pre><code>HVISOR_LA64_LINUX_DIR = ../hvisor-la64-linux
BUILDROOT_DIR = ../buildroot-loongarch64
HVISOR_TOOL_DIR = ../hvisor-tool
</code></pre>
<p>Then run <code>./make_world</code>, please note that the first time compiling Linux and buildroot may take quite a long time (possibly up to several tens of minutes, depending on your machine performance).</p>
<h2 id="obtain-hvisor-uefi-image-packer"><a class="header" href="#obtain-hvisor-uefi-image-packer">Obtain hvisor UEFI Image Packer</a></h2>
<p>Since the 3A5000 and subsequent 3 series CPUs' motherboards all use UEFI boot, hvisor can only be started through the efi image method, clone the repository <a href="https://github.com/enkerewpo/hvisor_uefi_packer">https://github.com/enkerewpo/hvisor_uefi_packer</a> locally:</p>
<pre><code class="language-bash">make menuconfig # Configure for your local loongarch64 gcc toolchain prefix, hvisor.bin path, vmlinux.bin path
# Modify make_image in HVISOR_SRC_DIR=../hvisor to your actual saved hvisor source code path, then run the script
./make_image
# Get BOOTLOONGARCH64.EFI file
</code></pre>
<p>The obtained <code>BOOTLOONGARCH64.EFI</code> must be placed in the <code>/EFI/BOOT/BOOTLOONGARCH64.EFI</code> position of the first FAT32 partition of the USB drive. Then insert the USB drive to boot into hvisor and automatically start root linux.</p>
<p>Since the metadata related to root linux (loading address, memory area, etc.) is hard-coded in the hvisor source code (<code>src/platform/ls3a5000_loongarch64.rs</code>), if you are manually compiling the Linux kernel, you need to modify the configuration here and recompile hvisor.</p>
<h2 id="board-boot"><a class="header" href="#board-boot">Board boot</a></h2>
<p>Power on the motherboard, press <strong>F12</strong> to enter the UEFI Boot Menu, select the inserted USB drive and press Enter, hvisor will automatically start and enter the root linux bash environment.</p>
<h2 id="start-nonroot"><a class="header" href="#start-nonroot">Start nonroot</a></h2>
<p>If you are using the related images provided in the release, after starting, enter in the root linux bash:</p>
<pre><code class="language-bash">./daemon.sh
./linux2_virtio.sh
</code></pre>
<p>Afterward, nonroot will automatically start (some related configuration files are located in the root linux <code>/tool</code> directory, including the nonroot zone configuration json and virtio configuration json files provided to hvisor-tool), then a screen process connected to nonroot linux's virtio-console will automatically open, and you will see a bash printed with the nonroot label appear, you can use the CTRL+A D shortcut key to detach during screen (please remember the displayed screen session name), at this point you will return to root linux, if you want to return to nonroot linux, run</p>
<pre><code class="language-bash">screen -r {the full name of the session just now or just enter the first few digits}
</code></pre>
<p>Afterward, you will return to the nonroot linux bash.</p>
<div style="break-before: page; page-break-before: always;"></div><p>This catalog is mainly related to ZCU102, and the introduction is as follows:</p>
<ol>
<li>How to use Qemu to simulate Xilinx ZynqMP ZCU102</li>
<li>How to boot hvisor root linux and nonroot linux on Qemu ZCU102 and ZCU102 physical development board.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="qemu-zcu102-hvisor-boot"><a class="header" href="#qemu-zcu102-hvisor-boot">Qemu ZCU102 hvisor Boot</a></h1>
<h2 id="install-petalinux"><a class="header" href="#install-petalinux">Install Petalinux</a></h2>
<ol>
<li>Install <a href="https://china.xilinx.com/support/download/index.html/content/xilinx/zh/downloadNav/embedded-design-tools/2024-1.html">Petalinux 2024.1</a>
Please note that this article uses 2024.1 as an example, which does not mean that other versions are not possible, but other versions have not been verified, and it has been found that Petalinux has a strong dependency on the operating system. Please install the version of Petalinux suitable for your operating system.</li>
<li>Place the downloaded <code>petalinux.run</code> file in the directory where you want to install it, add execution permissions to it, and then directly run the installer with <code>./petalinux.run</code>.</li>
<li>The installer will automatically detect the required environment, and if it does not meet the requirements, it will prompt for the missing environment, which can be installed one by one with <code>apt install</code>.</li>
<li>After installation, before using Petalinux each time, you need to enter the installation directory and manually <code>source settings.sh</code> to add environment variables. If it's too troublesome, you can add this command to <code>~/.bashrc</code>.</li>
</ol>
<h2 id="install-zcu102-bsp"><a class="header" href="#install-zcu102-bsp">Install ZCU102 BSP</a></h2>
<ol>
<li>Download the BSP corresponding to the Petalinux version, in the example it is <a href="https://china.xilinx.com/support/download/index.html/content/xilinx/zh/downloadNav/embedded-design-tools/2024-1.html">ZCU102 BSP 2024.1</a></li>
<li>Activate the Petalinux environment, i.e., in the Petalinux installation directory, run <code>source settings.sh</code>.</li>
<li>Create a Petalinux Project based on the BSP: <code>petalinux-create -t project -s xilinx-zcu102-v2024.1-05230256.bsp</code></li>
<li>This will create a <code>xilinx-zcu102-2024.1</code> folder, which contains the parameters required for QEMU to simulate ZCU102 (device tree), as well as precompiled Linux images, device trees, Uboot, etc., that can be directly loaded onto the board.</li>
</ol>
<h2 id="compile-hvisor"><a class="header" href="#compile-hvisor">Compile Hvisor</a></h2>
<p>Refer to "Running Hvisor on Qemu" for setting up the environment required to compile Hvisor, then in the hvisor directory, execute:</p>
<pre><code>make ARCH=aarch64 LOG=info FEATURES=platform_zcu102,gicv2 cp
</code></pre>
<p>to compile. The directory <code>/target/aarch64-unknown-none (may vary)/debug/hvisor</code> contains the required hvisor image.</p>
<h2 id="prepare-device-tree"><a class="header" href="#prepare-device-tree">Prepare Device Tree</a></h2>
<h3 id="use-existing-device-tree"><a class="header" href="#use-existing-device-tree">Use Existing Device Tree</a></h3>
<p>In the image/devicetree directory of Hvisor, there is zcu102-root-aarch64.dts, which is a device tree file tested for booting RootLinux, compile it as follows:</p>
<pre><code>dtc -I dts -O dtb -o zcu102-root-aarch64.dtb zcu102-root-aarch64.dts
</code></pre>
<p>If the dtc command is invalid, install device-tree-compiler.</p>
<pre><code>sudo apt-get install device-tree-compiler
</code></pre>
<h3 id="prepare-device-tree-yourself"><a class="header" href="#prepare-device-tree-yourself">Prepare Device Tree Yourself</a></h3>
<p>If you have custom requirements for the device, it is recommended to prepare the device tree yourself. You can decompile the <code>pre-built/linux/images/system.dtb</code> in the ZCU102 BSP to get a complete device tree, based on <code>zcu102-root-aarch64.dts</code> for modifications.</p>
<h2 id="prepare-image"><a class="header" href="#prepare-image">Prepare Image</a></h2>
<h3 id="use-existing-image"><a class="header" href="#use-existing-image">Use Existing Image</a></h3>
<p>It is recommended to use the <code>pre-built/linux/images/Image</code> from the ZCU102 BSP as the Linux kernel to boot on ZCU102, as its driver configuration is complete.</p>
<h3 id="compile-yourself"><a class="header" href="#compile-yourself">Compile Yourself</a></h3>
<p>After testing, the support for ZYNQMP in the Linux source code before 5.15 is not comprehensive, it is not recommended to use versions before this for compilation. For later versions, you can compile directly according to the general compilation process, as the basic support for ZYNQMP in the source code is enabled by default. Specific compilation operations are as follows:</p>
<ol>
<li>Visit the <a href="https://github.com/Xilinx/linux-xlnx/tags?after=xilinx-v2023.1">linux-xlnx</a> official website to download the Linux source code, it is best to download <code>zynqmp-soc-for-v6.3</code>.</li>
<li><code>tar -xvf zynqmp-soc-for-v6.3</code> to extract the source code</li>
<li>Enter the extracted directory, execute the following command using the default configuration, <code>make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- defconfig</code></li>
<li>Compile: <code>make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- Image -j$(nproc)</code></li>
<li>After compilation, the directory <code>arch/arm64/boot/Image</code> contains the required image.</li>
</ol>
<h2 id="enable-qemu-simulation"><a class="header" href="#enable-qemu-simulation">Enable QEMU Simulation</a></h2>
<ol>
<li>Activate the Petalinux environment, i.e., in the Petalinux installation directory, run <code>source settings.sh</code>.</li>
<li>Enter the <code>xilinx-zcu102-2024.1</code> folder, use the following command to start hvisor on the QEMU-simulated ZCU102, where the file paths need to be modified according to your actual situation.</li>
</ol>
<pre><code># QEMU parameter passing
petalinux-boot --qemu --prebuilt 2 --qemu-args '-device loader,file=hvisor,addr=0x40400000,force-raw=on -device loader,
file=zcu102-root-aarch64.dtb,addr=0x40000000,force-raw=on -device loader,file=zcu102-root-aarch64.dtb,addr=0x04000000,
force-raw=on -device loader,file=/home/hangqi-ren/Image,addr=0x00200000,force-raw=on -drive if=sd,format=raw,index=1,
file=rootfs.ext4' 
# Start hvisor
bootm 0x40400000 - 0x40000000
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zcu102-board-hvisor-multi-mode-boot"><a class="header" href="#zcu102-board-hvisor-multi-mode-boot">ZCU102 Board hvisor Multi-mode Boot</a></h1>
<h2 id="booting-hvisor-on-zcu102-development-board-in-sd-mode"><a class="header" href="#booting-hvisor-on-zcu102-development-board-in-sd-mode">Booting Hvisor on ZCU102 Development Board in SD mode</a></h2>
<h3 id="prepare-sd-card"><a class="header" href="#prepare-sd-card">Prepare SD Card</a></h3>
<ol>
<li>Prepare a standard SD card, partition it into a Boot partition (FAT32) and the rest as file system partitions (EXT4). For partitioning in Windows, you can use <a href="https://www.diskgenius.cn/download.php">DiskGenius</a>, and for Linux, you can use <a href="https://www.cnblogs.com/renshengdezheli/p/13941563.html">fdisk</a> or <a href="https://blog.csdn.net/linkedin_35878439/article/details/82020925">mkfs</a>.</li>
<li>Prepare a file system and copy its contents into any file system partition. You can refer to "NXPIMX8" for creating an Ubuntu file system or directly use the file system from the ZCU102 BSP.</li>
<li>Copy <code>zcu102-root-aarch64.dtb</code>, <code>Image</code>, and <code>hvisor</code> to the Boot partition.</li>
<li>In SD mode, it is necessary to provide ATF and Uboot from the SD card, therefore copy <code>pre-built/linux/images/boot.scr</code> and <code>BOOT.BIN</code> from the ZCU102 BSP to the BOOT partition.</li>
</ol>
<h4 id="booting-zcu102"><a class="header" href="#booting-zcu102">Booting ZCU102</a></h4>
<ol>
<li>Set the ZCU102 to SD mode, insert the SD card, connect the serial port, and power on.</li>
<li>Press any key to interrupt the Uboot auto script execution and run the following commands to boot hvisor and root linux:</li>
</ol>
<pre><code>fatload mmc 0:1 0x40400000 hvisor;fatload mmc 0:1 0x40000000 zcu102-root-aarch64.dtb
fatload mmc 0:1 0x04000000 zcu102-root-aarch64.dtb;fatload mmc 0:1 0x00200000 Image;bootm 0x40400000 - 0x40000000
</code></pre>
<ol start="3">
<li>If successfully booted, you will see hvisor and linux information on the serial port and eventually enter the file system.</li>
</ol>
<h2 id="booting-hvisor-on-zcu102-development-board-in-jtag-mode"><a class="header" href="#booting-hvisor-on-zcu102-development-board-in-jtag-mode">Booting Hvisor on ZCU102 Development Board in Jtag mode</a></h2>
<p>First, connect the two cables that come with the board to the JTAG and UART interfaces of the board, and the other end to the PC via USB.</p>
<p>Then, open a petalinux project in the command line, ensure the project has been compiled and has generated the corresponding boot files (vmlinux, BOOT.BIN, etc.), and then run from the project root directory:</p>
<pre><code class="language-bash">petalinux-boot --jtag --prebuilt 2
</code></pre>
<p>Where prebuilt represents the boot level:</p>
<ul>
<li><strong>Level 1</strong>: Only download the FPGA bitstream, boot FSBL and PMUFW</li>
<li><strong>Level 2</strong>: Download FPGA bitstream and boot UBOOT, and start FSBL, PMUFW, and TF-A (Trusted Firmware-A)</li>
<li><strong>Level 3</strong>: Download and boot linux, and load or boot FPGA bitstream, FSBL, PMUFW, TF-A, UBOOT</li>
</ul>
<p>Afterwards, JTAG will download the corresponding files to the board (save to the designated memory address) and boot the corresponding bootloader. For the default UBOOT script by the official, refer to the boot.scr file in the project image directory.</p>
<p>Since hvisor requires a separate UBOOT command and a custom-made fitImage to boot, please refer to <a href="chap02/subchap01/../../chap02/subchap01/UbootFitImage-ZCU102.html">UBOOT FIT Image Creation, Loading, and Booting</a>.</p>
<p>After creating the fitImage, replace the files in the petalinux images generation directory (Image.ub), so that JTAG loads our custom-made fitImage to the default FIT image load address configured in the petalinux project. This way, when JTAG boots, our fitImage will be loaded through the JTAG line to the corresponding address in the board memory, then extracted and booted through the uboot command line.</p>
<p>Another UART cable can be used to observe the output from the ZCU102 board (including FSBL, UBOOT, linux, etc.), which can be viewed through serial port tools such as <code>screen</code>, <code>gtkterm</code>, <code>termius</code>, or <code>minicom</code>.</p>
<div class="warning">
    <h3 id="please-note"><a class="header" href="#please-note">Please Note</a></h3>
    <p> Since petalinux has designated some fixed memory addresses, such as the default loading addresses for the linux kernel, fitImage, and DTB (configurable during petalinux project compilation), because we need to load and boot a custom-made fitImage, a problem currently identified is if the root linux dtb's load address in its matches the petalinux compilation load address, it will cause the dtb to be overwritten by the default petalinux dtb, leading to the root linux receiving an incorrect dtb and failing to boot. Therefore, it is necessary to specify a different address from the petalinux default dtb/fitImage load address during compilation to prevent other issues.
</div>
<h1 id="references"><a class="header" href="#references">References</a></h1>
<p>[1] PetaLinux Tools Documentation: Reference Guide (UG1144).<a href="https://docs.amd.com/r/2023.1-English/ug1144-petalinux-tools-reference-guide/Booting-a-PetaLinux-Image-on-Hardware-with-JTAG">https://docs.amd.com/r/2023.1-English/ug1144-petalinux-tools-reference-guide/Booting-a-PetaLinux-Image-on-Hardware-with-JTAG</a>
[2] Trusted Firmware-A Documentation.<a href="https://trustedfirmware-a.readthedocs.io/en/latest/">https://trustedfirmware-a.readthedocs.io/en/latest/</a></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="zcu102-nonroot-boot"><a class="header" href="#zcu102-nonroot-boot">ZCU102 NonRoot Boot</a></h2>
<ol>
<li>Use the Linux kernel source code used during the Root boot to compile <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a>, and the detailed compilation process can be found in <a href="https://github.com/syswonder/hvisor-tool/blob/main/README-zh.md">Readme</a>.</li>
<li>Prepare the <code>virtio_cfg.json</code> and <code>zone1_linux.json</code> needed to boot NonRoot. You can directly use the <code>example/zcu102-aarch64</code> in the hvisor-tool directory, which has been verified to ensure it can boot.</li>
<li>Prepare the Linux kernel Image, file system rootfs, and device tree linux1.dtb needed for NonRoot. The kernel and file system can be the same as Root, and Linux1.dtb can be configured as needed, or you can use the <code>images/aarch64/devicetree/zcu102-nonroot-aarch64.dts</code> in the hvisor directory.</li>
<li>Copy <code>hvisor.ko, hvisor, virtio_cfg, zone1_linux.json, linux1.dtb, Image, rootfs.ext4</code> to the file system used by Root Linux.</li>
<li>Enter the following commands in RootLinux to start NonRoot:</li>
</ol>
<pre><code># Load the kernel module
insmod hvisor.ko
# Create virtio device
nohup ./hvisor virtio start virtio_cfg.json &amp;
# Start NonRoot based on the json configuration file
./hvisor zone start zone1_linux.json 
# View the output of NonRoot and interact.
screen /dev/pts/0
</code></pre>
<p>For more operation details, refer to <a href="https://github.com/syswonder/hvisor-tool/blob/main/README-zh.md">hvisor-tool Readme</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="uboot-fit-image-creation-loading-and-booting"><a class="header" href="#uboot-fit-image-creation-loading-and-booting">UBOOT FIT Image Creation, Loading, and Booting</a></h1>
<p>wheatfox (enkerewpo@hotmail.com)</p>
<p>This article introduces the basic knowledge related to FIT images, as well as how to create, load, and boot FIT images.</p>
<h2 id="its-source-file"><a class="header" href="#its-source-file">ITS Source File</a></h2>
<p>ITS is the source code used by uboot to generate FIT images (FIT Image), i.e., Image Tree Source, which uses the Device Tree Source (DTS) syntax format. FIT images can be generated using the mkimage tool provided by uboot.
In the ZCU102 port of hvisor, FIT images are used to package hvisor, root linux, root dtb, and other files into one fitImage, facilitating booting on QEMU and actual hardware.
The ITS file for the ZCU102 platform is located at <code>scripts/zcu102-aarch64-fit.its</code>:</p>
<pre><code class="language-c">/dts-v1/;
/ {
    description = "FIT image for HVISOR with Linux kernel, root filesystem, and DTB";
    images {
        root_linux {
            description = "Linux kernel";
            data = /incbin/("__ROOT_LINUX_IMAGE__");
            type = "kernel";
            arch = "arm64";
            os = "linux";
            ...
        };
        ...
        root_dtb {
            description = "Device Tree Blob";
            data = /incbin/("__ROOT_LINUX_DTB__");
            type = "flat_dt";
            ...
        };
        hvisor {
            description = "Hypervisor";
            data = /incbin/("__HVISOR_TMP_PATH__");
            type = "kernel";
            arch = "arm64";
            ...
        };
    };

    configurations {
        default = "config@1";
        config@1 {
            description = "default";
            kernel = "hvisor";
            fdt = "root_dtb";
        };
    };
};
</code></pre>
<p>Here, <code>__ROOT_LINUX_IMAGE__</code>, <code>__ROOT_LINUX_DTB__</code>, <code>__HVISOR_TMP_PATH__</code> will be replaced with actual paths by the <code>sed</code> command in the Makefile. In the ITS source code, it mainly consists of images and configurations sections. The images section defines the files to be packaged, and the configurations section defines how to combine these files. During UBOOT booting, the files specified in the default configuration in configurations will be automatically loaded to the specified address. Multiple configurations can be set to support the loading of different image configurations at boot time.</p>
<p>The corresponding mkimage command in the Makefile:</p>
<pre><code class="language-Makefile">.PHONY: gen-fit
gen-fit: $(hvisor_bin) dtb
	@if [ ! -f scripts/zcu102-aarch64-fit.its ]; then \
		echo "Error: ITS file scripts/zcu102-aarch64-fit.its not found."; \
		exit 1; \
	fi
	$(OBJCOPY) $(hvisor_elf) --strip-all -O binary $(HVISOR_TMP_PATH)
# now we need to create the vmlinux.bin
	$(GCC_OBJCOPY) $(ROOT_LINUX_IMAGE) --strip-all -O binary $(ROOT_LINUX_IMAGE_BIN)
	@sed \
		-e "s|__ROOT_LINUX_IMAGE__|$(ROOT_LINUX_IMAGE_BIN)|g" \
		-e "s|__ROOT_LINUX_ROOTFS__|$(ROOT_LINUX_ROOTFS)|g" \
		-e "s|__ROOT_LINUX_DTB__|$(ROOT_LINUX_DTB)|g" \
		-e "s|__HVISOR_TMP_PATH__|$(HVISOR_TMP_PATH)|g" \
		scripts/zcu102-aarch64-fit.its &gt; temp-fit.its
	@mkimage -f temp-fit.its $(TARGET_FIT_IMAGE)
	@echo "Generated FIT image: $(TARGET_FIT_IMAGE)"
</code></pre>
<h2 id="booting-hvisor-and-root-linux-through-fit-image-in-petalinux-qemu"><a class="header" href="#booting-hvisor-and-root-linux-through-fit-image-in-petalinux-qemu">Booting hvisor and root linux through FIT image in petalinux qemu</a></h2>
<p>Since a fitImage includes all the necessary files, for qemu, it only needs to load this file into a suitable position in memory through the loader.</p>
<p>Then, qemu boots and enters UBOOT, where the following command can be used to boot (please modify the specific addresses according to the actual situation, and during actual use, all lines can be copied into one line and pasted into UBOOT for booting, or saved to the environment variable <code>bootcmd</code>, which requires UBOOT to mount a persistent flash for environment variable storage):</p>
<pre><code class="language-bash">setenv fit_addr 0x10000000; setenv root_linux_load 0x200000;
imxtract ${fit_addr} root_linux ${root_linux_load}; bootm ${fit_addr};
</code></pre>
<h1 id="references-1"><a class="header" href="#references-1">References</a></h1>
<p>[1] Flat Image Tree (FIT). <a href="https://docs.u-boot.org/en/stable/usage/fit/">https://docs.u-boot.org/en/stable/usage/fit/</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-compile"><a class="header" href="#how-to-compile">How to Compile</a></h1>
<h2 id="compile-using-docker"><a class="header" href="#compile-using-docker">Compile using Docker</a></h2>
<h3 id="1-install-docker"><a class="header" href="#1-install-docker">1. Install Docker</a></h3>
<pre><code class="language-bash">sudo snap install docker
</code></pre>
<p>You can also refer to the <a href="https://docs.docker.com/install/">Docker Official Documentation</a> to install Docker.</p>
<h3 id="2-build-the-image"><a class="header" href="#2-build-the-image">2. Build the Image</a></h3>
<pre><code class="language-bash">make build_docker
</code></pre>
<p>This step builds a Docker image, automatically compiling all required dependencies.</p>
<h3 id="3-run-the-container"><a class="header" href="#3-run-the-container">3. Run the Container</a></h3>
<pre><code class="language-bash">make docker
</code></pre>
<p>This step starts a container, mounts the current directory into the container, and enters the container's shell.</p>
<h3 id="4-compile"><a class="header" href="#4-compile">4. Compile</a></h3>
<p>Execute the following command in the container to compile.</p>
<pre><code class="language-bash">make all
</code></pre>
<h2 id="compile-using-the-local-environment"><a class="header" href="#compile-using-the-local-environment">Compile using the local environment</a></h2>
<h3 id="1-install-rustup-and-cargo"><a class="header" href="#1-install-rustup-and-cargo">1. Install RustUp and Cargo</a></h3>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | \
    sh -s -- -y --no-modify-path --profile minimal --default-toolchain nightly
</code></pre>
<h3 id="2-install-the-toolchain"><a class="header" href="#2-install-the-toolchain">2. Install the Toolchain</a></h3>
<p>The toolchain currently used by the project includes:</p>
<ul>
<li>Rust nightly 2023-07-12</li>
<li><a href="https://crates.io/crates/rustfmt">rustfmt</a></li>
<li><a href="https://crates.io/crates/clippy">clippy</a></li>
<li><a href="https://crates.io/crates/cargo-binutils/0.3.6">cargo-binutils</a></li>
<li>rust-src</li>
<li>llvm-tools-preview</li>
<li>target: aarch64-unknown-none</li>
</ul>
<p>You can check if these tools are installed yourself, or use the following commands to install them:</p>
<h4 id="1-install-toml-cli-and-cargo-binutils"><a class="header" href="#1-install-toml-cli-and-cargo-binutils">(1) Install toml-cli and cargo-binutils</a></h4>
<pre><code class="language-bash">cargo install toml-cli cargo-binutils
</code></pre>
<h4 id="2-install-the-cross-compilation-toolchain-for-the-target-platform"><a class="header" href="#2-install-the-cross-compilation-toolchain-for-the-target-platform">(2) Install the cross-compilation toolchain for the target platform</a></h4>
<pre><code class="language-bash">rustup target add aarch64-unknown-none
</code></pre>
<h4 id="3-parse-rust-toolchaintoml-to-install-the-rust-toolchain"><a class="header" href="#3-parse-rust-toolchaintoml-to-install-the-rust-toolchain">(3) Parse rust-toolchain.toml to install the Rust toolchain</a></h4>
<pre><code class="language-bash">RUST_VERSION=$(toml get -r rust-toolchain.toml toolchain.channel) &amp;&amp; \
Components=$(toml get -r rust-toolchain.toml toolchain.components | jq -r 'join(" ")') &amp;&amp; \
rustup install $RUST_VERSION &amp;&amp; \
rustup component add --toolchain $RUST_VERSION $Components
</code></pre>
<h4 id="4-compile-1"><a class="header" href="#4-compile-1">(4) Compile</a></h4>
<pre><code class="language-bash">make all
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-start-root-linux"><a class="header" href="#how-to-start-root-linux">How to Start Root Linux</a></h1>
<h2 id="qemu"><a class="header" href="#qemu">QEMU</a></h2>
<h3 id="install-dependencies"><a class="header" href="#install-dependencies">Install Dependencies</a></h3>
<h4 id="1-install-dependencies"><a class="header" href="#1-install-dependencies">1. Install Dependencies</a></h4>
<pre><code class="language-bash">apt-get install -y jq wget build-essential \
 libglib2.0-0 libfdt1 libpixman-1-0 zlib1g \
 libfdt-dev libpixman-1-dev libglib2.0-dev \
 zlib1g-dev ninja-build
</code></pre>
<h4 id="1-download-and-extract-qemu"><a class="header" href="#1-download-and-extract-qemu">1. Download and Extract QEMU</a></h4>
<pre><code class="language-bash">wget https://download.qemu.org/qemu-7.0.0.tar.xz
tar -xvf qemu-${QEMU_VERSION}.tar.xz
</code></pre>
<h4 id="2-conditionally-compile-and-install-qemu"><a class="header" href="#2-conditionally-compile-and-install-qemu">2. Conditionally Compile and Install QEMU</a></h4>
<p>Here we only compile QEMU for emulating aarch64, if you need QEMU for other architectures, refer to <a href="https://wiki.qemu.org/Hosts/Linux">QEMU Official Documentation</a>.</p>
<pre><code class="language-bash">cd qemu-7.0.0 &amp;&amp; \
./configure --target-list=aarch64-softmmu,aarch64-linux-user &amp;&amp; \
make -j$(nproc) &amp;&amp; \
make install
</code></pre>
<h4 id="3-test-if-qemu-is-successfully-installed"><a class="header" href="#3-test-if-qemu-is-successfully-installed">3. Test if QEMU is Successfully Installed</a></h4>
<pre><code class="language-bash">qemu-system-aarch64 --version
</code></pre>
<h3 id="start-root-linux"><a class="header" href="#start-root-linux">Start Root Linux</a></h3>
<h4 id="1-prepare-root-file-system-and-kernel-image"><a class="header" href="#1-prepare-root-file-system-and-kernel-image">1. Prepare Root File System and Kernel Image</a></h4>
<p>Place the image file in <code>hvisor/images/aarch64/kernel/</code>, named <code>Image</code>.</p>
<p>Place the Root file system in <code>hvisor/images/aarch64/virtdisk/</code>, named <code>rootfs1.ext4</code>.</p>
<h4 id="2-start-qemu"><a class="header" href="#2-start-qemu">2. Start QEMU</a></h4>
<p>Execute the following command in the hviosr directory:</p>
<pre><code class="language-bash">make run
</code></pre>
<h4 id="3-enter-qemu"><a class="header" href="#3-enter-qemu">3. Enter QEMU</a></h4>
<p>It will automatically load uboot, wait for uboot to finish loading, then enter <code>bootm 0x40400000 - 0x40000000</code> to enter Root Linux.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-start-nonroot-linux"><a class="header" href="#how-to-start-nonroot-linux">How to Start NonRoot Linux</a></h1>
<p>Hvisor has properly handled the startup of NonRoot, making it relatively simple, as follows:</p>
<ol>
<li>
<p>Prepare the kernel image, device tree, and file system for NonRoot Linux. Place the kernel and device tree in the file system of Root Linux.</p>
</li>
<li>
<p>Specify the serial port used by this NonRoot Linux and the file system to be mounted in the device tree file for NonRoot Linux, as shown in the example below:</p>
</li>
</ol>
<pre><code>	chosen {
		bootargs = "clk_ignore_unused console=ttymxc3,115200 earlycon=ec_imx6q3,0x30a60000,115200 root=/dev/mmcblk3p2 rootwait rw";
		stdout-path = "/soc@0/bus@30800000/serial@30a60000";
	};
</code></pre>
<ol start="3">
<li>
<p>Compile the <a href="https://github.com/syswonder/hvisor-tool?tab=readme-ov-file">kernel module and command line tools</a> for Hvisor and place them in the file system of Root Linux.</p>
</li>
<li>
<p>Start Hvisor's Root Linux and inject the kernel module that was just compiled:</p>
</li>
</ol>
<pre><code>insmod hvisor.ko
</code></pre>
<ol start="5">
<li>Use the command line tool, here assumed to be named <code>hvisor</code>, to start NonRoot Linux.</li>
</ol>
<pre><code>./hvisor zone start --kernel kernel image,addr=0x70000000 --dtb device tree file,addr=0x91000000 --id virtual machine number (starting from 1)
</code></pre>
<ol start="6">
<li>After NonRoot Linux has started, open the specified serial port to use it.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-and-management-of-zones"><a class="header" href="#configuration-and-management-of-zones">Configuration and Management of Zones</a></h1>
<p>The hvisor project, as a lightweight hypervisor, uses a Type-1 architecture that allows multiple virtual machines (zones) to run directly on top of hardware. Below is a detailed explanation of the key points for zone configuration and management:</p>
<h2 id="resource-allocation"><a class="header" href="#resource-allocation">Resource Allocation</a></h2>
<p>Resources such as CPU, memory, devices, and interrupts are statically allocated to each zone, meaning that once allocated, these resources are not dynamically scheduled between zones.</p>
<h2 id="root-zone-configuration"><a class="header" href="#root-zone-configuration">Root Zone Configuration</a></h2>
<p>The configuration of the root zone is hardcoded within hvisor, written in Rust, and represented as a C-style structure HvZoneConfig. This structure contains key information such as zone ID, number of CPUs, memory regions, interrupt information, physical addresses and sizes of the kernel and device tree binary (DTB).</p>
<h2 id="non-root-zones-configuration"><a class="header" href="#non-root-zones-configuration">Non-root Zones Configuration</a></h2>
<p>The configuration of non-root zones is stored in the root Linux file system, usually represented in JSON format. For example:</p>
<pre><code class="language-json">    {
        "arch": "arm64",
        "zone_id": 1,
        "cpus": [2, 3],
        "memory_regions": [
            {
                "type": "ram",
                "physical_start": "0x50000000",
                "virtual_start":  "0x50000000",
                "size": "0x30000000"
            },
            {
                "type": "io",
                "physical_start": "0x30a60000",
                "virtual_start":  "0x30a60000",
                "size": "0x1000"
            },
            {
                "type": "virtio",
                "physical_start": "0xa003c00",
                "virtual_start":  "0xa003c00",
                "size": "0x200"
            }
        ],
        "interrupts": [61, 75, 76, 78],
        "kernel_filepath": "./Image",
        "dtb_filepath": "./linux2.dtb",
        "kernel_load_paddr": "0x50400000",
        "dtb_load_paddr":   "0x50000000",
        "entry_point":      "0x50400000"
    }
</code></pre>
<ul>
<li>The <code>arch</code> field specifies the target architecture (e.g., arm64).</li>
<li><code>cpus</code> is a list that indicates the CPU core IDs allocated to the zone.</li>
<li><code>memory_regions</code> describe different types of memory regions and their physical and virtual start addresses and sizes.</li>
<li><code>interrupts</code> list the interrupt numbers allocated to the zone.</li>
<li><code>kernel_filepath</code> and <code>dtb_filepath</code> indicate the paths of the kernel and device tree binary files, respectively.</li>
<li><code>kernel_load_paddr</code> and <code>dtb_load_paddr</code> are the physical memory load addresses for the kernel and device tree binary.</li>
<li><code>entry_point</code> specifies the kernel's entry point address.</li>
</ul>
<p>The management tool of root Linux is responsible for reading the JSON configuration file and converting it into a C-style structure, which is then passed to hvisor to start the non-root zones.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="command-line-tool"><a class="header" href="#command-line-tool">Command Line Tool</a></h1>
<p>The command line tool is a management tool affiliated with hvisor, used to create and shut down other virtual machines on the Root Linux of the virtual machine manager, and is responsible for starting the Virtio daemon to provide Virtio device emulation. The repository is located at <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a>.</p>
<h2 id="how-to-compile-1"><a class="header" href="#how-to-compile-1">How to Compile</a></h2>
<p>The command line tool currently supports two architectures: arm64 and riscv, and needs to be used in conjunction with a kernel module. Cross-compilation on an x86 host can be done for different architectures.</p>
<ul>
<li>arm64 compilation</li>
</ul>
<p>Execute the following command in the hvisor-tool directory to obtain the command line tool hvisor and kernel module hvisorl.ko for the arm64 architecture.</p>
<pre><code>make all ARCH=arm64 KDIR=xxx
</code></pre>
<p>Where KDIR is the Root Linux source path, used for compiling the kernel module.</p>
<ul>
<li>riscv compilation</li>
</ul>
<p>Compile the command line tool and kernel module for the riscv architecture:</p>
<pre><code>make all ARCH=riscv KDIR=xxx
</code></pre>
<h2 id="managing-virtual-machines"><a class="header" href="#managing-virtual-machines">Managing Virtual Machines</a></h2>
<h3 id="load-the-kernel-module"><a class="header" href="#load-the-kernel-module">Load the Kernel Module</a></h3>
<p>Before using the command line tool, you need to load the kernel module to facilitate user-space programs to interact with Hypervisor:</p>
<pre><code>insmod hvisor.ko
</code></pre>
<p>The operation to unload the kernel module is:</p>
<pre><code>rmmod hvisor.ko
</code></pre>
<p>Where hvisor.ko is located in the hvisor-tool/driver directory.</p>
<h3 id="start-a-virtual-machine"><a class="header" href="#start-a-virtual-machine">Start a Virtual Machine</a></h3>
<p>On Root Linux, you can create a virtual machine with id 1 by the following command. This command will load the virtual machine's operating system image file <code>Image</code> to the real physical address <code>xxxa</code>, load the virtual machine's device tree file <code>linux2.dtb</code> to the real physical address <code>xxxb</code>, and start it.</p>
<pre><code>./hvisor zone start --kernel Image,addr=xxxa --dtb linux2.dtb,addr=xxxb --id 1
</code></pre>
<h3 id="shut-down-a-virtual-machine"><a class="header" href="#shut-down-a-virtual-machine">Shut Down a Virtual Machine</a></h3>
<p>Shut down the virtual machine with id 1:</p>
<pre><code>./hvisor zone shutdown -id 1
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="using-virtio-devices"><a class="header" href="#using-virtio-devices">Using VirtIO Devices</a></h1>
<p>Currently, hvisor supports three types of Virtio devices: Virtio block, Virtio net, and Virtio Console, which are presented to virtual machines other than Root Linux via MMIO. The Virtio device source code repository is located at <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a>, compiled and used together with the command-line tool. After creating a Virtio device through the command-line tool, the Virtio device becomes a daemon on Root Linux, and its log information is output to the nohup.out file.</p>
<h2 id="creating-and-starting-virtio-devices"><a class="header" href="#creating-and-starting-virtio-devices">Creating and Starting Virtio Devices</a></h2>
<p>Before creating a Virtio device through the command line, execute <code>insmod hvisor.ko</code> to load the kernel module.</p>
<h3 id="virtio-blk-device"><a class="header" href="#virtio-blk-device">Virtio blk Device</a></h3>
<p>Execute the following example command on the Root Linux console to create a Virtio blk device:</p>
<pre><code class="language-shell">nohup ./hvisor virtio start \
	--device blk,addr=0xa003c00,len=0x200,irq=78,zone_id=1,img=rootfs2.ext4 &amp;
</code></pre>
<p><code>--device blk</code> indicates creating a Virtio disk device for use by the virtual machine with id <code>zone_id</code>. The virtual machine will interact with the device through an MMIO region, starting at address <code>addr</code>, with length <code>len</code>, device interrupt number <code>irq</code>, and corresponding disk image path <code>img</code>.</p>
<blockquote>
<p>Virtual machines using Virtio devices need to add information about the Virtio mmio node in the device tree.</p>
</blockquote>
<h3 id="virtio-net-device"><a class="header" href="#virtio-net-device">Virtio net Device</a></h3>
<h4 id="creating-network-topology"><a class="header" href="#creating-network-topology">Creating Network Topology</a></h4>
<p>Before using a Virtio net device, a network topology needs to be created in Root Linux to connect the Virtio net device with the real network card through a Tap device and bridge device. Execute the following commands in Root Linux:</p>
<pre><code class="language-shell">mount -t proc proc /proc
mount -t sysfs sysfs /sys
ip link set eth0 up
dhclient eth0
brctl addbr br0
brctl addif br0 eth0
ifconfig eth0 0
dhclient br0
ip tuntap add dev tap0 mode tap
brctl addif br0 tap0
ip link set dev tap0 up
</code></pre>
<p>This creates a <code>tap0 device&lt;--&gt;bridge device&lt;--&gt;real network card</code> network topology.</p>
<h4 id="starting-virtio-net"><a class="header" href="#starting-virtio-net">Starting Virtio net</a></h4>
<p>Execute the following example command on the Root Linux console to create a Virtio net device:</p>
<pre><code class="language-shell">nohup ./hvisor virtio start \
	--device net,addr=0xa003600,len=0x200,irq=75,zone_id=1,tap=tap0 &amp;
</code></pre>
<p><code>--device net</code> indicates creating a Virtio network device for use by the virtual machine with id <code>zone_id</code>. The virtual machine will interact with the device through an MMIO region, starting at address <code>addr</code>, with length <code>len</code>, device interrupt number <code>irq</code>, and connected to a Tap device named <code>tap</code>.</p>
<h3 id="virtio-console-device"><a class="header" href="#virtio-console-device">Virtio Console Device</a></h3>
<p>Execute the following example command on the Root Linux console to create a Virtio console device:</p>
<pre><code class="language-shell">nohup ./hvisor virtio start \
	--device console,addr=0xa003800,len=0x200,irq=76,zone_id=1 &amp;
</code></pre>
<p><code>--device console</code> indicates creating a Virtio console for use by the virtual machine with id <code>zone_id</code>. The virtual machine will interact with the device through an MMIO region, starting at address <code>addr</code>, with length <code>len</code>, device interrupt number <code>irq</code>.</p>
<p>Execute <code>cat nohup.out | grep "char device"</code>, and you will see the output <code>char device redirected to /dev/pts/xx</code>. On Root Linux, execute:</p>
<pre><code>screen /dev/pts/xx
</code></pre>
<p>to enter the virtual console and interact with the virtual machine. Press the shortcut key <code>Ctrl +a d</code> to return to the Root Linux terminal. Execute <code>screen -r [session_id]</code> to re-enter the virtual console.</p>
<h3 id="creating-multiple-virtio-devices"><a class="header" href="#creating-multiple-virtio-devices">Creating Multiple Virtio Devices</a></h3>
<p>Execute the following command to simultaneously create Virtio blk, net, and console devices, all within one daemon process.</p>
<pre><code class="language-shell">nohup ./hvisor virtio start \
	--device blk,addr=0xa003c00,len=0x200,irq=78,zone_id=1,img=rootfs2.ext4 \
	--device net,addr=0xa003600,len=0x200,irq=75,zone_id=1,tap=tap0 \
	--device console,addr=0xa003800,len=0x200,irq=76,zone_id=1 &amp;
</code></pre>
<h2 id="closing-virtio-devices"><a class="header" href="#closing-virtio-devices">Closing Virtio Devices</a></h2>
<p>Execute the following command to close the Virtio daemon and all created devices:</p>
<pre><code>pkill hvisor
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hvisor-overall-architecture"><a class="header" href="#hvisor-overall-architecture">hvisor Overall Architecture</a></h1>
<ul>
<li>
<p>CPU Virtualization</p>
<ul>
<li>Architecture Compatibility: Supports architectures such as aarch64, riscv64, and loongarch, with dedicated CPU virtualization components for each architecture.</li>
<li>CPU Allocation: Uses static allocation method, pre-determining the CPU resources for each virtual machine.</li>
</ul>
</li>
<li>
<p>Memory Virtualization</p>
<ul>
<li>Two-stage Page Table: Utilizes two-stage page table technology to optimize the memory virtualization process.</li>
</ul>
</li>
<li>
<p>Interrupt Virtualization</p>
<ul>
<li>Interrupt Controller Virtualization: Supports virtualization of different architecture's interrupt controllers like ARM GIC and RISC-V PLIC.</li>
<li>Interrupt Handling: Manages the transmission and processing flow of interrupt signals.</li>
</ul>
</li>
<li>
<p>I/O Virtualization</p>
<ul>
<li>IOMMU Integration: Supports IOMMU to enhance the efficiency and security of DMA virtualization.</li>
<li>VirtIO Standard: Follows the VirtIO specification, providing high-performance virtual devices.</li>
<li>PCI Virtualization: Implements PCI virtualization, ensuring virtual machines can access physical or virtual I/O devices.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="initialization-process-of-hvisor"><a class="header" href="#initialization-process-of-hvisor">Initialization Process of hvisor</a></h1>
<p>Abstract: This article introduces the relevant knowledge involved in running hvisor on qemu and the initialization process of hvisor. Starting from the launch of qemu, the entire process is tracked, and after reading this article, you will have a general understanding of the initialization process of hvisor.</p>
<h2 id="boot-process-of-qemu"><a class="header" href="#boot-process-of-qemu">Boot Process of qemu</a></h2>
<p>The boot process of the computer simulated by qemu: After loading the necessary files into memory, the PC register is initialized to 0x1000, and a few instructions are executed from here before jumping to 0x80000000 to start executing the bootloader (hvsior arm part uses Uboot). After executing a few instructions, it jumps to the starting address of the kernel that uboot can recognize.</p>
<h3 id="generate-the-executable-file-of-hvisor"><a class="header" href="#generate-the-executable-file-of-hvisor">Generate the executable file of hvisor</a></h3>
<pre><code>rust-objcopy --binary-architecture=aarch64 target/aarch64-unknown-none/debug/hvisor --strip-all -O binary target/aarch64-unknown-none/debug/hvisor.bin.tmp
</code></pre>
<p>Convert the executable file of hvisor into a logical binary and save it as <code>hvisor.bin.tmp</code>.</p>
<h3 id="generate-an-image-file-recognizable-by-uboot"><a class="header" href="#generate-an-image-file-recognizable-by-uboot">Generate an image file recognizable by uboot</a></h3>
<p>Uboot is a bootloader whose main task is to jump to the first instruction of the hvisor image and start execution, so it is necessary to ensure that the generated hvisor image is recognizable by uboot. Here, the <code>mkimage</code> tool is needed.</p>
<pre><code>mkimage -n hvisor_img -A arm64 -O linux -C none -T kernel -a 0x40400000 -e 0x40400000 -d target/aarch64-unknown-none/debug/hvisor.bin.tmp target/aarch64-unknown-none/debug/hvisor.bin
</code></pre>
<ul>
<li><code>-n hvisor_img</code>: Specify the name of the kernel image.</li>
<li><code>-A arm64</code>: Specify the architecture as ARM64.</li>
<li><code>-O linux</code>: Specify the operating system as Linux.</li>
<li><code>-C none</code>: Do not use compression algorithms.</li>
<li><code>-T kernel</code>: Specify the type as kernel.</li>
<li><code>-a 0x40400000</code>: Specify the loading address as <code>0x40400000</code>.</li>
<li><code>-e 0x40400000</code>: Specify the entry address as <code>0x40400000</code>.</li>
<li><code>-d target/aarch64-unknown-none/debug/hvisor.bin.tmp</code>: Specify the input file as the previously generated temporary binary file.</li>
<li>The last parameter is the output file name, i.e., the final kernel image file <code>hvisor.bin</code>.</li>
</ul>
<h2 id="initialization-process"><a class="header" href="#initialization-process">Initialization Process</a></h2>
<h3 id="aarch64ld-link-script"><a class="header" href="#aarch64ld-link-script">aarch64.ld Link Script</a></h3>
<p>To understand how hvisor is executed, we first look at the link script <code>aarch64.ld</code>, which gives us a general understanding of the execution process of hvisor.</p>
<pre><code>ENTRY(arch_entry)
BASE_ADDRESS = 0x40400000;
</code></pre>
<p>The first line sets the program entry <code>arch_entry</code>, which can be found in <code>arch/aarch64/entry.rs</code>, introduced later.</p>
<pre><code>.text : {
        *(.text.entry)
        *(.text .text.*)
    }
</code></pre>
<p>We make the <code>.text</code> segment the first segment, and place the <code>.text.entry</code> containing the first instruction of the entry at the beginning of the <code>.text</code> segment, ensuring that hvisor indeed starts execution from the 0x40400000 location agreed with qemu.</p>
<p>Here we also need to remember something called <code>__core_end</code>, which is the address of the end position of the link script, and its role can be known during the startup process.</p>
<h3 id="arch_entry"><a class="header" href="#arch_entry">arch_entry</a></h3>
<p>With the above prerequisites, we can step into the first instruction of hvisor, which is <code>arch_entry()</code>.</p>
<pre><code>// src/arch/aarch64/entry.rs

pub unsafe extern "C" fn arch_entry() -&gt; i32 {
    unsafe {
        core::arch::asm!(
            "
            // x0 = dtbaddr
            mov x1, x0
            mrs x0, mpidr_el1
            and x0, x0, #0xff
            ldr x2, =__core_end          // x2 = &amp;__core_end
            mov x3, {per_cpu_size}      // x3 = per_cpu_size
            madd x4, x0, x3, x3       // x4 = cpuid * per_cpu_size + per_cpu_size
            add x5, x2, x4
            mov sp, x5           // sp = &amp;__core_end + (cpuid + 1) * per_cpu_size
            b {rust_main}             // x0 = cpuid, x1 = dtbaddr
            ",
            options(noreturn),
            per_cpu_size=const PER_CPU_SIZE,
            rust_main = sym crate::rust_main,
        );
    }
}
</code></pre>
<p>First, look at the embedded assembly part. The first instruction <code>mov x1, x0</code> transfers the value in the <code>x0</code> register to the <code>x1</code> register, where x0 contains the address of the device tree. Qemu simulates an ARM architecture computer, which also has various devices such as mice, display screens, and various storage devices. When we want to get input from the keyboard or output to the display, we need to get input from somewhere or put the output data somewhere. In the computer, we use specific addresses to access these devices. The device tree contains the access addresses of these devices, and the hypervisor, as the general manager of all software, naturally needs to know the information of the device tree. Therefore, Uboot will put this information in <code>x0</code> before entering the kernel, which is a convention.</p>
<p>In <code>mrs x0, mpidr_el1</code>, <code>mrs</code> is an instruction to access system-level registers, which means to send the contents of the system register <code>mpidr_el1</code> to <code>x0</code>. <code>mpidr_el1</code> contains information about which CPU we are currently dealing with (the computer supports multi-core CPUs), and there will be a lot of cooperation work with the CPU later, so we need to know which CPU is currently in use. This register contains a lot of information about the CPU, and we currently need to use the lower 8 bits to extract the corresponding CPU id, which is what the instruction <code>and x0, x0, #0xff</code> is doing.</p>
<p><code>ldr x2, = __core_end</code>, at the end of the link script, we set a symbol <code>__core_end</code> as the end address of the entire hvisor program space, and put this address into <code>x2</code>.</p>
<p><code>mov x3, {per_cpu_size}</code> puts the size of each CPU's stack into <code>x3</code>. This <code>{xxx}</code> is to replace the value of <code>xxx</code> defined externally into the assembly code. You can see that the parameter below <code>per_cpu_size=const PER_CPU_SIZE</code> has changed the name of an external variable as a parameter. Another parameter with <code>sym</code> indicates that a symbol follows, which is defined elsewhere.</p>
<p><code>per_cpu_size</code> in this size space, related registers can be saved and restored, including the CPU's stack space.</p>
<p><code>madd x4, x0, x3, x3</code> is a multiply-add instruction, cpu_id * per_cpu_size + per_cpu_size, and the result is put into <code>x4</code>. At this point, <code>x4</code> contains the total space required by the current number of CPUs. (Starting from 0, so add per_cpu_size one more time).</p>
<p><code>add x5, x2, x4</code> means to add the end address of hvisor and the total space required by the CPU to <code>x5</code>.</p>
<p><code>mov sp, x5</code> finds the top of the current CPU's stack.</p>
<p><code>b {rust_main}</code> represents jumping to <code>rust_main</code> to start execution, which also indicates that this section of assembly code will not return, corresponding to <code>option(noreturn)</code>.</p>
<h2 id="enter-rust_main"><a class="header" href="#enter-rust_main">Enter rust_main()</a></h2>
<h3 id="fn-rust_maincpuid-usize-host_dtb-usize"><a class="header" href="#fn-rust_maincpuid-usize-host_dtb-usize">fn rust_main(cpuid: usize, host_dtb: usize)</a></h3>
<p>Entering <code>rust_main</code> requires two parameters, which are passed through <code>x0</code> and <code>x1</code>. Remember that in the previous entry, our <code>x0</code> stored the cpu_id and <code>x1</code> stored the device tree information.</p>
<h3 id="install_trap_vector"><a class="header" href="#install_trap_vector">install_trap_vector()</a></h3>
<p>When the processor encounters an exception or interrupt, it needs to jump to the corresponding location for processing. Here, the corresponding jump addresses are set (which can be considered as setting a table) for handling exceptions at the Hypervisor level. Each privilege level has its own corresponding exception vector table, except for EL0, the application privilege level, which must jump to other privilege levels to handle exceptions. The <code>VBAR_ELn</code> register is used to store the base address of the exception vector table for the ELn privilege level.</p>
<pre><code>extern "C" {
    fn _hyp_trap_vector();
}

pub fn install_trap_vector() {
    // Set the trap vector.
    VBAR_EL2.set(_hyp_trap_vector as _)
}

</code></pre>
<p><code>VBAR_EL2.set()</code> sets the address of <code>_hyp_trap_vector()</code> as the base address of the exception vector table for the EL2 privilege level.</p>
<p><code>_hyp_trap_vector()</code> This assembly code constructs the exception vector table.</p>
<p><strong>Simple Introduction to the Exception Vector Table Format</strong></p>
<p>Based on the level of the exception and whether the level of handling the exception remains the same, it is divided into two categories. If the level does not change, it is divided into two groups based on whether the current level's SP is used. If the exception level changes, it is divided into two groups based on whether the execution mode is 64-bit/32-bit. Thus, the exception vector table is divided into 4 groups. In each group, each table entry represents an entry point for handling a specific type of exception.</p>
<h3 id="main-cpu"><a class="header" href="#main-cpu">Main CPU</a></h3>
<pre><code>static MASTER_CPU: AtomicI32 = AtomicI32::new(-1);

let mut is_primary = false;
    if MASTER_CPU.load(Ordering::Acquire) == -1 {
        MASTER_CPU.store(cpuid as i32, Ordering::Release);
        is_primary = true;
        println!("Hello, HVISOR!");
        #[cfg(target_arch = "riscv64")]
        clear_bss();
    }
</code></pre>
<p><code>static MASTER_CPU: AtomicI32</code> In this, <code>AtomicI32</code> indicates that this is an atomic type, meaning its operations are either successful or fail without any intermediate state, ensuring safe access in a multi-threaded environment. In short, it is a very safe <code>i32</code> type.</p>
<p><code>MASSTER_CPU.load()</code> is a method for performing read operations. The parameter <code>Ordering::Acquire</code> indicates that if there are some write operations before I read, I need to wait for these write operations to be completed in order. <strong>In short, this parameter ensures that the data is correctly changed before being read.</strong></p>
<p>If it reads -1, the same as when it was defined, it indicates that the main CPU has not been set, so set <code>cpu_id</code> as the main CPU. Similarly, the role of <code>Ordering::Release</code> is certainly to ensure that all other modifications are completed before the change.</p>
<h3 id="common-data-structure-for-cpus-percpu"><a class="header" href="#common-data-structure-for-cpus-percpu">Common Data Structure for CPUs: PerCpu</a></h3>
<p>hvisor supports different architectures, and a reasonable system design should allow different architectures to use a unified interface for easy description of each part's work. <code>PerCpu</code> is such a general CPU description.</p>
<pre><code>pub struct PerCpu {
    pub id: usize,
    pub cpu_on_entry: usize,
    pub arch_cpu: ArchCpu,
    pub zone: Option&lt;Arc&lt;RwLock&lt;Zone&gt;&gt;&gt;,
    pub ctrl_lock: Mutex&lt;()&gt;,
    pub boot_cpu: bool,
    // percpu stack
}
</code></pre>
<p>For each field of <code>PerCpu</code>:</p>
<ul>
<li><code>id</code>: CPU sequence number</li>
<li><code>cpu_on_entry</code>: The address of the first instruction when the CPU enters EL1, also known as the guest. Only when this CPU is the boot CPU will it be set to a valid value. Initially, we set it to an inaccessible address.</li>
<li><code>arch_cpu</code>: CPU description related to the architecture. The behavior is initiated by <code>PerCpu</code>, and the specific executor is <code>arch_cpu</code>.
<ul>
<li><code>cpu_id</code></li>
<li><code>psci_on</code>: Whether the cpu is started</li>
</ul>
</li>
<li><code>zone</code>: zone actually represents a guestOS. For the same guestOS, multiple CPUs may serve it.</li>
<li><code>ctrl_lock</code>: Set for concurrent safety.</li>
<li><code>boot_cpu</code>: For a guestOS, it distinguishes the main/secondary cores serving it. <code>boot_cpu</code> indicates whether the current CPU is the main core for a guest.</li>
</ul>
<h3 id="main-core-wakes-up-other-cores"><a class="header" href="#main-core-wakes-up-other-cores">Main Core Wakes Up Other Cores</a></h3>
<pre><code>if is_primary {
        wakeup_secondary_cpus(cpu.id, host_dtb);
}

fn wakeup_secondary_cpus(this_id: usize, host_dtb: usize) {
    for cpu_id in 0..MAX_CPU_NUM {
        if cpu_id == this_id {
            continue;
        }
        cpu_start(cpu_id, arch_entry as _, host_dtb);
    }
}

pub fn cpu_start(cpuid: usize, start_addr: usize, opaque: usize) {
    psci::cpu_on(cpuid as u64 | 0x80000000, start_addr as _, opaque as _).unwrap_or_else(|err| {
        if let psci::error::Error::AlreadyOn = err {
        } else {
            panic!("can't wake up cpu {}", cpuid);
        }
    });
}
</code></pre>
<p>If the current CPU is the main CPU, it will wake up other secondary cores, and the secondary cores execute <code>cpu_start</code>. In <code>cpu_start</code>, <code>cpu_on</code> actually calls the SMC instruction in <code>call64</code>, falling into EL3 to perform the action of waking up the CPU.</p>
<p>From the declaration of <code>cpu_on</code>, we can roughly guess its function: to wake up a CPU, which will start executing from the location <code>arch_entry</code>. This is because multi-core processors communicate and cooperate with each other, so CPU consistency must be ensured. Therefore, the same entry should be used to start execution to maintain synchronization. This can be verified by the following few lines of code.</p>
<pre><code>    ENTERED_CPUS.fetch_add(1, Ordering::SeqCst);
    wait_for(|| PerCpu::entered_cpus() &lt; MAX_CPU_NUM as _);
    assert_eq!(PerCpu::entered_cpus(), MAX_CPU_NUM as _);
</code></pre>
<p>Among them, <code>ENTERED_CPUS.fetch_add(1, Ordering::SeqCst)</code> represents increasing the value of <code>ENTERED_CPUS</code> in sequence consistency. After each CPU executes once, this <code>assert_eq</code> macro should pass smoothly.</p>
<h3 id="things-the-main-core-still-needs-to-do-primary_init_early"><a class="header" href="#things-the-main-core-still-needs-to-do-primary_init_early">Things the Main Core Still Needs to Do primary_init_early()</a></h3>
<p><strong>Initialize Logging</strong></p>
<ol>
<li>Creation of a global log recorder</li>
<li>Setting of the log level filter, the main purpose of setting the log level filter is to decide which log messages should be recorded and output.</li>
</ol>
<p><strong>Initialize Heap Space and Page Tables</strong></p>
<ol>
<li>A space in the .bss segment is allocated as heap space, and the allocator is set up.</li>
<li>Set up the page frame allocator.</li>
</ol>
<p><strong>Parse Device Tree Information</strong></p>
<p>Parse the device tree information based on the device tree address in the <code>rust_main</code> parameter.</p>
<p><strong>Create a GIC Instance</strong></p>
<p>Instantiate a global static variable GIC, an instance of the Generic Interrupt Controller.</p>
<p><strong>Initialize hvisor's Page Table</strong></p>
<p>This page table is only for the implementation of converting VA to PA for hypervisor itself (understood in terms of the relationship between the kernel and applications).</p>
<p><strong>Create a zone for each VM</strong></p>
<pre><code>zone_create(zone_id, TENANTS[zone_id] as _, DTB_IPA);

zone_create(vmid: usize, dtb_ptr: *const u8, dtb_ipa: usize) -&gt; Arc&lt;RwLock&lt;Zone&gt;&gt;
</code></pre>
<p>zone actually represents a guestVM, containing various information that a guestVM might use. Observing the function parameters, <code>dtb_ptr</code> is the address of the device information that the hypervisor wants this guest to see, which can be seen in <code>images/aarch64/devicetree</code>. The role of <code>dtb_ipa</code> is that each guest will obtain this address from the CPU's <code>x0</code> register to find the device tree information, so it is necessary to ensure that this IPA will map to the guest's dtb address during the construction of the stage2 page table. In this way, the guest is informed about the type of machine it is running on, the starting address of the physical memory, the number of CPUs, etc.</p>
<pre><code>let guest_fdt = unsafe { fdt::Fdt::from_ptr(dtb_ptr) }.unwrap();
    let guest_entry = guest_fdt
        .memory()
        .regions()
        .next()
        .unwrap()
        .starting_address as usize;
</code></pre>
<p>The above content, by parsing the device tree information, obtained <code>guest_entry</code>, which is the starting address of the physical address that this guest can see. In the qemu startup parameters, we can also see where a guest image is loaded into memory, and these two values are equal.</p>
<p>Next, the stage-2 page table, MMIO mapping, and IRQ bitmap for this guest will be constructed based on the <code>dtb</code> information.</p>
<pre><code>guest_fdt.cpus().for_each(|cpu| {
        let cpu_id = cpu.ids().all().next().unwrap();
        zone.cpu_set.set_bit(cpu_id as usize);
});

pub fn set_bit(&amp;mut self, id: usize) {
    assert!(id &lt;= self.max_cpu_id);
    self.bitmap |= 1 &lt;&lt; id;
}
</code></pre>
<p>The above code records the id of the CPU allocated to this zone in the bitmap according to the CPU information given in the dtb.</p>
<pre><code>let new_zone_pointer = Arc::new(RwLock::new(zone));
    {
        cpu_set.iter().for_each(|cpuid| {
            let cpu_data = get_cpu_data(cpuid);
            cpu_data.zone = Some(new_zone_pointer.clone());
            //chose boot cpu
            if cpuid == cpu_set.first_cpu().unwrap() {
                cpu_data.boot_cpu = true;
            }
            cpu_data.cpu_on_entry = guest_entry;
        });
    }
  
</code></pre>
<p>The task completed by the above code is: Traverse the CPUs allocated to this zone, obtain the mutable reference of the <code>PerCpu</code> of that CPU, modify its zone member variable, and mark the first CPU allocated to this zone as <code>boot_cpu</code>. Also, set the address of the first instruction after this zone's main CPU enters the guest as <code>guest_entry</code>.</p>
<p>The tasks that the main core CPU needs to do are paused, marked with <code>INIT_EARLY_OK.store(1, Ordering::Release)</code>, while other CPUs can only wait before the main core completes <code>wait_for_counter(&amp;INIT_EARLY_OK, 1)</code>.</p>
<h3 id="address-space-initialization"><a class="header" href="#address-space-initialization">Address Space Initialization</a></h3>
<p>The previous section mentioned IPA and PA, which are actually part of the address space. Specific content will be provided in the memory management document, and here is a brief introduction.</p>
<p>If Hypervisor is not considered, guestVM, as a kernel, will perform memory management work, which is the process from the application program's virtual address VA to the kernel's PA. In this case, the PA is the actual physical memory address.</p>
<p>When considering Hypervisor, Hypervisor, as a kernel role, will also perform memory management work, only this time the application program becomes guestVM, and guestVM will not be aware of the existence of Hypervisor (otherwise, it would require changing the design of guestVM, which does not meet our intention to improve performance). We call the PA in guestVM IPA or GPA because it</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="percpu-structure"><a class="header" href="#percpu-structure">PerCPU Structure</a></h1>
<p>In the architecture of hvisor, the PerCpu structure plays a core role, used to implement local state management for each CPU core and support CPU virtualization. Below is a detailed introduction to the PerCpu structure and related functions:</p>
<h2 id="percpu-structure-definition"><a class="header" href="#percpu-structure-definition">PerCpu Structure Definition</a></h2>
<p>The PerCpu structure is designed as a container for each CPU core to store its specific data and state. Its layout is as follows:</p>
<pre><code>#[repr(C)]
pub struct PerCpu {
    pub id: usize,
    pub cpu_on_entry: usize,
    pub dtb_ipa: usize,
    pub arch_cpu: ArchCpu,
    pub zone: Option&lt;Arc&lt;RwLock&lt;Zone&gt;&gt;&gt;,
    pub ctrl_lock: Mutex&lt;()&gt;,
    pub boot_cpu: bool,
    // percpu stack
}
</code></pre>
<p>The field definitions are as follows:</p>
<pre><code>    id: Identifier of the CPU core.
    cpu_on_entry: An address used to track the CPU's entry state, initialized to INVALID_ADDRESS, indicating an invalid address.
    dtb_ipa: Physical address of the device tree binary, also initialized to INVALID_ADDRESS.
    arch_cpu: A reference to the ArchCpu type, which contains architecture-specific CPU information and functions.
    zone: An optional Arc&lt;RwLock&lt;Zone&gt;&gt; type, representing the virtual machine (zone) currently running on the CPU core.
    ctrl_lock: A mutex used to control access and synchronize PerCpu data.
    boot_cpu: A boolean value indicating whether it is the boot CPU.
</code></pre>
<h2 id="construction-and-operation-of-percpu"><a class="header" href="#construction-and-operation-of-percpu">Construction and Operation of PerCpu</a></h2>
<pre><code>    PerCpu::new: This function creates and initializes the PerCpu structure. It first calculates the virtual address of the structure, then safely writes the initialization data. For the RISC-V architecture, it also updates the CSR_SSCRATCH register to store the pointer to ArchCpu.
    run_vm: When this method is called, if the current CPU is not the boot CPU, it will first be put into an idle state, then run the virtual machine.
    entered_cpus: Returns the number of CPU cores that have entered the virtual machine running state.
    activate_gpm: Activates the GPM (Guest Page Management) of the associated zone.
</code></pre>
<h2 id="obtaining-percpu-instances"><a class="header" href="#obtaining-percpu-instances">Obtaining PerCpu Instances</a></h2>
<pre><code>    get_cpu_data: Provides a method to obtain a PerCpu instance based on CPU ID.
    this_cpu_data: Returns the PerCpu instance of the currently executing CPU.
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu-virtualization-on-aarch64"><a class="header" href="#cpu-virtualization-on-aarch64">CPU Virtualization on AArch64</a></h1>
<h2 id="cpu-boot-mechanism"><a class="header" href="#cpu-boot-mechanism">CPU Boot Mechanism</a></h2>
<p>Under the AArch64 architecture, hvisor uses the <code>psci::cpu_on()</code> function to wake up a specified CPU core, bringing it from a shutdown state to a running state. This function takes the CPU's ID, boot address, and an opaque parameter as input. If an error occurs, such as the CPU already being awake, the function will handle the error appropriately to avoid redundant wake-ups.</p>
<h2 id="cpu-virtualization-initialization-and-operation"><a class="header" href="#cpu-virtualization-initialization-and-operation">CPU Virtualization Initialization and Operation</a></h2>
<p>The <code>ArchCpu</code> structure encapsulates architecture-specific CPU information and functionalities, and its <code>reset()</code> method is responsible for setting the CPU to the initial state of virtualization mode. This includes:</p>
<ul>
<li>Setting the ELR_EL2 register to the specified entry point</li>
<li>Configuring the SPSR_EL2 register</li>
<li>Clearing the general registers</li>
<li>Resetting the virtual machine-related registers</li>
<li><code>activate_vmm()</code>, activating the Virtual Memory Manager (VMM)</li>
</ul>
<p>The <code>activate_vmm()</code> method is used to configure the VTCR_EL2 and HCR_EL2 registers, enabling the virtualization environment.</p>
<p>The <code>ArchCpu</code>'s <code>run()</code> and <code>idle()</code> methods are used to start and idle the CPU, respectively. Upon starting, it activates the zone's GPM (Guest Page Management), resets to the specified entry point and device tree binary (DTB) address, and then jumps to the EL2 entry point through the <code>vmreturn</code> macro. In idle mode, the CPU is reset to a waiting state (WFI) and prepares a <code>parking</code> instruction page for use during idle periods.</p>
<h2 id="switching-between-el1-and-el2"><a class="header" href="#switching-between-el1-and-el2">Switching Between EL1 and EL2</a></h2>
<p>hvisor uses EL2 as the hypervisor mode and EL1 for the guest OS in the AArch64 architecture. The <code>handle_vmexit</code> macro handles the context switch from EL1 to EL2 (VMEXIT event), saves the user mode register context, calls an external function to handle the exit reason, and then returns to continue executing the hypervisor code segment. The <code>vmreturn</code> function is used to return from EL2 mode to EL1 mode (VMENTRY event), restores the user mode register context, and then returns to the guest OS code segment through the <code>eret</code> instruction.</p>
<h2 id="mmu-configuration-and-enabling"><a class="header" href="#mmu-configuration-and-enabling">MMU Configuration and Enabling</a></h2>
<p>To support virtualization, the <code>enable_mmu()</code> function configures MMU mapping in EL2 mode, including setting the MAIR_EL2, TCR_EL2, and SCTLR_EL2 registers, enabling instruction and data caching capabilities, and ensuring the virtual range covers the entire 48-bit address space.</p>
<p>Through these mechanisms, hvisor achieves efficient CPU virtualization on the AArch64 architecture, allowing multiple independent zones to operate under statically allocated resources while maintaining system stability and performance.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu-virtualization-under-riscv"><a class="header" href="#cpu-virtualization-under-riscv">CPU Virtualization under RISCV</a></h1>
<p>Abstract: Introduce the CPU virtualization work under the RISCV architecture around the ArchCpu structure.</p>
<h2 id="two-data-structures-involved"><a class="header" href="#two-data-structures-involved">Two Data Structures Involved</a></h2>
<p>Hvisor supports multiple architectures, and the work required for CPU virtualization in each architecture is different, but a unified interface should be provided in a system. Therefore, we split the CPU into two data structures: <code>PerCpu</code> and <code>ArchCpu</code>.</p>
<h3 id="percpu"><a class="header" href="#percpu">PerCpu</a></h3>
<p>This is a general description of the CPU, which has already been introduced in the <code>PerCpu</code> documentation.</p>
<h3 id="archcpu"><a class="header" href="#archcpu">ArchCpu</a></h3>
<p><code>ArchCpu</code> is a CPU structure for specific architectures (<strong>RISCV architecture is introduced in this article</strong>). This structure undertakes the specific behavior of the CPU.</p>
<p>In the ARM architecture, there is also a corresponding <code>ArchCpu</code>, which has a slightly different structure from the <code>ArchCpu</code> introduced in this section, but they have the same interface (i.e., they both have behaviors such as initialization).</p>
<p>The fields included are as follows:</p>
<pre><code>pub struct ArchCpu {
    pub x: [usize; 32], //x0~x31
    pub hstatus: usize,
    pub sstatus: usize,
    pub sepc: usize,
    pub stack_top: usize,
    pub cpuid: usize,
    pub power_on: bool,
    pub init: bool,
    pub sstc: bool,
}
</code></pre>
<p>The explanation of each field is as follows:</p>
<ul>
<li><code>x</code>: values of general-purpose registers</li>
<li><code>hstatus</code>: stores the value of the Hypervisor status register</li>
<li><code>sstatus</code>: stores the value of the Supervisor status register, managing S-mode state information, such as interrupt enable flags, etc.</li>
<li><code>sepc</code>: the return address at the end of exception handling</li>
<li><code>stack_top</code>: the stack top of the corresponding CPU stack</li>
<li><code>power_on</code>: whether the CPU is powered on</li>
<li><code>init</code>: whether the CPU has been initialized</li>
<li><code>sstc</code>: whether the timer interrupt has been configured</li>
</ul>
<h2 id="related-methods"><a class="header" href="#related-methods">Related Methods</a></h2>
<p>This part explains the methods involved.</p>
<h3 id="archcpuinit"><a class="header" href="#archcpuinit">ArchCpu::init</a></h3>
<p>This method mainly initializes the CPU, sets the context when first entering the VM, and some CSR initialization.</p>
<h3 id="archcpuidle"><a class="header" href="#archcpuidle">ArchCpu::idle</a></h3>
<p>By executing the wfi instruction, set non-primary CPUs to a low-power idle state.</p>
<p>Set up a special memory page containing instructions that make the CPU enter a low-power waiting state, allowing them to be placed in a low-power waiting state when no tasks are allocated to some CPUs in the system until an interrupt occurs.</p>
<h3 id="archcpurun"><a class="header" href="#archcpurun">ArchCpu::run</a></h3>
<p>The main content of this method is some initialization, setting the correct CPU execution entry, and modifying the flag that the CPU has been initialized.</p>
<h3 id="vcpu_arch_entry--vm_entry"><a class="header" href="#vcpu_arch_entry--vm_entry">vcpu_arch_entry / VM_ENTRY</a></h3>
<p>This is a piece of assembly code describing the work that needs to be handled when entering the VM from hvisor. First, it gets the context information in the original <code>ArchCpu</code> through the <code>sscratch</code> register, then sets <code>hstatus</code>, <code>sstatus</code>, and <code>sepc</code> to the values we previously saved, ensuring that when returning to the VM, it is in VS mode and starts executing from the correct position. Finally, restore the values of the general-purpose registers and return to the VM using <code>sret</code>.</p>
<h3 id="vm_exit"><a class="header" href="#vm_exit">VM_EXIT</a></h3>
<p>When exiting the VM and entering hvisor, it is also necessary to save the relevant state at the time of VM exit.</p>
<p>First, get the address of <code>ArchCpu</code> through the <code>sscratch</code> register, but here we will swap the information of <code>sscratch</code> and <code>x31</code>, rather than directly overwriting <code>x31</code>. Then save the values of the general-purpose registers except <code>x31</code>. Now the information of <code>x31</code> is in <code>sscratch</code>, so first save the value of <code>x31</code> to <code>sp</code>, then swap <code>x31</code> and <code>sscratch</code>, and store the information of <code>x31</code> through <code>sp</code> to the corresponding position in <code>ArchCpu</code>.</p>
<p>Then save <code>hstatus</code>, <code>sstatus</code>, and <code>sepc</code>. When we finish the work in hvisor and need to return to the VM, we need to use the VM_ENTRY code to restore the values of these three registers to the state before the VM entered hvisor, so we should save them here.</p>
<p><code>ld sp, 35*8(sp)</code> puts the top of the kernel stack saved by <code>ArchCpu</code> into <code>sp</code> for use, facilitating the use of the kernel stack in hvisor.</p>
<p><code>csrr a0, sscratch</code> puts the value of <code>sscratch</code> into the <code>a0</code> register. When we have saved the context and jump to the exception handling function, the parameters will be passed through <code>a0</code>, allowing access to the saved context during exception handling, such as the exit code, etc.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loongarch-processor-virtualization"><a class="header" href="#loongarch-processor-virtualization">LoongArch Processor Virtualization</a></h1>
<p>The LoongArch instruction set is an independent RISC instruction set released by China's Loongson Zhongke Company in 2020, which includes five modules: the basic instruction set, binary translation extension (LBT), vector extension (LSX), advanced vector extension (LASX), and virtualization extension (LVZ).</p>
<p>This article will provide a brief introduction to the CPU virtualization design of the LoongArch instruction set, with related explanations from the currently publicly available KVM source code and code comments.</p>
<h2 id="introduction-to-loongarch-registers"><a class="header" href="#introduction-to-loongarch-registers">Introduction to LoongArch Registers</a></h2>
<h3 id="conventions-for-general-registers-usage"><a class="header" href="#conventions-for-general-registers-usage">Conventions for General Registers Usage</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Alias</th><th>Usage</th><th>Preserved across calls</th></tr></thead><tbody>
<tr><td><code>$r0</code></td><td><code>$zero</code></td><td>Constant 0</td><td>(constant)</td></tr>
<tr><td><code>$r1</code></td><td><code>$ra</code></td><td>Return address</td><td>No</td></tr>
<tr><td><code>$r2</code></td><td><code>$tp</code></td><td>Thread pointer</td><td>(not allocatable)</td></tr>
<tr><td><code>$r3</code></td><td><code>$sp</code></td><td>Stack pointer</td><td>Yes</td></tr>
<tr><td><code>$r4 - $r5</code></td><td><code>$a0 - $a1</code></td><td>Argument/return value registers</td><td>No</td></tr>
<tr><td><code>$r6 - $r11</code></td><td><code>$a2 - $a7</code></td><td>Argument registers</td><td>No</td></tr>
<tr><td><code>$r12 - $r20</code></td><td><code>$t0 - $t8</code></td><td>Temporary registers</td><td>No</td></tr>
<tr><td><code>$r21</code></td><td>Reserved</td><td>(not allocatable)</td><td></td></tr>
<tr><td><code>$r22</code></td><td><code>$fp / $s9</code></td><td>Frame pointer / static register</td><td>Yes</td></tr>
<tr><td><code>$r23 - $r31</code></td><td><code>$s0 - $s8</code></td><td>Static registers</td><td>Yes</td></tr>
</tbody></table>
</div>
<h3 id="conventions-for-floating-point-registers-usage"><a class="header" href="#conventions-for-floating-point-registers-usage">Conventions for Floating Point Registers Usage</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Alias</th><th>Usage</th><th>Preserved across calls</th></tr></thead><tbody>
<tr><td><code>$f0 - $f1</code></td><td><code>$fa0 - $fa1</code></td><td>Argument/return value registers</td><td>No</td></tr>
<tr><td><code>$f2 - $f7</code></td><td><code>$fa2 - $fa7</code></td><td>Argument registers</td><td>No</td></tr>
<tr><td><code>$f8 - $f23</code></td><td><code>$ft0 - $ft15</code></td><td>Temporary registers</td><td>No</td></tr>
<tr><td><code>$f24 - $f31</code></td><td><code>$fs0 - $fs7</code></td><td>Static registers</td><td>Yes</td></tr>
</tbody></table>
</div>
<p>Temporary registers are also known as caller-saved registers. Static registers are also known as callee-saved registers.</p>
<h3 id="csr-registers"><a class="header" href="#csr-registers">CSR Registers</a></h3>
<p><strong>Control and Status Register (CSR)</strong> is a special class of registers in the LoongArch architecture used to control the processor's operational state.
List of CSR registers (excluding new CSRs in the LVZ virtualization extension):</p>
<div class="table-wrapper"><table><thead><tr><th>Number</th><th>Name</th><th>Number</th><th>Name</th><th>Number</th><th>Name</th></tr></thead><tbody>
<tr><td>0x0</td><td>Current mode information <code>CRMD</code></td><td>0x1</td><td>Exception prior mode information <code>PRMD</code></td><td>0x2</td><td>Extension part enable <code>EUEN</code></td></tr>
<tr><td>0x3</td><td>Miscellaneous control <code>MISC</code></td><td>0x4</td><td>Exception configuration <code>ECFG</code></td><td>0x5</td><td>Exception status <code>ESTAT</code></td></tr>
<tr><td>0x6</td><td>Exception return address <code>ERA</code></td><td>0x7</td><td>Error virtual address <code>BADV</code></td><td>0x8</td><td>Error instruction <code>BADI</code></td></tr>
<tr><td>0xc</td><td>Exception entry address <code>EENTRY</code></td><td>0x10</td><td>TLB index <code>TLBIDX</code></td><td>0x11</td><td>TLB entry high <code>TLBEHI</code></td></tr>
<tr><td>0x12</td><td>TLB entry low 0 <code>TLBELO0</code></td><td>0x13</td><td>TLB entry low 1 <code>TLBELO1</code></td><td>0x18</td><td>Address space identifier <code>ASID</code></td></tr>
<tr><td>0x19</td><td>Low half address space global directory base <code>PGDL</code></td><td>0x1A</td><td>High half address space global directory base <code>PGDH</code></td><td>0x1B</td><td>Global directory base <code>PGD</code></td></tr>
<tr><td>0x1C</td><td>Page table traversal control low half <code>PWCL</code></td><td>0x1D</td><td>Page table traversal control high half <code>PWCH</code></td><td>0x1E</td><td>STLB page size <code>STLBPS</code></td></tr>
<tr><td>0x1F</td><td>Reduced virtual address configuration <code>RVACFG</code></td><td>0x20</td><td>Processor number <code>CPUID</code></td><td>0x21</td><td>Privilege resource configuration info 1 <code>PRCFG1</code></td></tr>
<tr><td>0x22</td><td>Privilege resource configuration info 2 <code>PRCFG2</code></td><td>0x23</td><td>Privilege resource configuration info 3 <code>PRCFG3</code></td><td>0x30+n (0≤n≤15)</td><td>Data save <code>SAVEn</code></td></tr>
<tr><td>0x40</td><td>Timer number <code>TID</code></td><td>0x41</td><td>Timer configuration <code>TCFG</code></td><td>0x42</td><td>Timer value <code>TVAL</code></td></tr>
<tr><td>0x43</td><td>Timer compensation <code>CNTC</code></td><td>0x44</td><td>Timer interrupt clear <code>TICLR</code></td><td>0x60</td><td>LLBit control <code>LLBCTL</code></td></tr>
<tr><td>0x80</td><td>Implementation related control 1 <code>IMPCTL1</code></td><td>0x81</td><td>Implementation related control 2 <code>IMPCTL2</code></td><td>0x88</td><td>TLB refill exception entry address <code>TLBRENTRY</code></td></tr>
<tr><td>0x89</td><td>TLB refill exception error virtual address <code>TLBRBADV</code></td><td>0x8A</td><td>TLB refill exception return address <code>TLBRERA</code></td><td>0x8B</td><td>TLB refill exception data save <code>TLBRSAVE</code></td></tr>
<tr><td>0x8C</td><td>TLB refill exception entry low 0 <code>TLBRELO0</code></td><td>0x8D</td><td>TLB refill exception entry low 1 <code>TLBRELO1</code></td><td>0x8E</td><td>TLB refill exception entry high <code>TLBREHI</code></td></tr>
<tr><td>0x8F</td><td>TLB refill exception prior mode information <code>TLBRPRMD</code></td><td>0x90</td><td>Machine error control <code>MERRCTL</code></td><td>0x91</td><td>Machine error information 1 <code>MERRINFO1</code></td></tr>
<tr><td>0x92</td><td>Machine error information 2 <code>MERRINFO2</code></td><td>0x93</td><td>Machine error exception entry address <code>MERRENTRY</code></td><td>0x94</td><td>Machine error exception return address <code>MERRERA</code></td></tr>
<tr><td>0x95</td><td>Machine error exception data save <code>MERRSAVE</code></td><td>0x98</td><td>Cache tag <code>CTAG</code></td><td>0x180+n (0≤n≤3)</td><td>Direct mapping configuration window n <code>DMWn</code></td></tr>
<tr><td>0x200+2n (0≤n≤31)</td><td>Performance monitoring configuration n <code>PMCFGn</code></td><td>0x201+2n (0≤n≤31)</td><td>Performance monitoring counter n <code>PMCNTn</code></td><td>0x300</td><td>Load/store monitor point overall control <code>MWPC</code></td></tr>
<tr><td>0x301</td><td>Load/store monitor point overall status <code>MWPS</code></td><td>0x310+8n (0≤n≤7)</td><td>Load/store monitor point n configuration 1 <code>MWPnCFG1</code></td><td>0x311+8n (0≤n≤7)</td><td>Load/store monitor point n configuration 2 <code>MWPnCFG2</code></td></tr>
<tr><td>0x312+8n (0≤n≤7)</td><td>Load/store monitor point n configuration 3 <code>MWPnCFG3</code></td><td>0x313+8n (0≤n≤7)</td><td>Load/store monitor point n configuration 4 <code>MWPnCFG4</code></td><td>0x380</td><td>Instruction fetch monitor point overall control <code>FWPC</code></td></tr>
<tr><td>0x381</td><td>Instruction fetch monitor point overall status <code>FWPS</code></td><td>0x390+8n (0≤n≤7)</td><td>Instruction fetch monitor point n configuration 1 <code>FWPnCFG1</code></td><td>0x391+8n (0≤n≤7)</td><td>Instruction fetch monitor point n configuration 2 <code>FWPnCFG2</code></td></tr>
<tr><td>0x392+8n (0≤n≤7)</td><td>Instruction fetch monitor point n configuration 3 <code>FWPnCFG3</code></td><td>0x393+8n (0≤n≤7)</td><td>Instruction fetch monitor point n configuration 4 <code>FWPnCFG4</code></td><td>0x500</td><td>Debug register <code>DBG</code></td></tr>
<tr><td>0x501</td><td>Debug exception return address <code>DERA</code></td><td>0x502</td><td>Debug data save <code>DSAVE</code></td><td></td><td></td></tr>
</tbody></table>
</div>
<p>For processors that have implemented the LVZ virtualization extension, there is an additional set of CSR registers for controlling virtualization.</p>
<div class="table-wrapper"><table><thead><tr><th>Number</th><th>Name</th></tr></thead><tbody>
<tr><td>0x15</td><td>Guest TLB control <code>GTLBC</code></td></tr>
<tr><td>0x16</td><td>TLBRD read Guest item <code>TRGP</code></td></tr>
<tr><td>0x50</td><td>Guest status <code>GSTAT</code></td></tr>
<tr><td>0x51</td><td>Guest control <code>GCTL</code></td></tr>
<tr><td>0x52</td><td>Guest interrupt control <code>GINTC</code></td></tr>
<tr><td>0x53</td><td>Guest counter compensation <code>GCNTC</code></td></tr>
</tbody></table>
</div>
<h3 id="gcsr-register-group"><a class="header" href="#gcsr-register-group">GCSR Register Group</a></h3>
<p>In LoongArch processors that implement virtualization, there is an additional group of <strong>GCSR (Guest Control and Status Register)</strong> registers.</p>
<h3 id="process-of-entering-guest-mode-from-linux-kvm-source-code"><a class="header" href="#process-of-entering-guest-mode-from-linux-kvm-source-code">Process of Entering Guest Mode (from Linux KVM source code)</a></h3>
<ol>
<li><strong><code>switch_to_guest</code></strong>:</li>
<li>Clear the <code>CSR.ECFG.VS</code> field (set to 0, i.e., all exceptions share one entry address)</li>
<li>Read the guest eentry saved in the Hypervisor (guest OS interrupt vector address) -&gt; GEENTRY
<ol>
<li>Then write GEENTRY to <code>CSR.EENTRY</code></li>
</ol>
</li>
<li>Read the guest era saved in the Hypervisor (guest OS exception return address) -&gt; GPC
<ol>
<li>Then write GPC to <code>CSR.ERA</code></li>
</ol>
</li>
<li>Read the <code>CSR.PGDL</code> global page table address, save it in the Hypervisor</li>
<li>Load the guest pgdl from the Hypervisor into <code>CSR.PGDL</code></li>
<li>Read <code>CSR.GSTAT.GID</code> and <code>CSR.GTLBC.TGID</code>, write to <code>CSR.GTLBC</code></li>
<li>Set <code>CSR.PRMD.PIE</code> to 1, turn on Hypervisor-level global interrupts</li>
<li>Set <code>CSR.GSTAT.PGM</code> to 1, the purpose is to make the ertn instruction enter guest mode</li>
<li>The Hypervisor restores the guest's general registers (GPRS) saved by itself to the hardware registers (restore the scene)</li>
<li><strong>Execute the <code>ertn</code> instruction, enter guest mode</strong></li>
</ol>
<h2 id="virtualization-related-exceptions"><a class="header" href="#virtualization-related-exceptions">Virtualization-related Exceptions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>code</th><th>subcode</th><th>Abbreviation</th><th>Introduction</th></tr></thead><tbody>
<tr><td>22</td><td>-</td><td>GSPR</td><td>Guest-sensitive privileged resource exception, triggered by <code>cpucfg</code>, <code>idle</code>, <code>cacop</code> instructions, and when the virtual machine accesses non-existent GCSR and IOCSR, forcing a trap into the Hypervisor for processing (such as software simulation)</td></tr>
<tr><td>23</td><td>-</td><td>HVC</td><td>Exception triggered by the hvcl supercall instruction</td></tr>
<tr><td>24</td><td>0</td><td>GCM</td><td>Guest GCSR software modification exception</td></tr>
<tr><td>24</td><td>1</td><td>GCHC</td><td>Guest GCSR hardware modification exception</td></tr>
</tbody></table>
</div>
<h3 id="process-of-handling-exceptions-under-guest-mode-from-linux-kvm-source-code"><a class="header" href="#process-of-handling-exceptions-under-guest-mode-from-linux-kvm-source-code">Process of Handling Exceptions Under Guest Mode (from Linux KVM source code)</a></h3>
<ol>
<li>
<p><strong><code>kvm_exc_entry</code></strong>:</p>
</li>
<li>
<p>The Hypervisor first saves the guest's general registers (GPRS), protecting the scene.</p>
</li>
<li>
<p>The Hypervisor saves <code>CSR.ESTAT</code> -&gt; host ESTAT</p>
</li>
<li>
<p>The Hypervisor saves <code>CSR.ERA</code> -&gt; GPC</p>
</li>
<li>
<p>The Hypervisor saves <code>CSR.BADV</code> -&gt; host BADV, i.e., when an address error exception is triggered, the erroneous virtual address is recorded</p>
</li>
<li>
<p>The Hypervisor saves <code>CSR.BADI</code> -&gt; host BADI, this register is used to record the instruction code of the instruction that triggered the synchronous class exception, synchronous class exceptions refer to all exceptions except for interrupts (INT), guest CSR hardware modification exceptions (GCHC), and machine error exceptions (MERR).</p>
</li>
<li>
<p>Read the host ECFG saved by the Hypervisor, write to <code>CSR.ECFG</code> (i.e., switch to the host's exception configuration)</p>
</li>
<li>
<p>Read the host EENTRY saved by the Hypervisor, write to <code>CSR.EENTRY</code></p>
</li>
<li>
<p>Read the host PGD saved by the Hypervisor, write to <code>CSR.PGDL</code> (restore the host page table global directory base, low half space)</p>
</li>
<li>
<p>Set <code>CSR.GSTAT.PGM</code> off</p>
</li>
<li>
<p>Clear the <code>GTLBC.TGID</code> field</p>
</li>
<li>
<p>Restore kvm per CPU registers</p>
<ol>
<li>The kvm assembly involves KVM_ARCH_HTP, KVM_ARCH_HSP, KVM_ARCH_HPERCPU</li>
</ol>
</li>
<li>
<p><strong>Jump to KVM_ARCH_HANDLE_EXIT to handle the exception</strong></p>
</li>
<li>
<p>Determine if the return of the function just now is &lt;=0</p>
<ol>
<li>If &lt;=0, continue running the host</li>
<li>Otherwise, continue running the guest, save percpu registers, as it may switch to a different CPU to continue running the guest. Save host percpu registers to <code>CSR.KSAVE</code> register</li>
</ol>
</li>
<li>
<p>Jump to <code>switch_to_guest</code></p>
</li>
</ol>
<h2 id="vcpu-context-registers-to-be-saved"><a class="header" href="#vcpu-context-registers-to-be-saved">vCPU Context Registers to be Saved</a></h2>
<p>According to the LoongArch function call specification, if you need to manually switch the CPU function running context, the registers to be saved are as follows (excluding floating point registers): <code>$s0</code>-<code>$s9</code>, <code>$sp</code>, <code>$ra</code></p>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<p>[1] Loongson Zhongke Technology Co., Ltd. Loongson Architecture ELF psABI Specification. Version 2.01.</p>
<p>[2] Loongson Zhongke Technology Co., Ltd. Loongson Architecture Reference Manual. Volume One: Basic Architecture.</p>
<p>[3] <a href="https://github.com/torvalds/linux/blob/master/arch/loongarch/kvm/switch.S">https://github.com/torvalds/linux/blob/master/arch/loongarch/kvm/switch.S</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h1>
<h2 id="memory-allocation-on-heap"><a class="header" href="#memory-allocation-on-heap">Memory Allocation on Heap</a></h2>
<h3 id="allocator-initialization"><a class="header" href="#allocator-initialization">Allocator Initialization</a></h3>
<p>When using programming languages, dynamic memory allocation is often encountered, such as allocating a block of memory through <code>malloc</code> or <code>new</code> in C, or <code>Vec</code>, <code>String</code>, etc. in Rust, which are allocated on the heap.</p>
<p>To allocate memory on the heap, we need to do the following:</p>
<ul>
<li>Provide a large block of memory space during initialization</li>
<li>Provide interfaces for allocation and release</li>
<li>Manage free blocks</li>
</ul>
<p>In short, we need to allocate a large space and set up an allocator to manage this space, and tell Rust that we now have an allocator, asking it to use it, allowing us to use variables like <code>Vec</code>, <code>String</code> that allocate memory on the heap. This is what the following lines do.</p>
<pre><code>use buddy_system_allocator::LockedHeap;

use crate::consts::HV_HEAP_SIZE;

#[cfg_attr(not(test), global_allocator)]
static HEAP_ALLOCATOR: LockedHeap&lt;32&gt; = LockedHeap::&lt;32&gt;::new();

/// Initialize the global heap allocator.
pub fn init() {
    const MACHINE_ALIGN: usize = core::mem::size_of::&lt;usize&gt;();
    const HEAP_BLOCK: usize = HV_HEAP_SIZE / MACHINE_ALIGN;
    static mut HEAP: [usize; HEAP_BLOCK] = [0; HEAP_BLOCK];
    let heap_start = unsafe { HEAP.as_ptr() as usize };
    unsafe {
        HEAP_ALLOCATOR
            .lock()
            .init(heap_start, HEAP_BLOCK * MACHINE_ALIGN);
    }
    info!(
        "Heap allocator initialization finished: {:#x?}",
        heap_start..heap_start + HV_HEAP_SIZE
    );
}
</code></pre>
<p><code>#[cfg_attr(not(test), global_allocator)]</code> is a conditional compilation attribute, which sets the <code>HEAP_ALLOCATOR</code> defined in the next line as Rust's global memory allocator when not in a test environment. Now Rust knows we can do dynamic allocation.</p>
<p><code>HEAP_ALLOCATOR.lock().init(heap_start, HEAP_BLOCK * MACHINE_ALIGN)</code> hands over the large space we applied for to the allocator for management.</p>
<h3 id="testing"><a class="header" href="#testing">Testing</a></h3>
<pre><code>pub fn test() {
    use alloc::boxed::Box;
    use alloc::vec::Vec;
    extern "C" {
        fn sbss();
        fn ebss();
    }
    let bss_range = sbss as usize..ebss as usize;
    let a = Box::new(5);
    assert_eq!(*a, 5);
    assert!(bss_range.contains(&amp;(a.as_ref() as *const _ as usize)));
    drop(a);
    let mut v: Vec&lt;usize&gt; = Vec::new();
    for i in 0..500 {
        v.push(i);
    }
    for (i, val) in v.iter().take(500).enumerate() {
        assert_eq!(*val, i);
    }
    assert!(bss_range.contains(&amp;(v.as_ptr() as usize)));
    drop(v);
    info!("heap_test passed!");
}
</code></pre>
<p>In this test, <code>Box</code> and <code>Vec</code> are used to verify the memory we allocated, whether it is in the <code>bss</code> segment.</p>
<p>The large block of memory we just handed over to the allocator is an uninitialized global variable, which will be placed in the <code>bss</code> segment. We only need to test whether the address of the variable we obtained is within this range.</p>
<h2 id="memory-management-knowledge-for-armv8"><a class="header" href="#memory-management-knowledge-for-armv8">Memory Management Knowledge for Armv8</a></h2>
<h3 id="addressing"><a class="header" href="#addressing">Addressing</a></h3>
<p>The address bus is by default 48 bits, while the addressing request issued is 64 bits, so the virtual address can be divided into two spaces based on the high 16 bits:</p>
<ul>
<li>High 16 bits are 1: Kernel space</li>
<li>High 16 bits are 0: User space</li>
</ul>
<p>From the perspective of guestVM, when converting a virtual address to a physical address, the CPU will select the TTBR register based on the value of the 63rd bit of the virtual address. TTBR registers store the base address of the first-level page table. If it is user space, select TTBR0; if it is kernel space, select TTBR1.</p>
<h3 id="four-level-page-table-mapping-using-a-page-size-of-4k-as-an-example"><a class="header" href="#four-level-page-table-mapping-using-a-page-size-of-4k-as-an-example">Four-Level Page Table Mapping (using a page size of 4K as an example)</a></h3>
<p>In addition to the high 16 bits used to determine which page table base address register to use, the following 36 bits are used as indexes for each level of the page table, with the lower 12 bits as the page offset, as shown in the diagram below.</p>
<p><img src="chap04/./img/memory_level4_pagetable.png" alt="Level4_PageTable" /></p>
<h3 id="stage-2-page-table-mechanism"><a class="header" href="#stage-2-page-table-mechanism">Stage-2 Page Table Mechanism</a></h3>
<p>In a virtualized environment, there are two types of address mapping processes in the system:</p>
<ul>
<li>guestVM uses Stage-1 address conversion, using <code>TTBR0_EL1</code> or <code>TTBR1_EL1</code>, to convert the accessed VA to IPA, then through Stage-2 address conversion, using <code>VTTBR0_EL2</code> to convert IPA to PA.</li>
<li>Hypervisor may run its own applications, and the VA to PA conversion for these applications only requires one conversion, using the <code>TTBR0_EL2</code> register.</li>
</ul>
<p><img src="chap04/./img/memory_nested_translation.png" alt="Nested_Address_Translation" /></p>
<h2 id="memory-management-in-hvsior"><a class="header" href="#memory-management-in-hvsior">Memory Management in hvsior</a></h2>
<h3 id="management-of-physical-page-frames"><a class="header" href="#management-of-physical-page-frames">Management of Physical Page Frames</a></h3>
<p>Similar to the heap construction mentioned above, page frame allocation also requires an allocator, then we hand over a block of memory we use for allocation to the allocator for management.</p>
<p><strong>Bitmap-based Allocator</strong></p>
<pre><code>use bitmap_allocator::BitAlloc;
type FrameAlloc = bitmap_allocator::BitAlloc1M;

struct FrameAllocator {
    base: PhysAddr,
    inner: FrameAlloc,
}
</code></pre>
<p><code>BitAlloc1M</code> is a bitmap-based allocator, which manages page numbers, <strong>providing information on which pages are free and which are occupied</strong>.</p>
<p>Then, the bitmap allocator and the starting address used for page frame allocation are encapsulated into a page frame allocator.</p>
<p>So we see the initialization function as follows:</p>
<pre><code>fn init(&amp;mut self, base: PhysAddr, size: usize) {
        self.base = align_up(base);
        let page_count = align_up(size) / PAGE_SIZE;
        self.inner.insert(0..page_count);
    }
</code></pre>
<p>The starting address of the page frame allocation area and the size of the available space are passed in, the number of page frames available for allocation in this space <code>page_size</code> is calculated, and then all page frame numbers are told to the bitmap allocator through the <code>insert</code> function.</p>
<p><strong>Structure of Page Frames</strong></p>
<pre><code>pub struct Frame {
    start_paddr: PhysAddr,
    frame_count: usize,
}
</code></pre>
<p>The structure of the page frame includes the starting address of this page frame and the number of page frames corresponding to this frame instance, which may be 0, 1, or more than 1.</p>
<blockquote>
<p>Why are there cases where the number of page frames is 0?</p>
<p>When hvisor wants to access the contents of the page frame through <code>Frame</code>, it needs a temporary instance that does not involve page frame allocation and page frame recycling, so 0 is used as a flag.</p>
</blockquote>
<blockquote>
<p>Why are there cases where the number of page frames is greater than 1?</p>
<p>In some cases, we are required to allocate continuous memory, and the size exceeds one page, which means allocating multiple continuous page frames.</p>
</blockquote>
<p><strong>Allocation alloc</strong></p>
<p>Now we know that the page frame allocator can allocate a number of a free page frame, turning the number into a <code>Frame</code> instance completes the allocation of the page frame, as shown below for a single page frame allocation:</p>
<pre><code>impl FrameAllocator {
    fn init(&amp;mut self, base: PhysAddr, size: usize) {
        self.base = align_up(base);
        let page_count = align_up(size) / PAGE_SIZE;
        self.inner.insert(0..page_count);
    }
}

impl Frame {
    /// Allocate one physical frame.
    pub fn new() -&gt; HvResult&lt;Self&gt; {
        unsafe {
            FRAME_ALLOCATOR
                .lock()
                .alloc()
                .map(|start_paddr| Self {
                    start_paddr,
                    frame_count: 1,
                })
                .ok_or(hv_err!(ENOMEM))
        }
    }
}
</code></pre>
<p>As you can see, the frame allocator helps us allocate a page frame and returns the starting physical address, then creates a <code>Frame</code> instance.</p>
<p><strong>Recycling of Page Frames</strong></p>
<p>The <code>Frame</code> structure is linked to the actual physical page, following the RAII design standard, so when a <code>Frame</code> leaves the scope, the corresponding memory area also needs to be returned to hvisor. This requires us to implement the <code>drop</code> method in the <code>Drop Trait</code>, as follows:</p>
<pre><code>impl Drop for Frame {
    fn drop(&amp;mut self) {
        unsafe {
            match self.frame_count {
                0 =&gt; {} // Do not deallocate when use Frame::from_paddr()
                1 =&gt; FRAME_ALLOCATOR.lock().dealloc(self.start_paddr),
                _ =&gt; FRAME_ALLOCATOR
                    .lock()
                    .dealloc_contiguous(self.start_paddr, self.frame_count),
            }
        }
    }
}

impl FrameAllocator{
    unsafe fn dealloc(&amp;mut self, target: PhysAddr) {
        trace!("Deallocate frame: {:x}", target);
        self.inner.dealloc((target - self.base) / PAGE_SIZE)
    }
}
</code></pre>
<p>In <code>drop</code>, you can see that page frames with a frame count of 0 do not need to release the corresponding physical pages, and those with a frame count greater than 1 indicate that they are continuously allocated page frames, requiring the recycling of more than one physical page.</p>
<h3 id="data-structures-related-to-page-tables"><a class="header" href="#data-structures-related-to-page-tables">Data Structures Related to Page Tables</a></h3>
<p>With the knowledge of Armv8 memory management mentioned above, we know that the process of building page tables is divided into two parts: the page table used by hvisor itself and the Stage-2 conversion page table. We will focus on the Stage-2 page table.</p>
<p>Before that, we need to understand a few data structures that will be used.</p>
<p><strong>Logical Segment MemoryRegion</strong></p>
<p>Description of the logical segment, including starting address, size, permission flags, and mapping method.</p>
<pre><code>pub struct MemoryRegion&lt;VA&gt; {
    pub start: VA,
    pub size: usize,
    pub flags: MemFlags,
    pub mapper: Mapper,
}
</code></pre>
<p><strong>Address Space MemorySet</strong></p>
<p>Description of each process's address space, including a collection of logical segments and the corresponding page table for the process.</p>
<pre><code>pub struct MemorySet&lt;PT: GenericPageTable&gt;
where
    PT::VA: Ord,
{
    regions: BTreeMap&lt;PT::VA, MemoryRegion&lt;PT::VA&gt;&gt;,
    pt: PT,
}
</code></pre>
<p><strong>4-Level Page Table Level4PageTableImmut</strong></p>
<p><code>root</code> is the page frame where the L0 page table is located.</p>
<pre><code>pub struct Level4PageTableImmut&lt;VA, PTE: GenericPTE&gt; {
    /// Root table frame.
    root: Frame,
    /// Phantom data.
    _phantom: PhantomData&lt;(VA, PTE)&gt;,
}
</code></pre>
<h3 id="building-the-stage-2-page-table"><a class="header" href="#building-the-stage-2-page-table">Building the Stage-2 Page Table</a></h3>
<p>We need to build a Stage-2 page table for each zone.</p>
<h4 id="areas-to-be-mapped-by-the-stage-2-page-table"><a class="header" href="#areas-to-be-mapped-by-the-stage-2-page-table">Areas to be Mapped by the Stage-2 Page Table:</a></h4>
<ul>
<li>The memory area seen by guestVM</li>
<li>The IPA of the device tree accessed by guestVM</li>
<li>The memory area of the UART device seen by guestVM</li>
</ul>
<h4 id="adding-mapping-relationships-to-the-address-space"><a class="header" href="#adding-mapping-relationships-to-the-address-space">Adding Mapping Relationships to the Address Space</a></h4>
<pre><code>/// Add a memory region to this set.
    pub fn insert(&amp;mut self, region: MemoryRegion&lt;PT::VA&gt;) -&gt; HvResult {
        assert!(is_aligned(region.start.into()));
        assert!(is_aligned(region.size));
        if region.size == 0 {
            return Ok(());
        }
        if !self.test_free_area(&amp;region) {
            warn!(
                "MemoryRegion overlapped in MemorySet: {:#x?}\n{:#x?}",
                region, self
            );
            return hv_result_err!(EINVAL);
        }
        self.pt.map(&amp;region)?;
        self.regions.insert(region.start, region);
        Ok(())
    }
</code></pre>
<p>In addition to adding the mapping relationship between the virtual address and the logical segment to our <code>Map</code> structure, we also need to perform mapping in the page table, as follows:</p>
<pre><code>fn map(&amp;mut self, region: &amp;MemoryRegion&lt;VA&gt;) -&gt; HvResult {
        assert!(
            is_aligned(region.start.into()),
            "region.start = {:#x?}",
            region.start.into()
        );
        assert!(is_aligned(region.size), "region.size = {:#x?}", region.size);
        trace!(
            "create mapping in {}: {:#x?}",
            core::any::type_name::&lt;Self&gt;(),
            region
        );
        let _lock = self.clonee_lock.lock();
        let mut vaddr = region.start.into();
        let mut size = region.size;
        while size &gt; 0 {
            let paddr = region.mapper.map_fn(vaddr);
            let page_size = if PageSize::Size1G.is_aligned(vaddr)
                &amp;&amp; PageSize::Size1G.is_aligned(paddr)
                &amp;&amp; size &gt;= PageSize::Size1G as usize
                &amp;&amp; !region.flags.contains(MemFlags::NO_HUGEPAGES)
            {
                PageSize::Size1G
            } else if PageSize::Size2M.is_aligned(vaddr)
                &amp;&amp; PageSize::Size2M.is_aligned(paddr)
                and size &gt;= PageSize::Size2M as usize
                &amp;&amp; !region.flags.contains(MemFlags::NO_HUGEPAGES)
            {
                PageSize::Size2M
            } else {
                PageSize::Size4K
            };
            let page = Page::new_aligned(vaddr.into(), page_size);
            self.inner
                .map_page(page, paddr, region.flags)
                .map_err(|e: PagingError| {
                    error!(
                        "failed to map page: {:#x?}({:?}) -&gt; {:#x?}, {:?}",
                        vaddr, page_size, paddr, e
                    );
                    e
                })?;
            vaddr += page_size as usize;
            size -= page_size as usize;
        }
        Ok(())
    }
</code></pre>
<p>Let's briefly interpret this function. For a logical segment <code>MemoryRegion</code>, we map it page by page until the entire logical segment size is covered.</p>
<p>The specific behavior is as follows:</p>
<p>Before mapping each page, we first determine the physical address <code>paddr</code> corresponding to this page according to the mapping method of the logical segment.</p>
<p>Then determine the page size <code>page_size</code>. We start by checking the 1G page size. If the physical address can be aligned, the remaining unmapped page size is greater than 1G, and large page mapping is not disabled, then 1G is chosen as the page size. Otherwise, check the 2M page size, and if none of these conditions are met, use the standard 4KB page size.</p>
<p>We now have the information needed to fill in the page table entry. We combine the page starting address and page size into a <code>Page</code> instance and perform mapping in the page table, which is modifying the page table entry:</p>
<pre><code>fn map_page(
        &amp;mut self,
        page: Page&lt;VA&gt;,
        paddr: PhysAddr,
        flags: MemFlags,
    ) -&gt; PagingResult&lt;&amp;mut PTE&gt; {
        let entry: &amp;mut PTE = self.get_entry_mut_or_create(page)?;
        if !entry.is_unused() {
            return Err(PagingError::AlreadyMapped);
        }
        entry.set_addr(page.size.align_down(paddr));
        entry.set_flags(flags, page.size.is_huge());
        Ok(entry)
    }
</code></pre>
<p>This function briefly describes the following functionality: First, we obtain the PTE according to the VA, or more precisely, according to the page number VPN corresponding to this VA. We fill in the control bit information and the physical address (actually, it should be PPN) in the PTE. Specifically, you can see in the <code>PageTableEntry</code>'s set_addr method that we did not fill in the entire physical address but only the content excluding the lower 12 bits, because our page table only cares about the mapping of page frame numbers.</p>
<p>Let's take a closer look at how to obtain the PTE:</p>
<pre><code>fn get_entry_mut_or_create(&amp;mut self, page: Page&lt;VA&gt;) -&gt; PagingResult&lt;&amp;mut PTE&gt; {
        let vaddr: usize = page.vaddr.into();
        let p4 = table_of_mut::&lt;PTE&gt;(self.inner.root_paddr());
        let p4e = &amp;mut p4[p4_index(vaddr)];

        let p3 = next_table_mut_or_create(p4e, || self.alloc_intrm_table())?;
        let p3e = &amp;mut p3[p3_index(vaddr)];
        if page.size == PageSize::Size1G {
            return Ok(p3e);
        }

        let p2 = next_table_mut_or_create(p3e, || self.alloc_intrm_table())?;
        let p2e = &amp;mut p2[p2_index(vaddr)];
        if page.size == PageSize::Size2M {
            return Ok(p2e);
        }

        let p1 = next_table_mut_or_create(p2e, || self.alloc_intrm_table())?;
        let p1e = &amp;mut p1[p1_index(vaddr)];
        Ok(p1e)
    }
</code></pre>
<p>First, we find the starting address of the L0 page table, then obtain the corresponding page table entry <code>p4e</code> according to the L0 index in the VA. However, we cannot directly obtain the starting address of the next level page table from <code>p4e</code>, as the corresponding page table may not have been created yet. If it has not been created, we create a new page table (this process also requires page frame allocation), then return the starting address of the page table, and so forth, until we obtain the page table entry PTE corresponding to the L4 index in the L4 page table.</p>
<p><strong>After mapping the memory (the same applies to UART devices) through the process described above, we also need to fill the L0 page table base address into the VTTBR_EL2 register. This process can be seen in the activate function of the Zone's MemorySet's Level4PageTable.</strong></p>
<blockquote>
<p>In a non-virtualized environment, why isn't guestVM allowed to access memory areas related to MMIO and GIC devices?</p>
<p>This is because, in a virtualized environment, hvisor is the manager of resources and cannot arbitrarily allow guestVM to access areas related to devices. In the previous exception handling, we mentioned access to MMIO/GIC, which actually results in falling into EL2 due to the lack of address mapping, and EL2 accesses the resources and returns the results. If mapping was performed in the page table, it would directly access the resources through the second-stage address conversion without passing through EL2's control.</p>
<p>Therefore, in our design, only MMIOs that are allowed to be accessed by the Zone are registered in the Zone, and when related exceptions occur, they are used to determine whether a certain MMIO resource is allowed to be accessed by the Zone.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="arm-gicv3-module"><a class="header" href="#arm-gicv3-module">ARM GICv3 Module</a></h1>
<h2 id="1-gicv3-module"><a class="header" href="#1-gicv3-module">1. GICv3 Module</a></h2>
<h3 id="gicv3-initialization-process"><a class="header" href="#gicv3-initialization-process">GICv3 Initialization Process</a></h3>
<p>The GICv3 initialization process in hvisor involves the initialization of the GIC Distributor (GICD) and GIC Redistributor (GICR), as well as the mechanisms for interrupt handling and virtual interrupt injection. Key steps in this process include:</p>
<ul>
<li>SDEI version check: Obtain the version information of the Secure Debug Extensions Interface (SDEI) through smc_arg1!(0xc4000020).</li>
<li>ICCs configuration: Set icc_ctlr_el1 to only provide priority drop functionality, set icc_pmr_el1 to define the interrupt priority mask, and enable Group 1 IRQs.</li>
<li>Clear pending interrupts: Call the gicv3_clear_pending_irqs function to clear all pending interrupts, ensuring the system is in a clean state.</li>
<li>VMCR and HCR configuration: Set ich_vmcr_el2 and ich_hcr_el2 registers to enable the virtualization CPU interface and prepare for virtual interrupt handling.</li>
</ul>
<h3 id="pending-interrupt-handling"><a class="header" href="#pending-interrupt-handling">Pending Interrupt Handling</a></h3>
<ul>
<li>The <code>pending_irq</code> function reads the <code>icc_iar1_el1</code> register and returns the current interrupt ID being processed. If the value is greater than or equal to 0x3fe, it is considered an invalid interrupt.</li>
<li>The <code>deactivate_irq</code> function clears the interrupt flags by writing to the <code>icc_eoir1_el1</code> and <code>icc_dir_el1</code> registers, enabling the interrupt.</li>
</ul>
<h3 id="virtual-interrupt-injection"><a class="header" href="#virtual-interrupt-injection">Virtual Interrupt Injection</a></h3>
<ul>
<li>The <code>inject_irq</code> function checks for an available <code>List Register (LR)</code> and writes the virtual interrupt information into it. This function distinguishes between hardware interrupts and software-generated interrupts, appropriately setting fields in the LR.</li>
</ul>
<h3 id="gic-data-structure-initialization"><a class="header" href="#gic-data-structure-initialization">GIC Data Structure Initialization</a></h3>
<ul>
<li>GIC is a global Once container used for the lazy initialization of the Gic structure, which contains the base addresses and sizes of GICD and GICR.</li>
<li>The primary_init_early and primary_init_late functions configure the GIC during the early and late initialization stages, enabling interrupts.</li>
</ul>
<h3 id="zone-level-initialization"><a class="header" href="#zone-level-initialization">Zone-Level Initialization</a></h3>
<p>In the Zone structure, the <code>arch_irqchip_reset</code> method is responsible for resetting all interrupts allocated to a specific zone by directly writing to the GICD's ICENABLER and ICACTIVER registers.</p>
<h2 id="2-vgicv3-module"><a class="header" href="#2-vgicv3-module">2. vGICv3 Module</a></h2>
<p>hvisor's VGICv3 (Virtual Generic Interrupt Controller version 3) module provides virtualization support for GICv3 in the ARMv8-A architecture. It controls and coordinates interrupt requests between different zones (virtual machine instances) through MMIO (Memory Mapped I/O) access and interrupt bitmaps management.</p>
<h3 id="mmio-region-registration"><a class="header" href="#mmio-region-registration">MMIO Region Registration</a></h3>
<p>During initialization, the <code>Zone</code> structure's <code>vgicv3_mmio_init</code> method registers the MMIO regions for the GIC Distributor (GICD) and each CPU's GIC Redistributor (GICR). MMIO region registration is completed through the <code>mmio_region_register</code> function, which associates specific processor or interrupt controller addresses with corresponding handling functions <code>vgicv3_dist_handler</code> and <code>vgicv3_redist_handler</code>.</p>
<h3 id="interrupt-bitmap-initialization"><a class="header" href="#interrupt-bitmap-initialization">Interrupt Bitmap Initialization</a></h3>
<p>The <code>Zone</code> structure's <code>irq_bitmap_init</code> method initializes the interrupt bitmap to track which interrupts belong to the current <code>zone</code>. Each interrupt is inserted into the bitmap by iterating through the provided interrupt list. The <code>insert_irq_to_bitmap</code> function is responsible for mapping specific interrupt numbers to the appropriate positions in the bitmap.</p>
<h3 id="mmio-access-restriction"><a class="header" href="#mmio-access-restriction">MMIO Access Restriction</a></h3>
<p>The <code>restrict_bitmask_access</code> function restricts MMIO access to the <code>GICD</code> registers, ensuring that only interrupts belonging to the current <code>zone</code> can be modified. The function checks whether the access is for the current zone's interrupts and, if so, updates the access mask to allow or restrict specific read/write operations.</p>
<h3 id="vgicv3-mmio-handling"><a class="header" href="#vgicv3-mmio-handling">VGICv3 MMIO Handling</a></h3>
<p>The <code>vgicv3_redist_handler</code> and <code>vgicv3_dist_handler</code> functions handle MMIO access for GICR and GICD, respectively. The <code>vgicv3_redist_handler</code> function handles read/write operations for GICR, checking whether the access is for the current <code>zone</code>'s GICR and allowing access if so; otherwise, the access is ignored. The <code>vgicv3_dist_handler</code> function calls <code>vgicv3_handle_irq_ops</code> or <code>restrict_bitmask_access</code> based on different types of GICD registers to appropriately handle interrupt routing and configuration register access.</p>
<p>Through these mechanisms, hvisor effectively manages interrupts across zones, ensuring that each zone can only access and control the interrupt resources allocated to it while providing necessary isolation. This allows VGICv3 to work efficiently and securely in a multi-zone environment, supporting complex virtualization scenarios.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="source-of-interruptions"><a class="header" href="#source-of-interruptions">Source of Interruptions</a></h1>
<p>In hvisor, there are three types of interrupts: timer interrupts, software interrupts, and external interrupts.</p>
<p>Timer Interrupt: A timer interrupt is generated when the time register becomes greater than the timecmp register.</p>
<p>Software Interrupt: In a multi-core system, one hart sends an inter-core interrupt to another hart, implemented through an SBI call.</p>
<p>External Interrupt: External devices send interrupt signals to the processor through interrupt lines.</p>
<h1 id="timer-interrupt"><a class="header" href="#timer-interrupt">Timer Interrupt</a></h1>
<p>When a virtual machine needs to trigger a timer interrupt, it traps into hvisor through the ecall instruction.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>        ExceptionType::ECALL_VS =&gt; {
            trace!("ECALL_VS");
            sbi_vs_handler(current_cpu);
            current_cpu.sepc += 4;
        }
        ...
pub fn sbi_vs_handler(current_cpu: &amp;mut ArchCpu) {
    let eid: usize = current_cpu.x[17];
    let fid: usize = current_cpu.x[16];
    let sbi_ret;
    match eid {
        ...
            SBI_EID::SET_TIMER =&gt; {
            sbi_ret = sbi_time_handler(fid, current_cpu);
        }
        ...
    }
}
<span class="boring">}</span></code></pre></pre>
<p>If the sstc extension is not enabled, it is necessary to trap into machine mode through an SBI call, set the mtimecmp register, clear the virtual machine's timer interrupt pending bit, and enable hvisor's timer interrupt; if the sstc extension is enabled, stimecmp can be set directly.</p>
<pre><code class="language-rs">pub fn sbi_time_handler(fid: usize, current_cpu: &amp;mut ArchCpu) -&gt; SbiRet {
...
    if current_cpu.sstc {
        write_csr!(CSR_VSTIMECMP, stime);
    } else {
        set_timer(stime);
        unsafe {
            // clear guest timer interrupt pending
            hvip::clear_vstip();
            // enable timer interrupt
            sie::set_stimer();
        }
    }
    return sbi_ret;
}
</code></pre>
<p>When the time register becomes greater than the timecmp register, a timer interrupt is generated.</p>
<p>After the interrupt is triggered, the trap context is saved, and dispatched to the corresponding handling function.</p>
<pre><code class="language-rs">        InterruptType::STI =&gt; {
            unsafe {
                hvip::set_vstip();
                sie::clear_stimer();
            }
        }
</code></pre>
<p>Set the virtual machine's timer interrupt pending bit to 1, injecting a timer interrupt into the virtual machine, and clear hvisor's timer interrupt enable bit to complete the interrupt handling.</p>
<h1 id="software-interrupt"><a class="header" href="#software-interrupt">Software Interrupt</a></h1>
<p>When a virtual machine needs to send an IPI, it traps into hvisor through the ecall instruction.</p>
<pre><code class="language-rs">        SBI_EID::SEND_IPI =&gt; {
            ...
            sbi_ret = sbi_call_5(
                eid,
                fid,
                current_cpu.x[10],
                current_cpu.x[11],
                current_cpu.x[12],
                current_cpu.x[13],
                current_cpu.x[14],
            );
        }
</code></pre>
<p>Then through an SBI call, trap into machine mode to send an IPI to the specified hart, setting the SSIP bit in the mip register to inject an inter-core interrupt into hvisor.</p>
<p>After the interrupt is triggered, the trap context is saved, and dispatched to the corresponding handling function.</p>
<pre><code class="language-rs">pub fn handle_ssi(current_cpu: &amp;mut ArchCpu) {
    ...
    clear_csr!(CSR_SIP, 1 &lt;&lt; 1);
    set_csr!(CSR_HVIP, 1 &lt;&lt; 2);
    check_events();
}
</code></pre>
<p>Set the virtual machine's software interrupt pending bit to 1, injecting a software interrupt into the virtual machine. Then determine the type of inter-core interrupt, wake or block the CPU, or handle VIRTIO-related interrupt requests.</p>
<h1 id="external-interrupt"><a class="header" href="#external-interrupt">External Interrupt</a></h1>
<h2 id="plic"><a class="header" href="#plic">PLIC</a></h2>
<p>RISC-V implements external interrupt handling through PLIC, which does not support virtualization or MSI.</p>
<p>The architectural diagram of PLIC.</p>
<p>The interrupt process of PLIC is shown in the following diagram.</p>
<p>The interrupt source sends an interrupt signal to the PLIC through the interrupt line, and only when the interrupt priority is greater than the threshold, it can pass through the threshold register's filter.</p>
<p>Then read the claim register to get the pending highest priority interrupt, then clear the corresponding pending bit. Pass it to the target hart for interrupt handling.</p>
<p>After handling, write the interrupt number to the complete register to receive the next interrupt request.</p>
<h2 id="initialization"><a class="header" href="#initialization">Initialization</a></h2>
<p>The initialization process is similar to AIA.</p>
<h2 id="handling-process"><a class="header" href="#handling-process">Handling Process</a></h2>
<p>When an external interrupt is triggered in the virtual machine, it will access the vPLIC address space, however, PLIC does not support virtualization, and this address space is unmapped. Therefore, a page fault exception will be triggered, trapping into hvisor for handling.</p>
<p>After the exception is triggered, the trap context is saved, and enters the page fault exception handling function.</p>
<pre><code class="language-rs">pub fn guest_page_fault_handler(current_cpu: &amp;mut ArchCpu) {
    ...
    if addr &gt;= host_plic_base &amp;&amp; addr &lt; host_plic_base + PLIC_TOTAL_SIZE {
        let mut inst: u32 = read_csr!(CSR_HTINST) as u32;
        ...
        if let Some(inst) = inst {
            if addr &gt;= host_plic_base + PLIC_GLOBAL_SIZE {
                vplic_hart_emul_handler(current_cpu, addr, inst);
            } else {
                vplic_global_emul_handler(current_cpu, addr, inst);
            }
            current_cpu.sepc += ins_size;
        } 
        ...
    }
}
</code></pre>
<p>Determine if the address where the page fault occurred is within the PLIC's address space, then parse the instruction that caused the exception, and modify the PLIC's address space based on the access address and instruction to emulate the configuration for vPLIC.</p>
<pre><code class="language-rs">pub fn vplic_hart_emul_handler(current_cpu: &amp;mut ArchCpu, addr: GuestPhysAddr, inst: Instruction) {
    ...
    if offset &gt;= PLIC_GLOBAL_SIZE &amp;&amp; offset &lt; PLIC_TOTAL_SIZE {
        ...
        if index == 0 {
            // threshold
            match inst {
                Instruction::Sw(i) =&gt; {
                    // guest write threshold register to plic core
                    let value = current_cpu.x[i.rs2() as usize] as u32;
                    host_plic.write().set_threshold(context, value);
                }
                _ =&gt; panic!("Unexpected instruction threshold {:?}", inst),
            }
            ...
        }
    }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overall-structure"><a class="header" href="#overall-structure">Overall Structure</a></h1>
<p>AIA mainly includes two parts, the Interrupt Message Controller IMSIC and the Advanced Platform Level Interrupt Controller APLIC, with the overall structure shown in the diagram below.</p>
<img src="chap04/subchap02/../img/riscv_aia_struct.jpg"  style="zoom: 50%;" />
<p>Peripherals can choose to send message interrupts or send wired interrupts via a connected line.</p>
<p>If peripheral A supports MSI, it only needs to write the specified data into the interrupt file of the designated hart, after which IMSIC will deliver an interrupt to the target processor.</p>
<p>For all devices, they can connect to APLIC via an interrupt line, and APLIC will choose the interrupt delivery mode based on the configuration:</p>
<ul>
<li>Wired interrupt</li>
<li>MSI</li>
</ul>
<p>In hvisor, the interrupt delivery mode is MSI.</p>
<p>After enabling the AIA specification with <code>IRQ=aia</code> in hvisor, the handling of clock interrupts remains consistent, while the handling of software interrupts and external interrupts changes.</p>
<h1 id="external-interrupts"><a class="header" href="#external-interrupts">External Interrupts</a></h1>
<h2 id="imsic"><a class="header" href="#imsic">IMSIC</a></h2>
<p>In hvisor, a physical CPU corresponds to a virtual CPU, each having their own interrupt file.</p>
<img src="chap04/subchap02/../img/riscv_aia_intfile.png"  style="zoom: 80%;" />
<p>Writing to an interrupt file can trigger an external interrupt for a specified hart at a specified privilege level.</p>
<p>Provide a two-stage address mapping table for IMSIC.</p>
<pre><code class="language-rs">        let paddr = 0x2800_0000 as HostPhysAddr;
        let size = PAGE_SIZE;
        self.gpm.insert(MemoryRegion::new_with_offset_mapper(
            paddr as GuestPhysAddr,
            paddr + PAGE_SIZE * 1,
            size,
            MemFlags::READ | MemFlags::WRITE,
        ))?;
        ...
</code></pre>
<h2 id="aplic"><a class="header" href="#aplic">APLIC</a></h2>
<h3 id="structure"><a class="header" href="#structure">Structure</a></h3>
<p>There is only one global APLIC.</p>
<p>When a wired interrupt arrives, it first reaches the root interrupt domain in machine mode (OpenSBI), then the interrupt is routed to the sub-interrupt domain (hvisor), and hvisor sends the interrupt signal to the corresponding CPU of the virtual machine in MSI mode according to the target registers configured by APLIC.</p>
<img src="chap04/subchap02/../img/riscv_aia_aplicdomain.png"  style="zoom: 70%;" />
<p>The AIA specification manual specifies the byte offsets for various fields of APLIC. Define the APLIC structure as follows, and implement read and write operations for APLIC fields using the following methods:</p>
<pre><code class="language-rs">#[repr(C)]
pub struct Aplic {
    pub base: usize,
    pub size: usize,
}
impl Aplic {
    pub fn new(base: usize, size: usize) -&gt; Self {
        Self {
            base,
            size,
        }
    }
    pub fn read_domaincfg(&amp;self) -&gt; u32{
        let addr = self.base + APLIC_DOMAINCFG_BASE;
        unsafe { core::ptr::read_volatile(addr as *const u32) }
    }
    pub fn set_domaincfg(&amp;self, bigendian: bool, msimode: bool, enabled: bool){
        ...
        let addr = self.base + APLIC_DOMAINCFG_BASE;
        let src = (enabled &lt;&lt; 8) | (msimode &lt;&lt; 2) | bigendian;
        unsafe {
            core::ptr::write_volatile(addr as *mut u32, src);
        }
    }
    ...
}
</code></pre>
<h3 id="initialization-1"><a class="header" href="#initialization-1">Initialization</a></h3>
<p>Initialize APLIC based on the base address and size in the device tree.</p>
<pre><code class="language-rs">pub fn primary_init_early(host_fdt: &amp;Fdt) {
    let aplic_info = host_fdt.find_node("/soc/aplic").unwrap();
    init_aplic(
        aplic_info.reg().unwrap().next().unwrap().starting_address as usize,
        aplic_info.reg().unwrap().next().unwrap().size.unwrap(),
    );
}
pub fn init_aplic(aplic_base: usize, aplic_size: usize) {
    let aplic = Aplic::new(aplic_base, aplic_size);
    APLIC.call_once(|| RwLock::new(aplic));
}
pub static APLIC: Once&lt;RwLock&lt;Aplic&gt;&gt; = Once::new();
pub fn host_aplic&lt;'a&gt;() -&gt; &amp;'a RwLock&lt;Aplic&gt; {
    APLIC.get().expect("Uninitialized hypervisor aplic!")
}
</code></pre>
<p>There is only one global APLIC, so locking is used to avoid read-write conflicts, and the host_aplic() method is used for access.</p>
<p>When the virtual machine starts, the address space of APLIC is initialized, which is unmapped. This triggers a page fault, trapping into hvisor for handling.</p>
<pre><code class="language-rs">pub fn guest_page_fault_handler(current_cpu: &amp;mut ArchCpu) {
    ...
    if addr &gt;= host_aplic_base &amp;&amp; addr &lt; host_aplic_base + host_aplic_size {
        let mut inst: u32 = read_csr!(CSR_HTINST) as u32;
        ...
        if let Some(inst) = inst {
                vaplic_emul_handler(current_cpu, addr, inst);
                current_cpu.sepc += ins_size;
            }
        ...
    }
}
</code></pre>
<p>Determine if the accessed address space belongs to APLIC, parse the access instruction, and enter vaplic_emul_handler to simulate APLIC in the virtual machine.</p>
<pre><code class="language-rs">pub fn vaplic_emul_handler(
    current_cpu: &amp;mut ArchCpu,
    addr: GuestPhysAddr,
    inst: Instruction,
) {
    let host_aplic = host_aplic();
    let offset = addr.wrapping_sub(host_aplic.read().base);
    if offset &gt;= APLIC_DOMAINCFG_BASE &amp;&amp; offset &lt; APLIC_SOURCECFG_BASE {
        match inst {
            Instruction::Sw(i) =&gt; {
                ...
                host_aplic.write().set_domaincfg(bigendian, msimode, enabled);
            }
            Instruction::Lw(i) =&gt; {
                let value = host_aplic.read().read_domaincfg();
                current_cpu.x[i.rd() as usize] = value as usize;
            }
            _ =&gt; panic!("Unexpected instruction {:?}", inst),
        }
    }
    ...
}
</code></pre>
<h2 id="interrupt-process"><a class="header" href="#interrupt-process">Interrupt Process</a></h2>
<p>After hvisor completes the simulation of APLIC initialization for the virtual machine through a page fault, it enters the virtual machine. Taking the interrupt generated by a keyboard press as an example: the interrupt signal first arrives at OpenSBI, then is routed to hvisor, and based on the configuration of the target register, it writes to the virtual interrupt file to trigger an external interrupt in the virtual machine.</p>
<h1 id="software-interrupts"><a class="header" href="#software-interrupts">Software Interrupts</a></h1>
<p>After enabling the AIA specification, the Linux kernel of the virtual machine sends IPIs through MSI, eliminating the need to trap into hvisor using the ecall instruction.</p>
<img src="chap04/subchap02/../img/riscv_aia_ipi.jpg"  style="zoom:40%;" />
<p>As shown in the diagram, in hvisor, writing to the interrupt file of a specified hart can trigger an IPI.</p>
<p>In the virtual machine, writing to a specified virtual interrupt file can achieve IPIs within the virtual machine, without the need for simulation support from hvisor.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loongarch-interrupt-control"><a class="header" href="#loongarch-interrupt-control">LoongArch Interrupt Control</a></h1>
<p>Due to the different interrupt controllers designed for different Loongson processors/development boards (embedded processors like 2K1000 have their own interrupt controller designs, while the 3 series processors have the 7A1000 and 7A2000 bridge chips responsible for external interrupt control), this article mainly introduces the interrupt controller inside the latest <strong>Loongson 7A2000 bridge chip</strong>[1].</p>
<h2 id="cpu-interrupts"><a class="header" href="#cpu-interrupts">CPU Interrupts</a></h2>
<p>The interrupt configuration of LoongArch is controlled by <code>CSR.ECFG</code>. The interrupts under the Loongson architecture use line interrupts, and each processor core can record 13 line interrupts. These interrupts include: 1 inter-core interrupt (IPI), 1 timer interrupt (TI), 1 performance monitoring counter overflow interrupt (PMI), 8 hardware interrupts (HWI0~HWI7), and 2 software interrupts (SWI0~SWI1). All line interrupts are level interrupts and are active high[3].</p>
<ul>
<li><strong>Inter-core Interrupt</strong>: Comes from an external interrupt controller and is recorded in the <code>CSR.ESTAT.IS[12]</code> bit.</li>
<li><strong>Timer Interrupt</strong>: Originates from an internal constant frequency timer, triggered when the timer counts down to zero, and recorded in the <code>CSR.ESTAT.IS[11]</code> bit. It is cleared by writing 1 to the <code>TI</code> bit of the <code>CSR.TICLR</code> register through software.</li>
<li><strong>Performance Counter Overflow Interrupt</strong>: Comes from an internal performance counter, triggered when any performance counter enabled for interrupts has its 63rd bit set to 1, and recorded in the <code>CSR.ESTAT.IS[10]</code> bit. It is cleared by either clearing the 63rd bit of the performance counter causing the interrupt or disabling the interrupt enable of that performance counter.</li>
<li><strong>Hardware Interrupts</strong>: Come from an external interrupt controller outside the processor core, 8 hardware interrupts <code>HWI[7:0]</code> are recorded in the <code>CSR.ESTAT.IS[9:2]</code> bits.</li>
<li><strong>Software Interrupts</strong>: Originating from within the processor core, triggered by writing 1 to the <code>CSR.ESTAT.IS[1:0]</code> through software instructions, cleared by writing 0.</li>
</ul>
<p>The index value recorded in the <code>CSR.ESTAT.IS</code> field is also referred to as the interrupt number (Int Number). For example, the interrupt number for <code>SWI0</code> is 0, for <code>SWI1</code> is 1, and so on, with <code>IPI</code> being 12.</p>
<h2 id="traditional-io-interrupts"><a class="header" href="#traditional-io-interrupts">Traditional IO Interrupts</a></h2>
<p>The diagram above shows the interrupt system of the 3A series processor + 7A series bridge chip. It illustrates two types of interrupt processes, the upper part shows the interrupt through the <code>INTn0</code> line, and the lower part shows the interrupt through an HT message packet.</p>
<p>Interrupts <code>intX</code> issued by devices (except for PCIe devices working in MSI mode) are sent to the internal interrupt controller of 7A, after interrupt routing, they are sent to the bridge chip pins or converted into HT message packets sent to the 3A's HT controller. The 3A's interrupt controller receives the interrupt through external interrupt pins or HT controller and routes it to interrupt a specific processor core[1].</p>
<p>The <strong>traditional IO interrupts</strong> of the Loongson 3A5000 chip support 32 interrupt sources, managed in a unified manner as shown in the diagram below.
Any IO interrupt source can be configured to enable/disable, trigger mode, and the target processor core interrupt pin to which it is routed. Traditional interrupts <em>do not support</em> cross-chip distribution of interrupts; they can only interrupt processor cores within the same processor chip[2].</p>
<h2 id="extended-io-interrupts"><a class="header" href="#extended-io-interrupts">Extended IO Interrupts</a></h2>
<p>In addition to the existing traditional IO interrupt mode, starting with 3A5000, <strong>extended I/O interrupts</strong> are supported, which distribute the 256-bit interrupts on the HT bus directly to each processor core without going through the HT interrupt line, enhancing the flexibility of IO interrupt usage[2].</p>
<h2 id="references-3"><a class="header" href="#references-3">References</a></h2>
<p>[1] Loongson Technology Corporation Limited. Loongson 7A2000 Bridge Chip User Manual. V1.0. Chapter 5.</p>
<p>[2] Loongson Technology Corporation Limited. Loongson 3A5000/3B5000 Processor Register Usage Manual - Multi-core Processor Architecture, Register Description and System Software Programming Guide. V1.3. Chapter 11.</p>
<p>[3] Loongson Technology Corporation Limited. Loongson Architecture Reference Manual. Volume One: Basic Architecture.</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="arm-smmu-technical-documentation"><a class="header" href="#arm-smmu-technical-documentation">ARM-SMMU Technical Documentation</a></h1>
<p>Abstract: Introducing the development process of ARM-SMMU.</p>
<h2 id="background-knowledge"><a class="header" href="#background-knowledge">Background Knowledge</a></h2>
<p>A brief introduction to the principle and function of SMMU.</p>
<h3 id="what-is-dma-why-is-iommu-needed"><a class="header" href="#what-is-dma-why-is-iommu-needed">What is DMA? Why is IOMMU needed?</a></h3>
<p>Virtual machines running on top of the hypervisor need to interact with devices, but if they wait for the CPU to host this work every time, it would reduce processing efficiency. Therefore, the DMA mechanism appears. <strong>DMA is a mechanism that allows devices to exchange data directly with memory without CPU involvement.</strong></p>
<p>We can roughly figure out the process of virtual machines interacting with devices through DMA. First, the virtual machine issues a DMA request, telling the target device where to write the data, and then the device writes to the memory according to the address.</p>
<p>However, some issues need to be considered in the above process:</p>
<ul>
<li>The hypervisor has virtualized memory for each virtual machine, so the target memory address of the DMA request issued by the virtual machine is GPA, also called IOVA here, which needs to be converted to the real PA to be written to the correct position in physical memory.</li>
<li>Moreover, if the range of IOVA is not restricted, it means that any memory address can be accessed through the DMA mechanism, causing unforeseen severe consequences.</li>
</ul>
<p>Therefore, we need an institution that can help us with address conversion and ensure the legality of the operation address, just like the MMU memory management unit. This institution is called <strong>IOMMU</strong>, and it has another name in the Arm architecture called <strong>SMMU</strong> (referred to as SMMU hereafter).</p>
<p>Now you know that SMMU can convert virtual addresses to physical addresses, thus ensuring the legality of devices directly accessing memory.</p>
<h3 id="specific-tasks-of-smmu"><a class="header" href="#specific-tasks-of-smmu">Specific Tasks of SMMU</a></h3>
<p>As mentioned above, the function of SMMU is similar to MMU, whose target is virtual machines or applications, while the target of SMMU is each device. Each device is identified by a sid, and the corresponding table is called <strong>stream table</strong>. This table uses the device's sid as an index, and the sid of PCI devices can be obtained from the BDF number: sid = (B &lt;&lt; 5) | (D &lt;&lt; 3) | F.</p>
<h2 id="development-work"><a class="header" href="#development-work">Development Work</a></h2>
<p>Currently, we have implemented support for stage-2 address translation of SMMUv3 in Qemu, created a simple linear table, and conducted simple verification using PCI devices.</p>
<p>The work of IOMMU has not yet been merged into the mainline, and you can switch to the IOMMU branch to check.</p>
<h3 id="overall-idea"><a class="header" href="#overall-idea">Overall Idea</a></h3>
<p>We pass through the PCI HOST to zone0, that is, add the PCI node to the device tree provided to zone0, map the corresponding memory address in the second-stage page table of zone0, and ensure normal interrupt injection. Then zone0 will detect and configure the PCI device by itself, and we only need to configure SMMU in the hypervisor.</p>
<h3 id="qemu-parameters"><a class="header" href="#qemu-parameters">Qemu Parameters</a></h3>
<p>Add <code>iommu=smmuv3</code> in <code>machine</code> to enable SMMUv3 support, and add <code>arm-smmuv3.stage=2</code> in <code>global</code> to enable second-stage address translation.</p>
<p>Note that nested translation is not yet supported in Qemu. If <code>stage=2</code> is not specified, only the first stage of address translation is supported by default. Please use Qemu version 8.1 or above, as lower versions do not support enabling second-stage address translation.</p>
<p>When adding PCI devices, please ensure to enable <code>iommu_platform=on</code>.</p>
<p>The addr can specify the bdf number of the device.</p>
<p><strong>In the PCI bus simulated by Qemu, in addition to the PCI HOST, there is a default network card device, so the addr parameter of other added devices must start from 2.0.</strong></p>
<pre><code>// scripts/qemu-aarch64.mk

QEMU_ARGS := -machine virt,secure=on,gic-version=3,virtualization=on,iommu=smmuv3
QEMU_ARGS += -global arm-smmuv3.stage=2

QEMU_ARGS += -device virtio-blk-pci,drive=Xa003e000,disable-legacy=on,disable-modern=off,iommu_platform=on,addr=2.0
</code></pre>
<h3 id="mapping-smmu-related-memory-in-the-hypervisors-page-table"><a class="header" href="#mapping-smmu-related-memory-in-the-hypervisors-page-table">Mapping SMMU-related Memory in the Hypervisor's Page Table</a></h3>
<p>Consulting the Qemu source code reveals that the memory area corresponding to VIRT_SMMU starts at 0x09050000 and is 0x20000 in size. We need to access this area, so it must be mapped in the hypervisor's page table.</p>
<pre><code>// src/arch/aarch64/mm.rs

pub fn init_hv_page_table(fdt: &amp;fdt::Fdt) -&gt; HvResult {
    hv_pt.insert(MemoryRegion::new_with_offset_mapper(
        smmuv3_base(),
        smmuv3_base(),
        smmuv3_size(),
        MemFlags::READ | MemFlags::WRITE,
    ))?;
}
</code></pre>
<h3 id="smmuv3-data-structure"><a class="header" href="#smmuv3-data-structure">SMMUv3 Data Structure</a></h3>
<p>This structure contains a reference to the memory area of SMMUv3 that will be accessed, whether it supports secondary tables, the maximum number of bits of sid, and the base address and allocated page frames of the stream table.</p>
<p>The rp is a reference to the defined <code>RegisterPage</code>, which is set according to the offsets in Chapter 6 of the SMMUv3 manual. Readers can refer to it on their own.</p>
<pre><code>// src/arch/aarch64/iommu.rs

pub struct Smmuv3{
    rp:&amp;'static RegisterPage,

    strtab_2lvl:bool,
    sid_max_bits:usize,

    frames:Vec&lt;Frame&gt;,

    // strtab
    strtab_base:usize,

    // about queues...
}
</code></pre>
<h3 id="new"><a class="header" href="#new">new()</a></h3>
<p>After completing the mapping work, we can refer to the corresponding register area.</p>
<pre><code>impl Smmuv3{
    fn new() -&gt; Self{
        let rp = unsafe {
            &amp;*(SMMU_BASE_ADDR as *const RegisterPage)
        };

        let mut r = Self{
            ...
        };

        r.check_env();

        r.init_structures();

        r.device_reset();

        r
    }
}
</code></pre>
<h3 id="check_env"><a class="header" href="#check_env">check_env()</a></h3>
<p>Check which stage of address translation the current environment supports, what type of stream table it supports, how many bits of sid it supports, etc.</p>
<p>Taking the check of which table format the environment supports as an example, the supported table type is in the <code>IDR0</code> register. Obtain the value of <code>IDR0</code> by <code>self.rp.IDR0.get() as usize</code>, and use <code>extract_bit</code> to extract and get the value of the <code>ST_LEVEL</code> field. According to the manual, 0b00 represents support for linear tables, 0b01 represents support for linear tables and secondary tables, and 0b1x is a reserved bit. We can choose what type of stream table to create based on this information.</p>
<pre><code>impl Smmuv3{
    fn check_env(&amp;mut self){
        let idr0 = self.rp.IDR0.get() as usize;

        info!("Smmuv3 IDR0:{:b}",idr0);

        // supported types of stream tables.
        let stb_support = extract_bits(idr0, IDR0_ST_LEVEL_OFF, IDR0_ST_LEVEL_LEN);
        match stb_support{
            0 =&gt; info!("Smmuv3 Linear Stream Table Supported."),
            1 =&gt; {info!("Smmuv3 2-level Stream Table Supported.");
                self.strtab_2lvl = true;
            }
            _ =&gt; info!("Smmuv3 doesn't support any stream table."),
        }

	...
    }
}
</code></pre>
<h3 id="init_linear_strtab"><a class="header" href="#init_linear_strtab">init_linear_strtab()</a></h3>
<p>We need to support the second stage of address translation, and there are not many devices in the system, so we choose to use a linear table.</p>
<p>When applying for the space needed for the linear table, we should calculate the number of table entries based on the current maximum number of bits of sid, multiplied by the space needed for each table entry <code>STRTAB_STE_SIZE</code>, to know how many page frames need to be applied for. However, SMMUv3 has strict requirements for the starting address of the stream table. The low (5+sid_max_bits) bits of the starting address must be 0.</p>
<p>Since the current hypervisor does not support applying for space in this way, we apply for a space under safe conditions and select an address that meets the conditions as the table base address within this space, although this will cause some space waste.</p>
<p>After applying for space, we can fill in this table's base address into the <code>STRTAB_BASE</code> register:</p>
<pre><code>	let mut base = extract_bits(self.strtab_base, STRTAB_BASE_OFF, STRTAB_BASE_LEN);
	base = base &lt;&lt; STRTAB_BASE_OFF;
	base |= STRTAB_BASE_RA;
	self.rp.STRTAB_BASE.set(base as _);
</code></pre>
<p>Next, we also need to set the <code>STRTAB_BASE_CFG</code> register to indicate the format of the table we are using, whether it is a linear table or a secondary table, and the number of table items (represented in LOG2 form, i.e., the maximum number of bits of SID):</p>
<pre><code>        // format: linear table
        cfg |= STRTAB_BASE_CFG_FMT_LINEAR &lt;&lt; STRTAB_BASE_CFG_FMT_OFF;

        // table size: log2(entries)
        // entry_num = 2^(sid_bits)
        // log2(size) = sid_bits
        cfg |= self.sid_max_bits &lt;&lt; STRTAB_BASE_CFG_LOG2SIZE_OFF;

        // linear table -&gt; ignore SPLIT field
        self.rp.STRTAB_BASE_CFG.set(cfg as _);
</code></pre>
<h3 id="init_bypass_stesidusize"><a class="header" href="#init_bypass_stesidusize">init_bypass_ste(sid:usize)</a></h3>
<p>Currently, we have not configured any relevant information yet, so we need to set all table entries to the default state first.</p>
<p>For each sid, find the address of the table entry based on the table base address, i.e., the valid bit is 0, and the address translation is set to <code>BYPASS</code>.</p>
<pre><code>	let base = self.strtab_base + sid * STRTAB_STE_SIZE;
	let tab = unsafe{&amp;mut *(base as *mut [u64;STRTAB_STE_DWORDS])};

	let mut val:usize = 0;
	val |= STRTAB_STE_0_V;
	val |= STRTAB_STE_0_CFG_BYPASS &lt;&lt; STRTAB_STE_0_CFG_OFF;
</code></pre>
<h3 id="device_reset"><a class="header" href="#device_reset">device_reset()</a></h3>
<p>We have done some preparatory work above, but some additional configurations are still needed, such as enabling SMMU, otherwise, SMMU will be in a disabled state.</p>
<pre><code>	let cr0 = CR0_SMMUEN;
	self.rp.CR0.set(cr0 as _);
</code></pre>
<h3 id="write_stesidusizevmidusizeroot_ptusize"><a class="header" href="#write_stesidusizevmidusizeroot_ptusize">write_ste(sid:usize,vmid:usize,root_pt:usize)</a></h3>
<p>This method is used to configure specific device information.</p>
<p>First, we need to find the address of the corresponding table entry based on sid.</p>
<pre><code>	let base = self.strtab_base + sid * STRTAB_STE_SIZE;
        let tab = unsafe{&amp;mut *(base as *mut [u64;STRTAB_STE_DWORDS])};
</code></pre>
<p>In the second step, we need to indicate that the information related to this device is used for the second stage of address translation, and this table entry is now valid.</p>
<pre><code>        let mut val0:usize = 0;
        val0 |= STRTAB_STE_0_V;
        val0 |= STRTAB_STE_0_CFG_S2_TRANS &lt;&lt; STRTAB_STE_0_CFG_OFF;
</code></pre>
<p>In the third step, we need to specify which virtual machine this device is allocated to, and enable the second-stage page table traversal, <code>S2AA64</code> represents that the second-stage translation table is based on aarch64, <code>S2R</code> represents enabling error recording.</p>
<pre><code>        let mut val2:usize = 0;
        val2 |= vmid &lt;&lt; STRTAB_STE_2_S2VMID_OFF;
        val2 |= STRTAB_STE_2_S2PTW;
        val2 |= STRTAB_STE_2_S2AA64;
        val2 |= STRTAB_STE_2_S2R;
</code></pre>
<p>The last step is to point out the basis for the second-stage translation, which is the page table of the corresponding virtual machine in the hypervisor. Just fill in the base address of the page table in the corresponding position, i.e., the <code>S2TTB</code> field.</p>
<p>Here we also need to explain the configuration information of this page table, so that SMMU knows the format and other information of this page table and can use this page table, i.e., the <code>VTCR</code> field.</p>
<pre><code>	let vtcr = 20 + (2&lt;&lt;6) + (1&lt;&lt;8) + (1&lt;&lt;10) + (3&lt;&lt;12) + (0&lt;&lt;14) + (4&lt;&lt;16);
        let v = extract_bits(vtcr as _, 0, STRTAB_STE_2_VTCR_LEN);
        val2 |= v &lt;&lt; STRTAB_STE_2_VTCR_OFF;

        let vttbr = extract_bits(root_pt, STRTAB_STE_3_S2TTB_OFF, STRTAB_STE_3_S2TTB_LEN);
</code></pre>
<h2 id="initialization-and-device-allocation"><a class="header" href="#initialization-and-device-allocation">Initialization and Device Allocation</a></h2>
<p>In <code>src/main.rs</code>, after the hypervisor's page table is initialized (mapping the SMMU-related area), SMMU can be initialized.</p>
<pre><code>fn primary_init_early(dtb: usize) {
    ...

    crate::arch::mm::init_hv_page_table(&amp;host_fdt).unwrap();

    info!("Primary CPU init hv page table OK.");

    iommu_init();

    zone_create(0,ROOT_ENTRY,ROOT_ZONE_DTB_ADDR as _, DTB_IPA).unwrap();
    INIT_EARLY_OK.store(1, Ordering::Release);
}
</code></pre>
<p>Next, we need to allocate devices, which we complete synchronously when creating the virtual machine. Currently, we only allocate devices for zone0 to use.</p>
<pre><code>// src/zone.rs

pub fn zone_create(
    zone_id: usize,
    guest_entry: usize,
    dtb_ptr: *const u8,
    dtb_ipa: usize,
) -&gt; HvResult&lt;Arc&lt;RwLock&lt;Zone&gt;&gt;&gt; {
    ...

    if zone_id==0{
        // add_device(0, 0x8, zone.gpm.root_paddr());
        iommu_add_device(zone_id, BLK_PCI_ID, zone.gpm.root_paddr());
    }
  
    ...
}
</code></pre>
<h2 id="simple-verification"><a class="header" href="#simple-verification">Simple Verification</a></h2>
<p>Start qemu with the parameter <code>-trace smmuv3_*</code> to see related outputs:</p>
<pre><code>smmuv3_config_cache_hit Config cache HIT for sid=0x10 (hits=1469, misses=1, hit rate=99)
smmuv3_translate_success smmuv3-iommu-memory-region-16-2 sid=0x10 iova=0x8e043242 translated=0x8e043242 perm=0x3
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h3 id="4612-implementation-of-the-risc-v-iommu-standard"><a class="header" href="#4612-implementation-of-the-risc-v-iommu-standard">4.6.1.2 Implementation of the RISC-V IOMMU Standard</a></h3>
<h4 id="risc-v-iommu-workflow"><a class="header" href="#risc-v-iommu-workflow">RISC-V IOMMU Workflow</a></h4>
<p>For virtualized systems with DMA devices, the system could potentially be compromised by malicious DMA configurations by the virtual machines, affecting the overall system stability. The introduction of IOMMU can further enhance the isolation between Zones to ensure system security.</p>
<p>IOMMU supports two-stage address translation and provides DMA remapping functionality. On one hand, it offers memory protection for DMA operations, limiting the physical memory areas that devices can access, making DMA operations safer. On the other hand, device DMA operations only require contiguous IOVAs, not contiguous PAs, allowing for better utilization of scattered pages in physical memory.</p>
<p>To perform address translation and memory protection, RISC-V IOMMU uses the same page table format as the CPU's MMU in both the first and second stages. Using the same page table format as the CPU MMU simplifies some of the complexities associated with DMA in memory management, and allows the CPU MMU and IOMMU to share the same page tables.</p>
<p>In hvisor, the second-stage address translation process supported by IOMMU is the translation from device-side IOVA (GPA) to HPA, and the second-stage page table is shared between the CPU MMU and IOMMU, as illustrated below:</p>
<p><img src="chap04/subchap03/IOMMU/../../img/riscv_iommu_struct.png" alt="riscv_iommu_struct.png" /></p>
<p>Before translation, IOMMU needs to find the device context (DC) in the device directory table based on the device identifier (device_id). Each device has a unique device_id; for platform devices, the device_id is specified during hardware implementation, and for PCI/PCIe devices, the BDF number of the PCI/PCIe device is used as the device_id. The DC contains information such as the base address of the two-stage address translation page table and some translation control information. For example, in two-stage address translation, the IO device's IOVA is first translated into GPA in the Stage-1 page table pointed to by the fsc field, then into HPA in the Stage-2 page table pointed to by the iohgatp field, and accesses memory accordingly. In hvisor, only the second-stage translation using the iohgatp field is supported, as shown below:</p>
<p><img src="chap04/subchap03/IOMMU/../../img/riscv_iommu_translate.png" alt="riscv_iommu_translate.png" /></p>
<p>RISC-V IOMMU, as a physical hardware, can be accessed using the MMIO method, and its various fields' byte offsets are specified in the IOMMU specification manual. Implementation requires access according to the specified offsets and sizes to correctly retrieve the values of each field. The IommuHw structure is defined to simplify access to the physical IOMMU, as follows:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[repr(C)]
#[repr(align(0x1000))]
pub struct IommuHw {
    caps: u64,
    fctl: u32,
    __custom1: [u8; 4],
    ddtp: u64,
    cqb: u64,
    cqh: u32,
    cqt: u32,
    fqb: u64,
    fqh: u32,
    fqt: u32,
    pqb: u64,
    pqh: u32,
    pqt: u32,
    cqcsr: u32,
    fqcsr: u32,
    pqcsr: u32,
    ipsr: u32,
    iocntovf: u32,
    iocntinh: u32,
    iohpmcycles: u64,
    iohpmctr: [u64; 31],
    iohpmevt: [u64; 31],
    tr_req_iova: u64,
    tr_req_ctl: u64,
    tr_response: u64,
    __rsv1: [u8; 64],
    __custom2: [u8; 72],
    icvec: u64,
    msi_cfg_tbl: [MsiCfgTbl; 16],
    __rsv2: [u8;3072],
}
<span class="boring">}</span></code></pre></pre>
<p>The Capabilities of the IOMMU is a read-only register, which reports the features supported by the IOMMU. When initializing the IOMMU, it is necessary to first check this register to determine if the hardware can support IOMMU functionality.</p>
<p>When initializing, the IOMMU first checks if the current IOMMU matches the driver. The rv_iommu_check_features function is defined to check for hardware support for Sv39x4, WSI, etc., as implemented below:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl IommuHw {
    pub fn rv_iommu_check_features(&amp;self){
        let caps = self.caps as usize;
        let version = caps &amp; RV_IOMMU_CAPS_VERSION_MASK;
        // get version, version 1.0 -&gt; 0x10
        if version != RV_IOMMU_SUPPORTED_VERSION{
            error!("RISC-V IOMMU unsupported version: {}", version);
        }
        // support SV39x4
        if caps &amp; RV_IOMMU_CAPS_SV39X4_BIT == 0 {
            error!("RISC-V IOMMU HW does not support Sv39x4");
        }
        if caps &amp; RV_IOMMU_CAPS_MSI_FLAT_BIT == 0 {
            error!("RISC-V IOMMU HW does not support MSI Address Translation (basic-translate mode)");
        }
        if caps &amp; RV_IOMMU_CAPS_IGS_MASK == 0 {
            error!("RISC-V IOMMU HW does not support WSI generation");
        }
        if caps &amp; RV_IOMMU_CAPS_AMO_HWAD_BIT == 0 {
            error!("RISC-V IOMMU HW AMO HWAD unsupport");
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>The fctl of the IOMMU is a functional control register, which provides some functional controls of the IOMMU, including whether the IOMMU accesses memory data in big-endian or little-endian, whether the interrupt generated by the IOMMU is a WSI interrupt or an MSI interrupt, and the control of the guest address translation scheme.</p>
<p>The ddtp of the IOMMU is the device directory table pointer register, which contains the PPN of the root page of the device directory table, as well as the IOMMU Mode, which can be configured as Off, Bare, 1LVL, 2LVL, or 3LVL. Off means that the IOMMU does not allow devices to access memory, Bare means that the IOMMU allows all memory accesses by devices without translation and protection, and 1LVL, 2LVL, 3LVL indicate the levels of the device directory table used by the IOMMU.</p>
<p>The rv_iommu_init function is defined for functional checks and controls of the physical IOMMU, such as configuring interrupts as WSI, configuring the device directory table, etc., as implemented below:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl IommuHw {
	pub fn rv_iommu_init(&amp;mut self, ddt_addr: usize){
        // Read and check caps
        self.rv_iommu_check_features();
        // Set fctl.WSI We will be first using WSI as IOMMU interrupt mechanism
        self.fctl = RV_IOMMU_FCTL_DEFAULT;
        // Clear all IP flags (ipsr)
        self.ipsr = RV_IOMMU_IPSR_CLEAR;
        // Configure ddtp with DDT base address and IOMMU mode
        self.ddtp = IOMMU_MODE as u64 | ((ddt_addr &gt;&gt; 2) &amp; RV_IOMMU_DDTP_PPN_MASK) as u64;    
    }
}
<span class="boring">}</span></code></pre></pre>
<p>The format of the entries in the device directory table is given in the specification manual. To make the hardware work, it is necessary to implement it in accordance with the specification. The DdtEntry structure is defined to represent an entry in the device directory table, representing a DMA device. The iohgatp saves the PPN of the second-stage page table, the Guest Software Context ID (GSCID), and the Mode field used to select the second-stage address translation scheme. The tc contains many translation control-related bits, most of which are not used in hvisor, and the valid bits need to be set to 1 for subsequent higher-level functional extensions. The structure of the device directory table entry is as follows:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[repr(C)]
struct DdtEntry{
    tc: u64,
    iohgatp: u64,
    ta: u64,
    fsc: u64,
    msiptp: u64,
    msi_addr_mask: u64,
    msi_addr_pattern: u64,
    __rsv: u64,
}
<span class="boring">}</span></code></pre></pre>
<p>Currently, hvisor only supports a single-level device directory table. The Lvl1DdtHw structure is defined to facilitate access to the device directory table entries. A single-level device directory table can support 64 DMA devices, occupying one physical page, as follows:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Lvl1DdtHw{
    dc: [DdtEntry; 64],
}
<span class="boring">}</span></code></pre></pre>
<p>The Iommu structure is defined as a higher-level abstraction of the IOMMU, where base is the base address of IommuHw, i.e., the physical address of the IOMMU, which can be used to access the physical IOMMU. ddt is the device directory table, which needs to be allocated physical pages during IOMMU initialization. Since it only supports a single-level device directory table, only one physical page is needed, as defined below:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Iommu{
    pub base: usize,
    pub ddt: Frame,		// Lvl1 DDT -&gt; 1 phys page
}
<span class="boring">}</span></code></pre></pre>
<p>The device directory table and translation page table of the IOMMU are stored in memory and need to be allocated according to actual needs, i.e., the memory of the device directory table needs to be allocated when new is called. In addition, adding device entries to the device directory table is a very important task, because DMA devices perform DMA operations, and the first step is to find the translation page table and other information from the device directory table, and then the IOMMU performs translation based on the page table-related information. The tc, iohgatp, etc. need to be filled in, as implemented below:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl Iommu {
    pub fn new(base: usize) -&gt; Self{
        Self { 
            base: base,
            ddt: Frame::new_zero().unwrap(),
        }
    }

    pub fn iommu(&amp;self) -&gt; &amp;mut IommuHw{
        unsafe { &amp;mut *(self.base as *mut _) }
    }

    pub fn dc(&amp;self) -&gt; &amp;mut Lvl1DdtHw{
        unsafe { &amp;mut *(self.ddt.start_paddr() as *mut _)}
    }

    pub fn rv_iommu_init(&amp;mut self){
        self.iommu().rv_iommu_init(self.ddt.start_paddr());
    }

    pub fn rv_iommu_add_device(&amp;self, device_id: usize, vm_id: usize, root_pt: usize){
        // only support 64 devices
        if device_id &gt; 0 &amp;&amp; device_id &lt; 64{
            // configure DC
            let tc: u64 = 0 | RV_IOMMU_DC_VALID_BIT as u64 | 1 &lt;&lt; 4;
            self.dc().dc[device_id].tc = tc;
            let mut iohgatp: u64 = 0;
            iohgatp |= (root_pt as u64 &gt;&gt; 12) &amp; RV_IOMMU_DC_IOHGATP_PPN_MASK as u64;
            iohgatp |= (vm_id as u64) &amp; RV_IOMMU_DC_IOHGATP_GSCID_MASK as u64;
            iohgatp |= RV_IOMMU_IOHGATP_SV39X4 as u64;
            self.dc().dc[device_id].iohgatp = iohgatp;
            self.dc().dc[device_id].fsc = 0;
            info!("{:#x}", &amp;mut self.dc().dc[device_id] as *mut _ as usize);
            info!("RV IOMMU: Write DDT, add decive context, iohgatp {:#x}", iohgatp);
        }
        else{
            info!("RV IOMMU: Invalid device ID: {}", device_id);
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Since hvisor supports RISC-V's IOMMU and Arm's SMMUv3, two interfaces for external calls, iommu_init and iommu_add_device, are encapsulated during implementation. These two functions have the same function names and parameters as the common call interfaces under the Arm architecture, as implemented below:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// alloc the Fram for DDT &amp; Init
pub fn iommu_init() {
    let iommu = Iommu::new(0x10010000);
    IOMMU.call_once(|| RwLock::new(iommu));
    rv_iommu_init();
}

// every DMA device do!
pub fn iommu_add_device(vm_id: usize, device_id: usize, root_pt: usize){
    info!("RV_IOMMU_ADD_DEVICE: root_pt {:#x}, vm_id {}", root_pt, vm_id);
    let iommu = iommu();
    iommu.write().rv_iommu_add_device(device_id, vm_id, root_pt);
}
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio"><a class="header" href="#virtio">Virtio</a></h1>
<h2 id="introduction-to-virtio"><a class="header" href="#introduction-to-virtio">Introduction to Virtio</a></h2>
<p>Virtio, proposed by Rusty Russell in 2008, is a device virtualization standard aimed at improving device performance and unifying various semi-virtual device schemes. Currently, Virtio includes over a dozen peripherals such as disks, network cards, consoles, GPUs, etc., and many operating systems including Linux have implemented frontend drivers for various Virtio devices. Therefore, the virtual machine monitor only needs to implement the Virtio backend device, and it can directly allow virtual machines that have implemented Virtio drivers, such as Linux, to use Virtio devices.</p>
<p>The Virtio protocol defines a set of driver interfaces for semi-virtual IO devices, stipulating that the operating system of the virtual machine needs to implement the frontend driver, and the Hypervisor needs to implement the backend device. The virtual machine and Hypervisor communicate and interact through the data plane interface and control plane interface.</p>
<h3 id="data-plane-interface"><a class="header" href="#data-plane-interface">Data Plane Interface</a></h3>
<p>The data plane interface refers to the method of IO data transfer between the driver and the device. For Virtio, the data plane interface refers to a shared memory area between the driver and the device called Virtqueue. Virtqueue is an important data structure in the Virtio protocol, a mechanism and abstract representation for batch data transfer of Virtio devices, used for various data transfer operations between the driver and the device. Virtqueue consists of three main components: Descriptor Table, Available Ring, and Used Ring, which function as follows:</p>
<ol>
<li>
<p>Descriptor Table: An array of descriptors. Each descriptor contains four fields: addr, len, flag, next. Descriptors can represent the address (addr), size (len), and attributes (flag) of a memory buffer, which may contain the command or data of an IO request (filled by the Virtio driver), or the result after the IO request is completed (filled by the Virtio device). Descriptors can be linked into a descriptor chain by the next field as needed, where a descriptor chain represents a complete IO request or result.</p>
</li>
<li>
<p>Available Ring: A circular queue, each element in the queue represents the index of the IO request issued by the Virtio driver in the descriptor table, pointing to the starting descriptor of a descriptor chain.</p>
</li>
<li>
<p>Used Ring: A circular queue, each element in the queue represents the index of the IO result written by the Virtio device in the descriptor table after completing the IO request.</p>
</li>
</ol>
<p>Thus, using these three data structures, a complete description of the commands, data, and results for IO data transfer requests between the driver and the device can be made. The Virtio driver is responsible for allocating the memory area where the Virtqueue is located and writing its address into the corresponding MMIO control registers to inform the Virtio device. After obtaining the addresses of the three components, the device can perform IO transfer with the driver through the Virtqueue.</p>
<h3 id="control-plane-interface"><a class="header" href="#control-plane-interface">Control Plane Interface</a></h3>
<p>The control plane interface refers to the way the driver discovers, configures, and manages the device. In the Hypervisor, the control plane interface of Virtio mainly refers to the MMIO registers based on memory mapping. The operating system first detects the MMIO-based Virtio device through the device tree, and by reading and writing these memory-mapped control registers, it can negotiate, configure, and notify the device. Among the more important registers are:</p>
<ul>
<li>
<p>QueueSel: Used to select the current operating Virtqueue. A device may contain multiple Virtqueues, and the driver indicates which queue it is operating by writing this register.</p>
</li>
<li>
<p>QueueDescLow, QueueDescHigh: Used to indicate the intermediate physical address IPA of the descriptor table. The driver writes these two 32-bit registers to inform the device of the 64-bit physical address of the descriptor table, used to establish shared memory.</p>
</li>
<li>
<p>QueueDriverLow, QueueDriverHigh: Used to indicate the intermediate physical address IPA of the available ring.</p>
</li>
<li>
<p>QueueDeviceLow, QueueDeviceHigh: Used to indicate the intermediate physical address IPA of the used ring.</p>
</li>
<li>
<p>QueueNotify: When the driver writes this register, it indicates that there are new IO requests in the Virtqueue that need to be processed.</p>
</li>
</ul>
<p>In addition to control registers, the MMIO memory area of each device also contains a device configuration space. For disk devices, the configuration space indicates the disk's capacity and block size; for network devices, it indicates the device's MAC address and connection status. For console devices, the configuration space provides console size information.</p>
<p>For the MMIO memory area of the Virtio device, the Hypervisor does not map the second-stage address translation for the virtual machine. When the driver reads and writes this area, a page fault exception occurs causing a VM Exit into the Hypervisor, and the Hypervisor can determine the register accessed by the driver based on the access address causing the page fault exception, and make the corresponding processing, such as notifying the device to perform IO operations. After processing, the Hypervisor returns to the virtual machine through VM Entry.</p>
<h3 id="io-process-of-virtio-devices"><a class="header" href="#io-process-of-virtio-devices">IO Process of Virtio Devices</a></h3>
<p>A user process running on a virtual machine, from issuing an IO operation to obtaining the IO result, can generally be divided into the following four steps:</p>
<ol>
<li>The user process initiates an IO operation, the Virtio driver in the operating system kernel receives the IO operation command, writes it into the Virtqueue, and writes the QueueNotify register to notify the Virtio device.</li>
<li>After receiving the notification, the device parses the available ring and descriptor table to obtain the specific IO request and buffer address, and performs the actual IO operation.</li>
<li>After completing the IO operation, the device writes the result into the used ring. If the driver program uses the method of polling the used ring to wait for the IO result, the driver can immediately receive the result information; otherwise, it needs to notify the driver program through an interrupt.</li>
<li>The driver program obtains the IO result from the used ring and returns it to the user process.</li>
</ol>
<h2 id="design-and-implementation-of-the-virtio-backend-mechanism"><a class="header" href="#design-and-implementation-of-the-virtio-backend-mechanism">Design and Implementation of the Virtio Backend Mechanism</a></h2>
<p>The Virtio devices in hvisor follow the <a href="https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.pdf">Virtio v1.2</a> protocol for design and implementation. To maintain good device performance while ensuring the lightweight nature of hvisor, the two design points of the Virtio backend are:</p>
<ol>
<li>
<p>Adopting the microkernel design philosophy, moving the implementation of Virtio devices from the Hypervisor layer to the management virtual machine user space. The management virtual machine runs the Linux operating system, known as Root Linux. Physical devices such as disks and network cards are directly passed through to Root Linux, while Virtio devices act as daemons on Root Linux, providing device emulation for other virtual machines (Non Root Linux). This ensures the lightweight nature of the Hypervisor layer and facilitates formal verification.</p>
</li>
<li>
<p>The Virtio drivers located on other virtual machines and the Virtio devices on Root Linux interact directly through shared memory. The shared memory area that stores interaction information is called a communication springboard and adopts the producer-consumer model, shared by the Virtio device backend and Hypervisor. This reduces the interaction overhead between the driver and the device, enhancing the device's performance.</p>
</li>
</ol>
<p>Based on the above two design points, the implementation of the Virtio backend device will be divided into three parts: communication springboard, Virtio daemon, and kernel service module:</p>
<p><img src="chap04/subchap03/VirtIO/./img/architecture.svg" alt="architecture" /></p>
<h3 id="communication-springboard"><a class="header" href="#communication-springboard">Communication Springboard</a></h3>
<p>To achieve efficient interaction between drivers and devices distributed across different virtual machines, this paper designs a communication springboard as a bridge for passing control plane interaction information between the driver and the device. It is essentially a shared memory area containing two circular queues: the request submission queue and the request result queue, which store interaction requests issued by the driver and results returned by the device, respectively. Both queues are located in the memory area shared by the Hypervisor and the Virtio daemon and adopt the producer-consumer model. The Hypervisor acts as the producer of the request submission queue and the consumer of the request result queue, while the Virtio daemon acts as the consumer of the request submission queue and the producer of the request result queue. This facilitates the transfer of Virtio control plane interaction information between Root Linux and other virtual machines. It should be noted that the request submission queue and request result queue are different from the Virtqueue. The Virtqueue is the data plane interface between the driver and the device, used for data transfer, and essentially contains information about the data buffer's address and structure. The communication springboard, on the other hand, is used for control plane interaction and communication between the driver and the device.</p>
<div align=center><img src="chap04/subchap03/VirtIO/./img/ps_module.svg" alt="ps_module" style="zoom:75%;" /></div>
<ul>
<li>Communication Springboard Structure</li>
</ul>
<p>The communication springboard is represented by the struct virtio_bridge, where req_list is the request submission queue, and res_list and cfg_values together form the request result queue. The device_req struct represents interaction requests sent by the driver to the device, and the device_res struct represents interrupt information to be injected by the device, used to notify the virtual machine driver that the IO operation has been completed.</p>
<pre><code class="language-c">// Communication Springboard Structure:
struct virtio_bridge {
	__u32 req_front;
	__u32 req_rear;
    __u32 res_front;
    __u32 res_rear;
    // Request submission queue
	struct device_req req_list[MAX_REQ]; 
    // res_list, cfg_flags, and cfg_values together form the request result queue
    struct device_res res_list[MAX_REQ];
	__u64 cfg_flags[MAX_CPUS]; 
	__u64 cfg_values[MAX_CPUS];
	__u64 mmio_addrs[MAX_DEVS];
	__u8 mmio_avail;
	__u8 need_wakeup;
};
// Interaction requests sent by the driver to the device
struct device_req {
	__u64 src_cpu;
	__u64 address; // zone's ipa
	__u64 size;
	__u64 value;
	__u32 src_zone;
	__u8 is_write;
	__u8 need_interrupt;
	__u16 padding;
};
// Interrupt information to be injected by the device
struct device_res {
    __u32 target_zone;
    __u32 irq_id;
};
</code></pre>
<h4 id="request-submission-queue"><a class="header" href="#request-submission-queue">Request Submission Queue</a></h4>
<p>The request submission queue is used for the driver to send control plane interaction requests to the device. When the driver reads and writes the MMIO memory area of the Virtio device, since the Hypervisor does not perform second-stage address mapping for this memory area in advance, the CPU executing the driver program will receive a page fault exception and fall into the Hypervisor. The Hypervisor will combine the current CPU number, the address of the page fault exception, the address width, the value to be written (ignored if it is a read), the virtual machine ID, whether it is a write operation, etc., into a structure called device_req, and add it to the request submission queue req_list. At this time, the Virtio daemon monitoring the request submission queue will retrieve and process the request.</p>
<p>To facilitate communication between the Virtio daemon and the Hypervisor based on shared memory, the request submission queue req_list is implemented as a circular queue. The head index req_front is updated only by the Virtio process after retrieving the request, and the tail index req_rear is updated only by the Hypervisor after adding the request. If the head and tail indices are equal, the queue is empty; if the tail index plus 1 modulo the head index is equal, the queue is full, and the driver needs to block in place when adding more requests, waiting for the queue to become available. To ensure that the Hypervisor and the Virtio process have real-time observation and mutual exclusion access to shared memory, the Hypervisor needs to perform a write memory barrier after adding a request to the queue, then update the tail index, ensuring that the Virtio process can correctly retrieve the request in the queue when observing the tail index update. After the Virtio daemon retrieves the request from the queue, it needs to perform a write memory barrier to ensure that the Hypervisor can immediately observe the update of the head index. By using this producer-consumer model and circular queue method, along with the necessary memory barriers, the mutual exclusion problem of shared memory under different privilege levels is solved. Since multiple virtual machines may have multiple CPUs simultaneously adding requests to the request submission queue, the CPU needs to first acquire a mutex lock before operating the request submission queue; however, only the main thread of the Virtio daemon operates the request submission queue, so no lock is needed. This solves the mutual exclusion problem of shared memory under the same privilege level.</p>
<h4 id="request-result-queue"><a class="header" href="#request-result-queue">Request Result Queue</a></h4>
<p>When the Virtio daemon completes the processing of a request, it will put the related information into the request result queue and notify the driver program. To improve communication efficiency, based on the classification of Virtio interaction information, the request result queue is divided into two sub-queues:</p>
<ul>
<li>Data Plane Result Sub-queue</li>
</ul>
<p>The data plane result queue, represented by the res_list structure, is used to store information for injecting interrupts. When the driver program writes to the Queue Notify register in the device memory area, it indicates that there is new data in the available ring, requiring the device to perform IO operations. Since IO operations take too long, Linux, to avoid unnecessary blocking and improve CPU utilization, requires the Hypervisor to submit the IO request to the device, and the CPU needs to immediately return from the Hypervisor to the virtual machine to perform other tasks. Therefore, after completing the IO operation and updating the used ring, the Virtio process will combine the device's interrupt number irq_id and the virtual machine ID of the device into a device_res structure, add it to the data plane result sub-queue res_list, and fall into the Hypervisor through ioctl and hvc. The data plane result queue res_list is similar to the request submission queue, being a circular queue. The queue length can be determined by the head index res_front and the tail index res_rear. The Hypervisor will retrieve all elements from res_list and add them to the interrupt injection table VIRTIO_IRQS. The interrupt injection table is a key-value pair collection based on a B-tree, where the key is the CPU number, and the value is an array. The first element of the array indicates the effective length of the array, and the subsequent elements indicate the interrupts to be injected into this CPU. To prevent multiple CPUs from simultaneously operating the interrupt injection table, the CPU needs to first acquire a global mutex lock to access the interrupt injection table. Through the interrupt injection table, the CPU can determine which interrupts need to be injected into itself based on its own CPU number. Subsequently, the Hypervisor will send IPI inter-core interrupts to these CPUs needing interrupt injection. The CPUs receiving the inter-core interrupts will traverse the interrupt injection table and inject interrupts into themselves. The diagram below describes the entire process, where the black solid triangle arrows represent operations executed by CPUs running other virtual machines, and the black ordinary arrows represent operations executed by CPUs running Root Linux.</p>
<img src="chap04/subchap03/VirtIO/./img/data_plane_queue.svg" alt="data_plane_queue"  />
<ul>
<li>Control Plane Result Sub-queue</li>
</ul>
<p>The control plane result queue, represented by the cfg_values and cfg_flags arrays, where the array index is the CPU number, i.e., each CPU uniquely corresponds to the same position in the two arrays. cfg_values is used to store the results of control plane interface interactions, and cfg_flags are used to indicate whether the device has completed the control plane interaction request. When the driver program reads and writes the registers in the device memory area (except the Queue Notify register), it sends out configuration and negotiation-related control plane interaction requests. After such an interaction request is added to the request submission queue, the CPU that fell into the Hypervisor due to the driver needs to wait for the result to return before it can return to the virtual machine. Since the Virtio daemon does not need to perform IO operations for this type of request, it only needs to query related information, so it can quickly complete the processing of the request and does not need to update the used ring. After completing the request, the daemon will write the result value into cfg_values[id] (for read requests) according to the driver's CPU number id, perform a write memory barrier, then increment cfg_flags[id], and perform a second write memory barrier to ensure that when the driver-side CPU observes changes in cfg_flags[id], cfg_values[id] has already saved the correct result value. When the driver-side CPU observes changes in cfg_flags[id], it can determine that the device has returned the result, directly retrieve the value from cfg_values[id], and return to the virtual machine. This allows the Virtio device to avoid executing ioctl and hvc, causing unnecessary CPU context switches, thereby enhancing the device's performance. The diagram below describes the entire process, where the black solid triangle arrows represent operations executed by CPUs running other virtual machines, and the black ordinary arrows represent operations executed by CPUs running Root Linux.</p>
<p><img src="chap04/subchap03/VirtIO/./img/control_plane_queue.svg" alt="control_plane_queue" /></p>
<h3 id="kernel-service-module"><a class="header" href="#kernel-service-module">Kernel Service Module</a></h3>
<p>Since the Virtio daemon located in the Root Linux user space needs to communicate with hvisor, this paper uses the kernel module hvisor.ko in Root Linux as a bridge for communication. In addition to being used by command-line tools, this module also undertakes the following tasks:</p>
<ol>
<li>Establishing the shared memory area where the communication springboard is located between the Virtio daemon and the Hypervisor during Virtio device initialization.</li>
</ol>
<p>When the Virtio daemon initializes, it requests the kernel module to allocate the shared memory where the communication springboard is located through ioctl. At this time, the kernel module allocates a page of continuous physical memory as shared memory through the memory allocation function <code>__get_free_pages</code>, and sets the page attribute to the reserved state through the <code>SetPageReserved</code> function to avoid the page being swapped to disk due to Linux's page recycling mechanism. Subsequently, the kernel module needs to make this memory accessible to both the Virtio daemon and the Hypervisor. For the Hypervisor, the kernel module executes hvc to notify the Hypervisor and passes the physical address of the shared memory as a parameter. For the Virtio daemon, the process calls mmap on /dev/hvisor, and the kernel module maps the shared memory to a free virtual memory area of the Virtio process in the hvisor_map function. The starting address of this area is returned as the return value of mmap.</p>
<ol start="2">
<li>
<p>When the Virtio backend device needs to inject device interrupts into other virtual machines, it notifies the kernel module through ioctl, and the kernel module calls the system interface provided by the Hypervisor through the hvc command to notify the Hypervisor to perform the corresponding operations.</p>
</li>
<li>
<p>Waking up the Virtio daemon.</p>
</li>
</ol>
<p>When the driver accesses the device's MMIO area, it falls into EL2 and enters the mmio_virtio_handler function. This function determines whether it needs to wake up the Virtio daemon based on the need_wakeup flag in the communication springboard. If the flag is 1, it sends an SGI interrupt with event id IPI_EVENT_WAKEUP_VIRTIO_DEVICE to Root Linux's CPU 0. CPU 0, upon receiving the SGI interrupt, falls into EL2 and injects the interrupt number of the hvisor_device node in the Root Linux device tree into itself. When CPU 0 returns to the virtual machine, it receives the injected interrupt and enters the interrupt handling function previously registered by the kernel service module. This function sends the SIGHVI signal to the Virtio daemon through the send_sig_info function. The Virtio daemon, previously blocked in the sig_wait function, receives the SIGHVI signal, polls the request submission queue, and sets the need_wakeup flag to 0.</p>
<h3 id="virtio-daemon"><a class="header" href="#virtio-daemon">Virtio Daemon</a></h3>
<p>To ensure the lightweight nature of the Hypervisor, this paper</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio-block"><a class="header" href="#virtio-block">Virtio Block</a></h1>
<p>The implementation of Virtio disk devices follows the conventions of the Virtio specification and adopts the MMIO device access method for discovery and use by other virtual machines. Currently, it supports five features: <code>VIRTIO_BLK_F_SEG_MAX</code>, <code>VIRTIO_BLK_F_SIZE_MAX</code>, <code>VIRTIO_F_VERSION_1</code>, <code>VIRTIO_RING_F_INDIRECT_DESC</code>, and <code>VIRTIO_RING_F_EVENT_IDX</code>.</p>
<h2 id="top-level-description-of-virtio-devices---virtiodevice"><a class="header" href="#top-level-description-of-virtio-devices---virtiodevice">Top-level description of Virtio devices - VirtIODevice</a></h2>
<p>A Virtio device is represented by the VirtIODevice structure, which includes the device ID, the number of Virtqueues vqs_len, the ID of the virtual machine it belongs to, the device interrupt number irq_id, the starting address of the MMIO area base_addr, the length of the MMIO area len, the device type, some MMIO registers saved by the device regs, an array of Virtqueues vqs, and a pointer dev pointing to specific device information. With this information, a Virtio device can be fully described.</p>
<pre><code class="language-c">// The highest representations of virtio device
struct VirtIODevice
{
    uint32_t id;
    uint32_t vqs_len;
    uint32_t zone_id;
    uint32_t irq_id;
    uint64_t base_addr; // the virtio device's base addr in non root zone's memory
    uint64_t len;       // mmio region's length
    VirtioDeviceType type;
    VirtMmioRegs regs;
    VirtQueue *vqs;
    // according to device type, blk is BlkDev, net is NetDev, console is ConsoleDev.
    void *dev;          
    bool activated;
};

typedef struct VirtMmioRegs {
    uint32_t device_id;
    uint32_t dev_feature_sel;
    uint32_t drv_feature_sel;
    uint32_t queue_sel;
    uint32_t interrupt_status;
    uint32_t interrupt_ack;
    uint32_t status;
    uint32_t generation;
    uint64_t dev_feature;
    uint64_t drv_feature;
} VirtMmioRegs;
</code></pre>
<h2 id="description-of-virtio-block-devices"><a class="header" href="#description-of-virtio-block-devices">Description of Virtio Block devices</a></h2>
<p>For Virtio disk devices, the type field in VirtIODevice is VirtioTBlock, vqs_len is 1, indicating that there is only one Virtqueue, and the dev pointer points to the virtio_blk_dev structure that describes specific information about the disk device. virtio_blk_dev's config represents the device's data capacity and the maximum amount of data in a single data transfer, img_fd is the file descriptor of the opened disk image, tid, mtx, cond are used for the worker thread, procq is the work queue, and closing indicates when the worker thread should close. Definitions of virtio_blk_dev and blkp_req structures are shown in Figure 4.6.</p>
<pre><code class="language-c">typedef struct virtio_blk_dev {
    BlkConfig config;
    int img_fd;
	// describe the worker thread that executes read, write and ioctl.
	pthread_t tid;
	pthread_mutex_t mtx;
	pthread_cond_t cond;
	TAILQ_HEAD(, blkp_req) procq;
	int close;
} BlkDev;

// A request needed to process by blk thread.
struct blkp_req {
	TAILQ_ENTRY(blkp_req) link;
    struct iovec *iov;
	int iovcnt;
	uint64_t offset;
	uint32_t type;
	uint16_t idx;
};
</code></pre>
<h2 id="virtio-block-device-worker-thread"><a class="header" href="#virtio-block-device-worker-thread">Virtio Block device worker thread</a></h2>
<p>Each Virtio disk device has a worker thread and a work queue. The thread ID of the worker thread is saved in the tid field of virtio_blk_dev, and the work queue is procq. The worker thread is responsible for data IO operations and calling the interrupt injection system interface. It is created after the Virtio disk device starts and continuously checks whether there are new tasks in the work queue. If the queue is empty, it waits for the condition variable cond; otherwise, it processes tasks.</p>
<p>When the driver writes to the QueueNotify register in the MMIO area of the disk device, it indicates that there are new IO requests in the available ring. After receiving this request, the Virtio disk device (located in the main thread's execution flow) first reads the available ring to get the first descriptor of the descriptor chain. The first descriptor points to a memory buffer containing the type of IO request (read/write) and the sector number to be read or written. Subsequent descriptors point to data buffers; for read operations, the read data is stored in these data buffers, and for write operations, the data to be written is retrieved from these data buffers. The last descriptor's memory buffer (result buffer) is used to describe the completion result of the IO request, with options including success (OK), failure (IOERR), or unsupported operation (UNSUPP). This parsing of the entire descriptor chain provides all the information about the IO request, which is then saved in the blkp_req structure. The fields in this structure, iov, represent all data buffers, offset represents the data offset of the IO operation, type represents the type of IO operation (read/write), and idx is the index of the first descriptor in the descriptor chain, used to update the used ring. The device then adds the blkp_req to the work queue procq and wakes up the blocked worker thread through the signal function. The worker thread can then process the task.</p>
<p>After obtaining the task, the worker thread reads and writes the disk image corresponding to img_fd using the preadv and pwritev functions according to the IO operation information indicated by blkp_req. After completing the read/write operation, it first updates the last descriptor of the descriptor chain, which describes the completion result of the IO request, such as success, failure, or unsupported operation. Then it updates the used ring and writes the first descriptor of the descriptor chain to a new entry. Subsequently, it injects an interrupt to notify other virtual machines.</p>
<p>The establishment of the worker thread effectively distributes time-consuming operations to other CPU cores, improving the efficiency and throughput of the main thread in dispatching requests and enhancing device performance.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio-network-device"><a class="header" href="#virtio-network-device">Virtio Network Device</a></h1>
<p>The Virtio network device is essentially a virtual network card. Currently supported features include <code>VIRTIO_NET_F_MAC</code>, <code>VIRTIO_NET_F_STATUS</code>, <code>VIRTIO_F_VERSION_1</code>, <code>VIRTIO-RING_F_INDIRECT_DESC</code>, <code>VIRTIO_RING_F_EVENT_IDX</code>.</p>
<h2 id="description-of-virtio-network-device"><a class="header" href="#description-of-virtio-network-device">Description of Virtio Network Device</a></h2>
<p>For the Virtio network device, the type field in VirtIODevice is VirtioTNet, vqs_len is 2, indicating there are 2 Virtqueues, which are the Receive Queue and the Transmit Queue, respectively. The dev pointer points to the virtio_net_dev structure that describes specific information about the network device. In Virtio_net_dev, config is used to represent the MAC address and connection status of the network card, tapfd is the file descriptor of the Tap device corresponding to the device, rx_ready indicates whether the receive queue is available, and event is used for the receive packet thread to monitor the Tap device's readable events through epoll.</p>
<pre><code class="language-c">typedef struct virtio_net_dev {
    NetConfig config;
    int tapfd;
    int rx_ready;   
    struct hvisor_event *event;
} NetDev;

struct hvisor_event {
    void        (*handler)(int, int, void *);
    void        *param;
    int         fd;
    int         epoll_type;
};
</code></pre>
<h2 id="tap-device-and-bridge-device"><a class="header" href="#tap-device-and-bridge-device">Tap Device and Bridge Device</a></h2>
<p>The implementation of the Virtio network device is based on two virtual devices provided by the Linux kernel: Tap device and bridge device.</p>
<p>A Tap device is an Ethernet device implemented in software by the Linux kernel, and Ethernet frames can be simulated by reading and writing to the Tap device in user space. Specifically, when a process or kernel performs a write operation on the Tap device, it is equivalent to sending a packet to the Tap device. When a read operation is performed on the Tap device, it is equivalent to receiving a packet from the Tap device. Thus, by performing read and write operations on the Tap device, packet transfer between the kernel and the process can be achieved.</p>
<p>The command to create a tap device is: <code>ip tuntap add dev tap0 mode tap</code>. This command creates a tap device named tap0. If a process wants to use this device, it needs to first open the /dev/net/tun device, obtain a file descriptor tun_fd, and call ioctl(TUNSETIFF) on it to link the process to the tap0 device. Afterwards, tun_fd actually becomes the file descriptor of the tap0 device, and it can be read, written, and polled.</p>
<p>A bridge device is a virtual device provided by the Linux kernel that functions similarly to a switch. When other network devices are connected to the bridge device, those devices become ports of the bridge device, and the bridge device takes over the packet sending and receiving process of all devices. When other devices receive packets, they are sent directly to the bridge device, which forwards them to other ports based on the MAC address. Therefore, all devices connected to the bridge can communicate with each other.</p>
<p>The command to create a bridge device is: <code>brctl addbr br0</code>. The command to connect the physical network card eth0 to br0 is: brctl addif br0 eth0. The command to connect the tap0 device to br0 is: brctl addif br0 tap0.</p>
<p>Before the Virtio network device starts, Root Linux needs to create and start the tap device and bridge device in advance from the command line, and connect the tap device and the physical network card on Root Linux to the bridge device respectively. Each Virtio network device needs to connect to a tap device, ultimately forming a network topology diagram as shown below. In this way, the Virtio network device can transmit packets to the external network by reading and writing to the tap device.</p>
<p><img src="chap04/subchap03/VirtIO/./img/hvisor-virtio-net.svg" alt="hvisor-virtio-net" /></p>
<h2 id="sending-packets"><a class="header" href="#sending-packets">Sending Packets</a></h2>
<p>The Transmit Virtqueue of the Virtio network device is used to store the send buffer. When the device receives a request from the driver to write to the QueueNotify register, if the QueueSel register points to the Transmit Queue at this time, it indicates that the driver is notifying the device that there is a new packet to send. The Virtio-net device will take out a descriptor chain from the available ring, each descriptor chain corresponds to a packet, and the memory buffer it points to contains the packet data to be sent. The packet data includes 2 parts, the first part is the packet header virtio_net_hdr_v1 structure specified by the Virtio protocol, which contains some description information of the packet, and the second part is the Ethernet frame. To send a packet, just write the Ethernet frame part to the Tap device through the writev function. After the Tap device receives the frame, it will forward it to the bridge device, and the bridge device will forward it to the external network through the physical network card based on the MAC address.</p>
<h2 id="receiving-packets"><a class="header" href="#receiving-packets">Receiving Packets</a></h2>
<p>When initializing, the Virtio network device adds the file descriptor of the Tap device to the interest list of the event monitor thread epoll instance. The event monitor thread will loop and call the epoll_wait function to monitor the readable events of the tap device. Once a readable event occurs, indicating that the tap device has received a packet from the kernel, the epoll_wait function returns and executes the packet reception processing function. The processing function will take out a descriptor chain from the available ring of the Receive Virtqueue, read the tap device, write the data into the memory buffer pointed to by the descriptor chain, and update the used ring. The processing function will repeat this step until reading the tap device returns a negative value and errno is EWOULDBLOCK, indicating that there are no new packets in the tap device, and then interrupt to notify other virtual machines to receive packets.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio-console"><a class="header" href="#virtio-console">Virtio Console</a></h1>
<p>The Virtio Console device is essentially a virtual console device used for input and output of data, and can serve as a virtual terminal for other virtual machines. Currently, hvisor supports the <code>VIRTIO_CONSOLE_F_SIZE</code> and <code>VIRTIO_F_VERSION_1</code> features.</p>
<h2 id="description-of-the-virtio-console-device"><a class="header" href="#description-of-the-virtio-console-device">Description of the Virtio Console Device</a></h2>
<p>For the Virtio console device, the type field in the VirtIODevice structure is VirtioTConsole, vqs_len is 2, indicating that there are two Virtqueues, the receive virtqueue and the transmit virtqueue, used for receiving and sending data on port 0. The dev pointer points to the virtio_console_dev structure that describes specific information about the console device. In this structure, config represents the number of rows and columns of the console, master_fd is the file descriptor of the pseudo-terminal master device connected to the device, rx_ready indicates whether the receive queue is available, and event is used for the event monitor thread to monitor readable events of the pseudo-terminal master device through epoll.</p>
<pre><code class="language-c">typedef struct virtio_console_dev {
    ConsoleConfig config;
    int master_fd;
    int rx_ready;
    struct hvisor_event *event;
} ConsoleDev;
</code></pre>
<h2 id="pseudo-terminal"><a class="header" href="#pseudo-terminal">Pseudo Terminal</a></h2>
<p>A terminal is essentially an input and output device. In the early days of computing, terminals were called teleprinters (TTY). Now, terminals have become a virtual device on computers, connected by terminal emulation programs to graphics card drivers and keyboard drivers to implement data input and output. There are two different implementations of terminal emulation programs: one is as a kernel module in Linux, exposed to user programs as <code>/dev/tty[n]</code>; the other is as an application running in Linux user space, known as a pseudo-terminal (PTY).</p>
<p>Pseudo-terminals themselves are not the focus of this article, but the two devices used by pseudo-terminals—PTY master and PTY slave—are used in this article to implement the Virtio Console device.</p>
<p>Applications can obtain an available PTY master by executing <code>posix_openpt</code>, and get the corresponding PTY slave through the <code>ptsname</code> function. A TTY driver connecting PTY master and PTY slave will copy data between the master and slave. Thus, when a program writes data to the master (or slave), the program can read the same data from the slave (or master).</p>
<h2 id="overall-design-of-virtio-console"><a class="header" href="#overall-design-of-virtio-console">Overall Design of Virtio Console</a></h2>
<p>The Virtio Console device, as a daemon on Root Linux, opens a PTY master during device initialization and outputs the path of the corresponding PTY slave <code>/dev/pts/x</code> to the log file for screen session connection. Meanwhile, the event monitor thread in the Virtio daemon monitors the readable events of the PTY slave so that the PTY master can promptly receive user input data.</p>
<p>When a user executes <code>screen /dev/pts/x</code> on Root Linux, a screen session is created on the current terminal, connecting the device corresponding to the PTY slave <code>/dev/pts/x</code>, and taking over the input and output of the current terminal. The implementation structure diagram of the Virtio Console device is shown below.</p>
<p><img src="chap04/subchap03/VirtIO/./img/virtio_console.svg" alt="virtio_console" /></p>
<h3 id="input-commands"><a class="header" href="#input-commands">Input Commands</a></h3>
<p>When a user types commands on the keyboard, the input characters are passed to the Screen session through the terminal device. The Screen session writes the characters into the PTY slave. When the event monitor thread detects through epoll that the PTY slave is readable, it calls the <code>virtio_console_event_handler</code> function. This function reads from the PTY slave and writes the data into the Virtio Console device's Receive Virtqueue, and sends an interrupt to the corresponding virtual machine.</p>
<p>The corresponding virtual machine, after receiving the interrupt, passes the received character data to the Shell through the TTY subsystem for interpretation and execution.</p>
<h3 id="display-information"><a class="header" href="#display-information">Display Information</a></h3>
<p>When a virtual machine using the Virtio Console driver wants to output information through the Virtio Console device, the Virtio Console driver writes the data to be output into the Transmit Virtqueue and writes to the QueueNotify register in the MMIO area to notify the Virtio Console device to handle the IO operation.</p>
<p>The Virtio Console device reads from the Transmit Virtqueue, retrieves the data to be output, and writes it into the PTY master. The Screen session then retrieves the data to be output from the PTY slave and displays the output information on the monitor through the terminal device.</p>
<blockquote>
<p>Since the PTY master and PTY slave are connected by a TTY driver, which includes a line discipline to pass the data written from the PTY master to the PTY slave back to the PTY master, we need to disable this functionality using the <code>cfmakeraw</code> function to turn off the line discipline feature.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>PCI devices mainly have three spaces: Configuration Space, Memory Space, and I/O Space.</p>
<h3 id="1-configuration-space"><a class="header" href="#1-configuration-space">1. Configuration Space</a></h3>
<ul>
<li><strong>Purpose</strong>: Used for device initialization and configuration.</li>
<li><strong>Size</strong>: Each PCI device has 256 bytes of configuration space.</li>
<li><strong>Access method</strong>: Accessed via bus number, device number, and function number.</li>
<li><strong>Contents</strong>:
<ul>
<li>Device identification information (such as vendor ID, device ID).</li>
<li>Status and command registers.</li>
<li>Base Address Registers (BARs), used to map the device's memory space and I/O space.</li>
<li>Information about interrupt lines and interrupt pins.</li>
</ul>
</li>
</ul>
<h3 id="2-memory-space"><a class="header" href="#2-memory-space">2. Memory Space</a></h3>
<ul>
<li><strong>Purpose</strong>: Used to access device registers and memory, suitable for high bandwidth access.</li>
<li><strong>Size</strong>: Defined by the device manufacturer, mapped into the system memory address space.</li>
<li><strong>Access method</strong>: Accessed via memory read and write instructions.</li>
<li><strong>Contents</strong>:
<ul>
<li>Device registers: Used for control and status reading.</li>
<li>Device-specific memory: such as frame buffers, DMA buffers, etc.</li>
</ul>
</li>
</ul>
<h3 id="3-io-space"><a class="header" href="#3-io-space">3. I/O Space</a></h3>
<ul>
<li><strong>Purpose</strong>: Used to access device control registers, suitable for low bandwidth access.</li>
<li><strong>Size</strong>: Defined by the device manufacturer, mapped into the system's I/O address space.</li>
<li><strong>Access method</strong>: Accessed via special I/O instructions (such as <code>in</code> and <code>out</code>).</li>
<li><strong>Contents</strong>:
<ul>
<li>Device control registers: Used to perform specific I/O operations.</li>
</ul>
</li>
</ul>
<h3 id="summary"><a class="header" href="#summary">Summary</a></h3>
<ul>
<li><strong>Configuration Space</strong> is mainly used for device initialization and configuration.</li>
<li><strong>Memory Space</strong> is used for high-speed access to device registers and memory.</li>
<li><strong>I/O Space</strong> is used for low-speed access to device control registers.</li>
</ul>
<p>PCI virtualization mainly involves managing the three spaces mentioned above. Considering that most devices do not have multiple PCI buses, and the ownership of the PCI bus generally belongs to zone0, to ensure the access speed of PCI devices in zone0, hvisor does not process the PCI bus and PCI devices in zone0 when there is no need to allocate devices on this bus to other zones.</p>
<p>When allocating PCI devices to a zone, we need to ensure that Linux in zone0 no longer uses them. As long as the devices are allocated to other zones, zone0 should not access these devices. Unfortunately, we cannot simply use PCI hot-plugging to remove/re-add devices at runtime, as Linux may reprogram the BARs and locate resources to positions we do not expect or allow. Therefore, a driver in the zone0 kernel needs to intercept access to these PCI devices, and we turned our attention to the hvisor tool.</p>
<p>The hvisor tool registers itself as a PCI virtual driver and claims management of these devices when other zones use them. Before creating a zone, hvisor lets these devices unbind from their own drivers and bind to the hvisor tool. When a zone is destroyed, these devices are actually no longer used by any zone, but from the perspective of zone0, the hvisor tool is still a valid virtual driver, so the release of the devices needs to be done manually. The hvisor tool releases the devices bound to these zones, from the perspective of zone0 Linux, these devices are not bound to any drivers, so if these devices are needed, Linux will automatically rebind the correct drivers.</p>
<p>Now we need to allow the zone to correctly access the PCI devices. To achieve this goal as simply as possible, we directly reused the structure of the PCI bus, meaning that the content about the PCI bus will appear in the device tree of the zone that needs to use the devices on this bus, but other than the zone that actually owns this bus, other zones can only access the device through mmio proxy by hvisor. When a zone tries to access a PCI device, hvisor checks whether it owns the device, and the ownership is declared when the zone is created.</p>
<p>Currently, the treatment of I/O space and memory space is the same as that of configuration space. Because of the uniqueness of the BARs resources, the configuration space cannot be directly allocated to a zone, and the frequency of access to the BAR space is low, which does not significantly affect efficiency. However, direct allocation of I/O space and memory space is theoretically feasible, and further, I/O space and memory space will be directly allocated to the corresponding zone to improve access speed.</p>
<p>To facilitate testing PCI virtualization in QEMU, we wrote a PCI device.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hvisor-management-tool"><a class="header" href="#hvisor-management-tool">hvisor Management Tool</a></h1>
<p>hvisor manages the entire system through a Root Linux that manages virtual machines. Root Linux provides services for starting and shutting down virtual machines and starting and shutting down Virtio daemons through a set of management tools. The management tools include a command-line tool and a kernel module. The command-line tool is used to parse and execute commands entered by the user, and the kernel module is used for communication between the command-line tool, Virtio daemon, and Hypervisor. The repository address for the management tools is: <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a>.</p>
<h2 id="starting-virtual-machines"><a class="header" href="#starting-virtual-machines">Starting Virtual Machines</a></h2>
<p>Users can create a new virtual machine on Root Linux for hvisor by entering the following command:</p>
<pre><code>./hvisor zone start [vm_name].json
</code></pre>
<p>The command-line tool first parses the contents of the <code>[vm_name].json</code> file, writing the virtual machine configuration into the <code>zone_config</code> structure. Based on the images and dtb files specified in the file, their contents are read into temporary memory through the <code>read</code> function. To load the images and dtb files into a specified physical memory address, the <code>hvisor.ko</code> kernel module provides the <code>hvisor_map</code> function, which can map a physical memory area to user-space virtual address space.</p>
<p>When the command-line tool executes the <code>mmap</code> function on <code>/dev/hvisor</code>, the kernel calls the <code>hvisor_map</code> function to map user virtual memory to the specified physical memory. Afterwards, the image and dtb file contents can be moved from temporary memory to the user-specified physical memory area through a memory copy function.</p>
<p>After the image is loaded, the command-line tool calls <code>ioctl</code> on <code>/dev/hvisor</code>, specifying the operation code as <code>HVISOR_ZONE_START</code>. The kernel module then notifies the Hypervisor through a Hypercall and passes the address of the <code>zone_config</code> structure object, informing the Hypervisor to start the virtual machine.</p>
<h2 id="shutting-down-virtual-machines"><a class="header" href="#shutting-down-virtual-machines">Shutting Down Virtual Machines</a></h2>
<p>Users can shut down a virtual machine with ID <code>vm_id</code> by entering the command:</p>
<pre><code>./hvisor shutdown -id [vm_id]
</code></pre>
<p>This command calls <code>ioctl</code> on <code>/dev/hvisor</code>, specifying the operation code as <code>HVISOR_ZONE_SHUTDOWN</code>. The kernel module then notifies the Hypervisor through a Hypercall, passing <code>vm_id</code>, and informs the Hypervisor to shut down the virtual machine.</p>
<h2 id="starting-virtio-daemons"><a class="header" href="#starting-virtio-daemons">Starting Virtio Daemons</a></h2>
<p>Users can start a Virtio device by entering the command:</p>
<pre><code>nohup ./hvisor virtio start [virtio_cfg.json] &amp;
</code></pre>
<p>This will create a Virtio device and initialize related data structures according to the Virtio device information specified in <code>virtio_cfg.json</code>. Currently, three types of Virtio devices can be created, including Virtio-net, Virtio-block, and Virtio-console devices.</p>
<p>Since the command-line parameters include <code>nohup</code> and <code>&amp;</code>, the command will exist in the form of a daemon, with all output of the daemon redirected to <code>nohup.out</code>. The daemon's output includes six levels, from low to high: <code>LOG_TRACE</code>, <code>LOG_DEBUG</code>, <code>LOG_INFO</code>, <code>LOG_WARN</code>, <code>LOG_ERROR</code>, <code>LOG_FATAL</code>. When compiling the command-line tool, the LOG level can be specified. For example, when LOG is <code>LOG_INFO</code>, outputs equal to or higher than <code>LOG_INFO</code> will be recorded in the log file, while <code>log_trace</code> and <code>log_debug</code> will not be output.</p>
<p>After the Virtio device is created, the Virtio daemon will poll the request submission queue to obtain Virtio requests from other virtual machines. When there are no requests for a long time, it will automatically enter sleep mode.</p>
<h2 id="shutting-down-virtio-daemons"><a class="header" href="#shutting-down-virtio-daemons">Shutting Down Virtio Daemons</a></h2>
<p>Users can shut down the Virtio daemon by entering the command:</p>
<pre><code>pkill hvisor
</code></pre>
<p>The Virtio daemon, when started, registers a signal handler <code>virtio_close</code> for the <code>SIGTERM</code> signal. When executing <code>pkill hvisor</code>, a <code>SIGTERM</code> signal is sent to the process named <code>hvisor</code>. At this point, the daemon executes <code>virtio_close</code>, recycles resources, shuts down various sub-threads, and finally exits.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hypercall-description"><a class="header" href="#hypercall-description">Hypercall Description</a></h1>
<p>As a Hypervisor, hvisor provides a hypercall processing mechanism to the upper layer virtual machines.</p>
<h2 id="how-virtual-machines-execute-hypercall"><a class="header" href="#how-virtual-machines-execute-hypercall">How Virtual Machines Execute Hypercall</a></h2>
<p>Virtual machines execute a specified assembly instruction, which is <code>hvc</code> on Arm64 and <code>ecall</code> on riscv64. When executing the assembly instruction, the parameters passed are:</p>
<ul>
<li>code: hypercall id, its range and meaning are detailed in hvisor's handling of hypercalls</li>
<li>arg0: the first parameter passed by the virtual machine, type is u64</li>
<li>arg1: the second parameter passed by the virtual machine, type is u64</li>
</ul>
<p>For example, for riscv linux:</p>
<pre><code class="language-c">#ifdef RISCV64

// according to the riscv sbi spec
// SBI return has the following format:
// struct sbiret
//  {
//  long error;
//  long value;
// };

// a0: error, a1: value
static inline __u64 hvisor_call(__u64 code,__u64 arg0, __u64 arg1) {
	register __u64 a0 asm("a0") = code;
	register __u64 a1 asm("a1") = arg0;
	register __u64 a2 asm("a2") = arg1;
	register __u64 a7 asm("a7") = 0x114514;
	asm volatile ("ecall"
	        : "+r" (a0), "+r" (a1)
			: "r" (a2), "r" (a7)
			: "memory");
	return a1;
}
#endif
</code></pre>
<p>For arm64 linux:</p>
<pre><code class="language-c">#ifdef ARM64
static inline __u64 hvisor_call(__u64 code, __u64 arg0, __u64 arg1) {
	register __u64 x0 asm("x0") = code;
	register __u64 x1 asm("x1") = arg0;
	register __u64 x2 asm("x2") = arg1;

	asm volatile ("hvc #0x4856"
	        : "+r" (x0)
			: "r" (x1), "r" (x2)
			: "memory");
	return x0;
}
#endif /* ARM64 */
</code></pre>
<h2 id="hvisors-handling-of-hypercall"><a class="header" href="#hvisors-handling-of-hypercall">hvisor's Handling of Hypercall</a></h2>
<p>After the virtual machine executes a hypercall, the CPU enters the exception handling function specified by hvisor: <code>hypercall</code>. Then hvisor continues to call different processing functions based on the hypercall parameters <code>code</code>, <code>arg0</code>, <code>arg1</code>, which are:</p>
<div class="table-wrapper"><table><thead><tr><th>code</th><th>Function Called</th><th>Parameter Description</th><th>Function Summary</th></tr></thead><tbody>
<tr><td>0</td><td>hv_virtio_init</td><td>arg0: start address of shared memory</td><td>Used for root zone to initialize virtio mechanism</td></tr>
<tr><td>1</td><td>hv_virtio_inject_irq</td><td>None</td><td>Used for root zone to send virtio device interrupts to other virtual machines</td></tr>
<tr><td>2</td><td>hv_zone_start</td><td>arg0: virtual machine configuration file address; arg1: configuration file size</td><td>Used for root zone to start a virtual machine</td></tr>
<tr><td>3</td><td>hv_zone_shutdown</td><td>arg0: id of the virtual machine to be shut down</td><td>Used for root zone to shut down a virtual machine</td></tr>
<tr><td>4</td><td>hv_zone_list</td><td>arg0: address of data structure representing virtual machine information; arg1: number of virtual machine information</td><td>Used for root zone to view information of all virtual machines in the system</td></tr>
<tr><td>5</td><td>hv_ivc_info</td><td>arg0: start address of ivc information</td><td>Used for a zone to view its own communication domain information</td></tr>
</tbody></table>
</div>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="assets/fzf.umd.js"></script>
        <script src="assets/elasticlunr.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
