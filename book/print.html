<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>hvisor Manual</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="hvisor Manual">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">hvisor Manual</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/syswonder/hvisor" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div align=center>
	<img src="chap01/img/hvisor-logo.svg"/>
</div>
<h1 id="welcome-to-hvisor"><a class="header" href="#welcome-to-hvisor">Welcome to hvisor!</a></h1>
<p>Hello~</p>
<div style="break-before: page; page-break-before: always;"></div><div align=center>
	<img src="chap01/img/hvisor-logo.svg"/>
</div>
<h1 id="welcome-to-hvisor-1"><a class="header" href="#welcome-to-hvisor-1">Welcome to hvisor!</a></h1>
<p><a href="https://github.com/syswonder/hvisor">hvisor</a> is a lightweight Type-1 hypervisor written in Rust, offering efficient resource management and low-overhead virtualization performance.</p>
<p>Features</p>
<ol>
<li><strong>Cross-platform support</strong>: Supports multiple architectures including AARCH64, RISC-V, and LoongArch.</li>
<li><strong>Lightweight</strong>: Focuses on core virtualization functionalities, avoiding unnecessary complexity found in traditional virtualization solutions, suitable for resource-constrained environments.</li>
<li><strong>Efficient</strong>: Runs directly on hardware without the need for an operating system layer, providing near-native performance.</li>
<li><strong>Security</strong>: Rust is known for its memory safety and concurrent programming model, helping to reduce common system-level programming errors such as memory leaks and data races.</li>
<li><strong>Fast startup</strong>: Designed with simplicity in mind, it has a short startup time, suitable for scenarios requiring rapid deployment of virtualization.</li>
</ol>
<p>Main Functions</p>
<ol>
<li><strong>Virtual Machine Management</strong>: Provides basic management functions for creating, starting, stopping, and deleting virtual machines.</li>
<li><strong>Resource Allocation and Isolation</strong>: Supports efficient allocation and management of CPU, memory, and I/O devices, using virtualization technology to ensure isolation between different virtual machines, enhancing system security and stability.</li>
</ol>
<p>Use Cases</p>
<ol>
<li><strong>Edge Computing</strong>: Suitable for running on edge devices, providing virtualization support for IoT and edge computing scenarios.</li>
<li><strong>Development and Testing</strong>: Developers can quickly create and destroy virtual machine environments for software development and testing.</li>
<li><strong>Security Research</strong>: Provides an isolated environment for security research and malware analysis.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="hvisor-currently-supported-hardware-platforms"><a class="header" href="#hvisor-currently-supported-hardware-platforms">hvisor currently supported hardware platforms</a></h1>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
QEMU</li>
</ul>
<h1 id="hvisor-upcoming-hardware-platforms"><a class="header" href="#hvisor-upcoming-hardware-platforms">hvisor upcoming hardware platforms</a></h1>
<ul>
<li><input disabled="" type="checkbox"/>
OKMX8MP-C</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-hvisor-on-qemu"><a class="header" href="#running-hvisor-on-qemu">Running hvisor on QEMU</a></h1>
<h2 id="1-install-cross-compiler-aarch64-none-linux-gnu-103"><a class="header" href="#1-install-cross-compiler-aarch64-none-linux-gnu-103">1. Install Cross Compiler aarch64-none-linux-gnu-10.3</a></h2>
<p>URL: <a href="https://developer.arm.com/downloads/-/gnu-a">https://developer.arm.com/downloads/-/gnu-a</a></p>
<p>Tool selection: AArch64 GNU/Linux target (aarch64-none-linux-gnu)</p>
<p>Download link: <a href="https://developer.arm.com/-/media/Files/downloads/gnu-a/10.3-2021.07/binrel/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz?rev=1cb9c51b94f54940bdcccd791451cec3&amp;hash=B380A59EA3DC5FDC0448CA6472BF6B512706F8EC">https://developer.arm.com/-/media/Files/downloads/gnu-a/10.3-2021.07/binrel/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz?rev=1cb9c51b94f54940bdcccd791451cec3&amp;hash=B380A59EA3DC5FDC0448CA6472BF6B512706F8EC</a></p>
<pre><code class="language-bash">wget https://armkeil.blob.core.windows.net/developer/Files/downloads/gnu-a/10.3-2021.07/binrel/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz
tar xvf gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu.tar.xz
ls gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin/
</code></pre>
<p>Once installed, remember the path, for example: /home/tools/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu-, this path will be used later.</p>
<h2 id="2-compile-and-install-qemu-7212"><a class="header" href="#2-compile-and-install-qemu-7212">2. Compile and Install QEMU 7.2.12</a></h2>
<pre><code># Install required dependencies for compilation
sudo apt install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \
              gawk build-essential bison flex texinfo gperf libtool patchutils bc \
              zlib1g-dev libexpat-dev pkg-config  libglib2.0-dev libpixman-1-dev libsdl2-dev \
              git tmux python3 python3-pip ninja-build
# Download the source code
wget https://download.qemu.org/qemu-7.2.12.tar.xz
# Extract
tar xvJf qemu-7.2.12.tar.xz
cd qemu-7.2.12
# Generate configuration file
./configure --enable-kvm --enable-slirp --enable-debug --target-list=aarch64-softmmu,x86_64-softmmu
# Compile
make -j$(nproc)
</code></pre>
<p>Then edit the <code>~/.bashrc</code> file, add a few lines at the end of the file:</p>
<pre><code># Note that the parent directory of qemu-7.2.12 can be adjusted flexibly according to your actual installation location
export PATH=$PATH:/path/to/qemu-7.2.12/build
</code></pre>
<p>Then you can update the system path in the current terminal by <code>source ~/.bashrc</code>, or simply restart a new terminal. At this point, you can confirm the qemu version:</p>
<pre><code>qemu-system-aarch64 --version   # Check the version
</code></pre>
<blockquote>
<p>Note, the above dependencies may not be complete, for example:</p>
<ul>
<li>If <code>ERROR: pkg-config binary 'pkg-config' not found</code> occurs, you can install the <code>pkg-config</code> package;</li>
<li>If <code>ERROR: glib-2.48 gthread-2.0 is required to compile QEMU</code> occurs, you can install the <code>libglib2.0-dev</code> package;</li>
<li>If <code>ERROR: pixman &gt;= 0.21.8 not present</code> occurs, you can install the <code>libpixman-1-dev</code> package.</li>
</ul>
</blockquote>
<blockquote>
<p>If you encounter an error ERROR: Dependency "slirp" not found, tried pkgconfig when generating the configuration file:</p>
<p>Download the <a href="https://gitlab.freedesktop.org/slirp/libslirp">https://gitlab.freedesktop.org/slirp/libslirp</a> package and install it according to the readme.</p>
</blockquote>
<h2 id="3-compile-linux-kernel-54"><a class="header" href="#3-compile-linux-kernel-54">3. Compile Linux Kernel 5.4</a></h2>
<p>Before compiling the root linux image, change the CONFIG_IPV6 and CONFIG_BRIDGE config in the .config file to y to support creating bridges and tap devices in root linux. The specific operations are as follows:</p>
<pre><code>git clone https://github.com/torvalds/linux -b v5.4 --depth=1
cd linux
git checkout v5.4
# Modify the CROSS_COMPILE path according to the path of the cross compiler installed in the first step
make ARCH=arm64 CROSS_COMPILE=/root/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu- defconfig
# Add a line in .config
CONFIG_BLK_DEV_RAM=y
# Modify two CONFIG parameters in .config
CONFIG_IPV6=y
CONFIG_BRIDGE=y
# Compile, modify the CROSS_COMPILE path according to the path of the cross compiler installed in the first step
make ARCH=arm64 CROSS_COMPILE=/root/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu- Image -j$(nproc)
</code></pre>
<blockquote>
<p>If there is an error when compiling linux:</p>
<pre><code>/usr/bin/ld: scripts/dtc/dtc-parser.tab.o:(.bss+0x20): multiple definition of `yylloc'; scripts/dtc/dtc-lexer.lex.o:(.bss+0x0): first defined here
</code></pre>
<p>Then modify <code>scripts/dtc/dtc-lexer.lex.c</code> under the linux folder, add <code>extern</code> before <code>YYLTYPE yylloc;</code>. Compile again, if you encounter the error: openssl/bio.h: No such file or directory, then execute <code>sudo apt install libssl-dev</code></p>
</blockquote>
<p>Once compiled, the kernel file is located at: arch/arm64/boot/Image. Remember the path of the entire linux folder, for example: home/korwylee/lgw/hypervisor/linux, we will use this path again in step 7.</p>
<h2 id="4-build-file-system-based-on-ubuntu-2004-arm64-base"><a class="header" href="#4-build-file-system-based-on-ubuntu-2004-arm64-base">4. Build File System Based on Ubuntu 20.04 Arm64 Base</a></h2>
<blockquote>
<p>This section can be omitted, you can directly download the ready-made disk image to use. https://blog.syswonder.org/#/2024/20240415_Virtio_devices_tutorial</p>
</blockquote>
<p>We use Ubuntu 20.04 (22.04 is also possible) to build the root file system.</p>
<p>Download: <a href="http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.5-base-arm64.tar.gz">ubuntu-base-20.04.5-base-arm64.tar.gz</a></p>
<p>Link: <a href="http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.5-base-arm64.tar.gz">http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.5-base-arm64.tar.gz</a></p>
<pre><code class="language-bash">wget http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.5-base-arm64.tar.gz

mkdir rootfs
# Create a 1G size ubuntu.img, you can modify the count to change the img size
dd if=/dev/zero of=ubuntu-20.04-rootfs_ext4.img bs=1M count=1024 oflag=direct
mkfs.ext4 ubuntu-20.04-rootfs_ext4.img
# Put ubuntu.tar.gz into the ubuntu.img that has been mounted on rootfs
sudo mount -t ext4 ubuntu-20.04-rootfs_ext4.img rootfs/
sudo tar -xzf ubuntu-base-20.04.5-base-arm64.tar.gz -C rootfs/

# Let rootfs bind and obtain some information and hardware of the physical machine
# qemu-path is your qemu path
sudo cp qemu-path/build/qemu-system-aarch64 rootfs/usr/bin/
sudo cp /etc/resolv.conf rootfs/etc/resolv.conf
sudo mount -t proc /proc rootfs/proc
sudo mount -t sysfs /sys rootfs/sys
sudo mount -o bind /dev rootfs/dev
sudo mount -o bind /dev/pts rootfs/dev/pts

# Executing this command may report an error, please refer to the solution below
sudo chroot rootfs
sudo apt-get install git sudo vim bash-completion \
		kmod net-tools iputils-ping resolvconf ntpdate

# The following content surrounded by # can be done or not
###################
adduser arm64
adduser arm64 sudo
echo "kernel-5_4" &gt;/etc/hostname
echo "127.0.0.1 localhost" &gt;/etc/hosts
echo "127.0.0.1 kernel-5_4"&gt;&gt;/etc/hosts
dpkg-reconfigure resolvconf
dpkg-reconfigure tzdata
###################
exit

sudo umount rootfs/proc
sudo umount rootfs/sys
sudo umount rootfs/dev/pts
sudo umount rootfs/dev
sudo umount rootfs
</code></pre>
<p>Finally, unmount the mount to complete the production of the root file system.</p>
<blockquote>
<p>When executing `sudo chroot .</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-qemu"><a class="header" href="#install-qemu">Install QEMU</a></h1>
<p>Install QEMU 7.2.12:</p>
<pre><code>wget https://download.qemu.org/qemu-7.2.12.tar.xz
# Extract
tar xvJf qemu-7.2.12.tar.xz
cd qemu-7.2.12
# Configure Riscv support
./configure --target-list=riscv64-softmmu,riscv64-linux-user 
make -j$(nproc)
# Add to environment variable
export PATH=$PATH:/path/to/qemu-7.2.12/build
# Test if the installation is successful
qemu-system-riscv64 --version
</code></pre>
<h1 id="install-cross-compiler"><a class="header" href="#install-cross-compiler">Install Cross Compiler</a></h1>
<p>The RISC-V cross compiler needs to be obtained and compiled from riscv-gnu-toolchain.</p>
<pre><code># Install necessary tools
sudo apt-get install autoconf automake autotools-dev curl python3 python3-pip libmpc-dev libmpfr-dev libgmp-dev gawk build-essential bison flex texinfo gperf libtool patchutils bc zlib1g-dev libexpat-dev ninja-build git cmake libglib2.0-dev libslirp-dev

git clone https://github.com/riscv/riscv-gnu-toolchain
cd riscv-gnu-toolchain
git rm qemu 
git submodule update --init --recursive
# The above operation will occupy more than 5GB of disk space
# If git reports a network error, you can execute:
git config --global http.postbuffer 524288000
</code></pre>
<p>Then start compiling the toolchain:</p>
<pre><code>cd riscv-gnu-toolchain
mkdir build
cd build
../configure --prefix=/opt/riscv64
sudo make linux -j $(nproc)
# After compilation, add the toolchain to the environment variable
echo 'export PATH=/opt/riscv64/bin:$PATH' &gt;&gt; ~/.bashrc
source ~/.bashrc
</code></pre>
<p>This gives you the riscv64-unknown-linux-gnu toolchain.</p>
<h1 id="compile-linux"><a class="header" href="#compile-linux">Compile Linux</a></h1>
<pre><code>git clone https://github.com/torvalds/linux -b v6.2 --depth=1
cd linux
git checkout v6.2
make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig
make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- modules -j$(nproc)
# Start compiling
make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- Image -j$(nproc)

</code></pre>
<h1 id="create-ubuntu-root-file-system"><a class="header" href="#create-ubuntu-root-file-system">Create Ubuntu Root File System</a></h1>
<pre><code>wget http://cdimage.ubuntu.com/ubuntu-base/releases/20.04/release/ubuntu-base-20.04.2-base-riscv64.tar.gz
mkdir rootfs
dd if=/dev/zero of=riscv_rootfs.img bs=1M count=1024 oflag=direct
mkfs.ext4 riscv_rootfs.img
sudo mount -t ext4 riscv_rootfs.img rootfs/
sudo tar -xzf ubuntu-base-20.04.2-base-riscv64.tar.gz -C rootfs/

sudo cp /path-to-qemu/build/qemu-system-riscv64 rootfs/usr/bin/
sudo cp /etc/resolv.conf rootfs/etc/resolv.conf
sudo mount -t proc /proc rootfs/proc
sudo mount -t sysfs /sys rootfs/sys
sudo mount -o bind /dev rootfs/dev
sudo mount -o bind /dev/pts rootfs/dev/pts
sudo chroot rootfs 
# After entering chroot, install necessary packages:
apt-get update
apt-get install git sudo vim bash-completion \
    kmod net-tools iputils-ping resolvconf ntpdate
exit

sudo umount rootfs/proc
sudo umount rootfs/sys
sudo umount rootfs/dev/pts
sudo umount rootfs/dev
sudo umount rootfs
</code></pre>
<h1 id="run-hvisor"><a class="header" href="#run-hvisor">Run hvisor</a></h1>
<p>Place the prepared root file system and Linux kernel image in the specified location in the hvisor directory, and execute <code>make run ARCH=riscv64</code> in the root directory of hvisor</p>
<p>By default, it uses PLIC, execute <code>make run ARCH=riscv64 IRQ=aia</code> to enable AIA specification</p>
<h1 id="possible-issues"><a class="header" href="#possible-issues">Possible Issues</a></h1>
<p>After running Linux, the display shows <code>/bin/sh: 0: can't access tty; job control turned off</code>, enter <code>bash</code> in the console</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nxp-launches-jailhouse"><a class="header" href="#nxp-launches-jailhouse">NXP Launches Jailhouse</a></h1>
<p>Date: 2024/2/25
Updated: 2024/3/13</p>
<p>Authors: Yang Junyi, Chen Xingyu</p>
<p>Overall Approach:</p>
<ol>
<li>Boot the first Linux using an SD card, it is recommended to use Ubuntu's rootfs and set up the network for convenience in package installation.</li>
<li>Boot the root Linux and compile the Linux kernel and Jailhouse.</li>
<li>Restart, modify the root dtb, and boot the root Linux.</li>
<li>Jailhouse boots non-root Linux, which is the Linux on the eMMC (original manufacturer's Linux), specifying the rootfs as eMMC.</li>
</ol>
<h2 id="1-creating-an-ubuntu-sd-card-image"><a class="header" href="#1-creating-an-ubuntu-sd-card-image">1. Creating an Ubuntu SD Card Image</a></h2>
<pre><code class="language-shell">wget https://cdimage.ubuntu.com/ubuntu-base/releases/18.04/release/ubuntu-base-18.04.5-base-arm64.tar.gz
tar zxvf ubuntu-base-18.04.5-base-arm64.tar.gz

cd ubuntu-base-18.04.5-base-arm64

# chroot in x86
sudo apt-get install qemu
sudo cp /usr/bin/qemu-aarch64-static usr/bin/

sudo mount /sys ./sys -o bind
sudo mount /proc ./proc -o bind
sudo mount /dev ./dev -o bind

sudo mv etc/resolv.conf etc/resolv.conf.saved
sudo cp /etc/resolv.conf etc

sudo LC_ALL=C chroot . /bin/bash

# chroot in arm
sudo arch-chroot .

sudo apt-get update 
# Install required packages such as vim, build-essential, python3, python3-dev, gcc, g++, git, make, kmod.
sudo apt-get install &lt;PKG_NAME&gt; 

exit

# If using arch-chroot, manual umount is not needed
sudo umount ./sys
sudo umount ./proc
sudo umount ./dev

mv etc/resolv.conf.saved etc/resolv.conf

## Additionally, copy Linux and jailhouse to the SD card, change to local path.
sudo cp -r LINUX_DEMO ubuntu-base-18.04.5-base-arm64/home #source path see Linux kernel compilation section
sudo cp -r Jailhouse_DEMO ubuntu-base-18.04.5-base-arm64/home
# Then copy the ubuntu-base-18.04.5-base-arm64 directory into the SD card as rootfs.
# It is recommended to complete the "Compilation" section before copying, or you can enter the system and then compile
sudo fdisk -l # determine the SD card device name
sudo mount /dev/sdb1 /mnt 
sudo cp -r ubuntu-base-18.04.5-base-arm64 /mnt
</code></pre>
<h2 id="2-compile-nxp-linux-kernel"><a class="header" href="#2-compile-nxp-linux-kernel">2. Compile NXP Linux Kernel</a></h2>
<p>The source code can be obtained from the manufacturer's materials (source location: /OKMX8MP-C_Linux5.4.70+Qt5.15.0_User Information_R5 (update date: 20231012)/Linux/source/OK8MP-linux-sdk/OK8MP-linux-kernel)</p>
<p>This step can be done in a chroot environment or by first using an existing Image and dtb to boot the board (official files provide an Image and OK8MP-C.dtb)</p>
<h3 id="add-root-device-tree"><a class="header" href="#add-root-device-tree">Add Root Device Tree</a></h3>
<p>The device tree storage location is arch/arm64/boot/dts/freescale, add a new device tree OK8MP-C-root.dts, mainly modify to disable usdhc3 (eMMC) and uart4, and share pins between usdhc3 and usdhc2 to facilitate booting non-root-linux</p>
<p>Content:</p>
<pre><code class="language-C">// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
/*
 * Copyright 2019 NXP
 */

/dts-v1/;

#include "OK8MP-C.dts"

/ {
        interrupt-parent = &lt;&amp;gic&gt;;

        resmem: reserved-memory {
                #address-cells = &lt;2&gt;;
                #size-cells = &lt;2&gt;;
                ranges;
        };
};

&amp;cpu_pd_wait {
        /delete-property/ compatible;
};

&amp;clk {
        init-on-array = &lt;IMX8MP_CLK_USDHC3_ROOT
                         IMX8MP_CLK_NAND_USDHC_BUS
                         IMX8MP_CLK_HSIO_ROOT
                         IMX8MP_CLK_UART4_ROOT
                         IMX8MP_CLK_OCOTP_ROOT&gt;;
};

&amp;{/busfreq} {
        status = "disabled";
};

&amp;{/reserved-memory} { // Reserved jailhouse memory area
        jh_reserved: jh@fdc00000 {
                no-map;
                reg = &lt;0 0xfdc00000 0x0 0x400000&gt;;
        };

        loader_reserved: loader@fdb00000 {
                no-map;
                reg = &lt;0 0xfdb00000 0x0 0x00100000&gt;;
        };

        ivshmem_reserved: ivshmem@fda00000 {
                no-map;
                reg = &lt;0 0xfda00000 0x0 0x00100000&gt;;
        };

        ivshmem2_reserved: ivshmem2@fd900000 {
                no-map;
                reg = &lt;0 0xfd900000 0x0 0x00100000&gt;;
        };

        pci_reserved: pci@fd700000 {
                no-map;
                reg = &lt;0 0xfd700000 0x0 0x00200000&gt;;
        };

        inmate_reserved: inmate@60000000 {
                no-map;
                reg = &lt;0 0x60000000 0x0 0x10000000&gt;;
        };
};

&amp;iomuxc {
        pinctrl_uart4: uart4grp {
                fsl,pins = &lt;
                        MX8MP_IOMUXC_UART4_RXD__UART4_DCE_RX    0x49
                        MX8MP_IOMUXC_UART4_TXD__UART4_DCE_TX    0x49
                &gt;;
        };
};

&amp;usdhc3 { // eMMC: mmc2, since this eMMC is nonroot, root should not occupy it, so disable it
        status = "disabled";
};

&amp;uart4 { // Also disable this, used for nonroot boot.
        /delete-property/ dmas;
        /delete-property/ dma-names;
        pinctrl-names = "default";
        pinctrl-0 = &lt;&amp;pinctrl_uart4&gt;;
        status = "disabled";
};

&amp;uart2 { // uart1=ttymxc0 uart4=ttymxc3 default for ttymxc1.
        /* uart4 is used by the 2nd OS, so configure pin and clk */
        pinctrl-0 = &lt;&amp;pinctrl_uart2&gt;, &lt;&amp;pinctrl_uart4&gt;;
        assigned-clocks = &lt;&amp;clk IMX8MP_CLK_UART4&gt;;
        assigned-clock-parents = &lt;&amp;clk IMX8MP_CLK_24M&gt;;
};

&amp;usdhc2 {
        pinctrl-0 = &lt;&amp;pinctrl_usdhc3&gt;, &lt;&amp;pinctrl_usdhc2&gt;, &lt;&amp;pinctrl_usdhc2_gpio&gt;;
        pinctrl-1 = &lt;&amp;pinctrl_usdhc3&gt;, &lt;&amp;pinctrl_usdhc2_100mhz&gt;, &lt;&amp;pinctrl_usdhc2_gpio&gt;;
        pinctrl-2 = &lt;&amp;pinctrl_usdhc3&gt;, &lt;&amp;pinctrl_usdhc2_200mhz&gt;, &lt;&amp;pinctrl_usdhc2_gpio&gt;;
};
</code></pre>
<h3 id="kernel-compilation"><a class="header" href="#kernel-compilation">Kernel Compilation</a></h3>
<pre><code class="language-shell"># First, refer to the previous chroot and enter the source directory
make OK8MP-C_defconfig # Configure default config
make -j$(nproc) ARCH=arm64 # Compilation takes about 15 minutes
</code></pre>
<p>If the gcc version is too high, it might cause yylloc issues. You can downgrade the version or add 'extern' before yylloc in scripts/dtc/dtc-lexer.lex.c_shipped</p>
<p>If there are definition conflicts between jailhouse and the kernel, prioritize the kernel and modify jailhouse accordingly</p>
<h3 id="compile-jailhouse"><a class="header" href="#compile-jailhouse">Compile Jailhouse</a></h3>
<p>Jailhouse version uses v0.12 then manually add dts and configuration files</p>
<pre><code class="language-shell">git checkout v0.12
</code></pre>
<p>.c file addition location configs/arm64</p>
<p>.dts file addition location configs/arm64/dts</p>
<p>imx8mp.c</p>
<pre><code class="language-C">/*
 * i.MX8MM Target
 *
 * Copyright 2018 NXP
 *
 * Authors:
 *  Peng Fan &lt;peng.fan@nxp.com&gt;
 *
 * This work is licensed under the terms of the GNU GPL, version 2.  See
 * the COPYING file in the top-level directory.
 *
 * Reservation via device tree: reg = &lt;0x0 0xffaf0000 0x0 0x510000&gt;
 */

#include &lt;jailhouse/types.h&gt;
#include &lt;jailhouse/cell-config.h&gt;

struct {
        struct jailhouse_system header;
        __u64 cpus[1];
        struct jailhouse_memory mem_regions[15];
        struct jailhouse_irqchip irqchips[3];
        struct jailhouse_pci_device pci_devices[2];
} __attribute__((packed)) config = {
        .header = {
                .signature = JAILHOUSE_SYSTEM_SIGNATURE,
                .revision = JAILHOUSE_CONFIG_REVISION,
                .flags = JAILHOUSE_SYS_VIRTUAL_DEBUG_CONSOLE,
                .hypervisor_memory = {
                        .phys_start = 0xfdc00000,
                        .size =       0x00400000,
                },
                .debug_console = {
                        .address = 0x30890000,
                        .size = 0x1000,
                        .flags = JAILHOUSE_CON_TYPE_IMX |
                                 JAILHOUSE</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fpga-zcu102"><a class="header" href="#fpga-zcu102">FPGA zcu102</a></h1>
<p>Author: 杨竣轶(Jerry) github.com/comet959</p>
<pre><code class="language-shell"># Before, Install vivado 2022.2 software
# Ubuntu 20.04 can work fine
sudo apt update

git clone https://github.com/U-interrupt/uintr-rocket-chip.git
cd uintr-rocket-chip
git submodule update --init --recursive
export RISCV=/opt/riscv64
git checkout 98e9e41
vim digilent-vivado-script/config.ini # Env Config

make checkout
make clean
make build

# Use vivado to open the vivado project, then change the top file, run synthesis, run implementation, generate bitstream.
# Connect the zcu102 - Jtag and Uart on your PC.
# Use dd command to flash the image include boot and rootfs part.
# Change the boot button mode to (On Off Off Off)
# Boot the power.

sudo screen /dev/ttyUSB0 115200 # Aarch64 Core Uart
sudo screen /dev/ttyUSB2 115200 # Riscv Core Uart

# On /dev/ttyUSB0
cd uintr-rocket-chip
./load-and-reset.sh

# Focus on ttyUSB2, then you will see the Riscv Linux Boot Msg.

</code></pre>
<h2 id="enabling-h-extension-in-rocketchip"><a class="header" href="#enabling-h-extension-in-rocketchip">Enabling H Extension in RocketChip</a></h2>
<pre><code class="language-shell">vim path/to/repo/common/src/main/scala/Configs.scala
</code></pre>
<pre><code class="language-scala">// change
class UintrConfig extends Config(
  new WithNBigCores(4) ++
    new WithNExtTopInterrupts(6) ++
    new WithTimebase((BigInt(10000000))) ++ // 10 MHz
    new WithDTS("freechips.rocketchip-unknown", Nil) ++
    new WithUIPI ++
    new WithCustomBootROM(0x10000, "../common/boot/bootrom/bootrom.img") ++
    new WithDefaultMemPort ++
    new WithDefaultMMIOPort ++
    new WithDefaultSlavePort ++
    new WithoutTLMonitors ++
    new WithCoherentBusTopology ++
    new BaseSubsystemConfig
)

// to

class UintrConfig extends Config(
  new WithHypervisor ++
  new WithNBigCores(4) ++
    new WithNExtTopInterrupts(6) ++
    new WithTimebase((BigInt(10000000))) ++ // 10 MHz
    new WithDTS("freechips.rocketchip-unknown", Nil) ++
    new WithUIPI ++
    new WithCustomBootROM(0x10000, "../common/boot/bootrom/bootrom.img") ++
    new WithDefaultMemPort ++
    new WithDefaultMMIOPort ++
    new WithDefaultSlavePort ++
    new WithoutTLMonitors ++
    new WithCoherentBusTopology ++
    new BaseSubsystemConfig
)

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="booting-hvisor-on-loongson-3a5000-motherboard-7a2000"><a class="header" href="#booting-hvisor-on-loongson-3a5000-motherboard-7a2000">Booting hvisor on Loongson 3A5000 Motherboard (7A2000)</a></h1>
<p>Han Yulu <a href="mailto:chap02/enkerewpo@hotmail.com">enkerewpo@hotmail.com</a></p>
<p>Updated: December 4, 2024</p>
<h2 id="step-1-obtain-and-compile-the-hvisor-source-code"><a class="header" href="#step-1-obtain-and-compile-the-hvisor-source-code">Step 1: Obtain and compile the hvisor source code</a></h2>
<p>Clone the code locally:</p>
<pre><code class="language-bash">git clone -b dev-loongarch https://github.com/syswonder/hvisor # dev-loongarch branch
make ARCH=loongarch64
</code></pre>
<p>After compilation, you can find the stripped hvisor.bin in the target directory (the file path will be displayed in the last line of the compilation output).</p>
<h2 id="obtain-the-vmlinuxbin-image"><a class="header" href="#obtain-the-vmlinuxbin-image">Obtain the vmlinux.bin image</a></h2>
<p>Please download the latest released hvisor default Loongson Linux image from <a href="https://github.com/enkerewpo/linux-hvisor-loongarch64/releases">https://github.com/enkerewpo/linux-hvisor-loongarch64/releases</a> (including root Linux kernel + root Linux dtb + root Linux rootfs, where root Linux rootfs includes non-root Linux + non-root Linux dtb + non-root Linux rootfs). If you need to compile the Linux kernel and rootfs yourself, refer to the <code>arch/loongarch</code> directory in the repository for hvisor-related device trees and the buildroot environment I ported for 3A5000 (<a href="https://github.com/enkerewpo/buildroot-loongarch64">https://github.com/enkerewpo/buildroot-loongarch64</a>). If you need to manually compile hvisor-tool, refer to <a href="https://github.com/enkerewpo/hvisor-tool">https://github.com/enkerewpo/hvisor-tool</a>. For the compilation order and script invocation process of all environments, refer to the code inside the <code>world</code> target in the <code>Makefile.1</code> file (<a href="https://github.com/enkerewpo/hvisor_uefi_packer/blob/main/Makefile.1">https://github.com/enkerewpo/hvisor_uefi_packer/blob/main/Makefile.1</a>), and compile everything by running the <code>./make_world</code> script. If you need to manually compile these, you need to modify the corresponding code path variables in Makefile.1, including:</p>
<pre><code>HVISOR_LA64_LINUX_DIR = ../hvisor-la64-linux
BUILDROOT_DIR = ../buildroot-loongarch64
HVISOR_TOOL_DIR = ../hvisor-tool
</code></pre>
<p>Then run <code>./make_world</code>. Please note that the first compilation of Linux and buildroot may take a long time (possibly up to several tens of minutes, depending on your machine performance).</p>
<h2 id="obtain-hvisor-uefi-image-packer"><a class="header" href="#obtain-hvisor-uefi-image-packer">Obtain hvisor UEFI Image Packer</a></h2>
<p>Since the 3A5000 and subsequent Series 3 CPUs' motherboards use UEFI boot, hvisor can only be booted via an EFI image method. Clone the repository <a href="https://github.com/enkerewpo/hvisor_uefi_packer">https://github.com/enkerewpo/hvisor_uefi_packer</a> locally:</p>
<pre><code class="language-bash">make menuconfig # Configure for your local loongarch64 gcc toolchain prefix, hvisor.bin path, vmlinux.bin path
# Modify make_image's HVISOR_SRC_DIR=../hvisor to your actual hvisor source code path, then run the script
./make_image
# You will get the BOOTLOONGARCH64.EFI file
</code></pre>
<p>The obtained <code>BOOTLOONGARCH64.EFI</code> must be placed in the first FAT32 partition of the USB drive at <code>/EFI/BOOT/BOOTLOONGARCH64.EFI</code>. Then insert the USB drive to boot and enter hvisor, which will automatically start root Linux.</p>
<p>Since the metadata related to root Linux (loading address, memory area, etc.) is hardcoded in the hvisor source code (<code>src/platform/ls3a5000_loongarch64.rs</code>), if you are manually compiling the Linux kernel, you need to modify the configuration here and recompile hvisor.</p>
<h2 id="board-boot"><a class="header" href="#board-boot">Board Boot</a></h2>
<p>Power on the motherboard, press <strong>F12</strong> to enter the UEFI Boot Menu, select your inserted USB drive and press Enter. It will automatically boot hvisor and enter the root Linux bash environment.</p>
<h2 id="start-nonroot"><a class="header" href="#start-nonroot">Start nonroot</a></h2>
<p>If you are using the related images provided in the release, after booting, enter in the root Linux bash:</p>
<pre><code class="language-bash">./daemon.sh
./linux2_virtio.sh
</code></pre>
<p>This will automatically start nonroot (some related configuration files are located in the root Linux <code>/tool</code> directory, including the nonroot zone configuration JSON and virtio configuration JSON files provided to hvisor-tool). Afterwards, a screen process will open connecting to nonroot Linux's virtio-console, displaying a bash with "nonroot" printed. You can use the CTRL+A D shortcut key to detach (remember the displayed screen session name) during screen use. To return to nonroot Linux, run:</p>
<pre><code class="language-bash">screen -r {the full name of the session just now or just enter the first few numbers}
</code></pre>
<p>This will return you to the nonroot Linux bash.</p>
<div style="break-before: page; page-break-before: always;"></div><p>This directory is mainly related to ZCU102, and the introduction is as follows:</p>
<ol>
<li>How to use Qemu to simulate Xilinx ZynqMP ZCU102</li>
<li>How to boot hvisor root linux and nonroot linux on Qemu ZCU102 and ZCU102 physical development board.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="qemu-zcu102-hvisor-startup"><a class="header" href="#qemu-zcu102-hvisor-startup">Qemu ZCU102 hvisor Startup</a></h1>
<h2 id="install-petalinux"><a class="header" href="#install-petalinux">Install Petalinux</a></h2>
<ol>
<li>Install <a href="https://china.xilinx.com/support/download/index.html/content/xilinx/zh/downloadNav/embedded-design-tools/2024-1.html">Petalinux 2024.1</a>
Please note that this article uses version 2024.1 as an example, which does not mean that other versions are not possible, but other versions have not been verified, and tests have found that Petalinux has a strong dependency on the operating system, so please install the version of Petalinux suitable for your operating system.</li>
<li>Place the downloaded <code>petalinux.run</code> file in the directory where you want to install it, add execution permissions to it, and then run the installer directly with <code>./petalinux.run</code>.</li>
<li>The installer will automatically detect the required environment, and if it does not meet the requirements, it will prompt for the missing environment, just need to <code>apt install</code> one by one.</li>
<li>After installation, you need to enter the installation directory and manually <code>source settings.sh</code> to add environment variables before using Petalinux each time. If it is too troublesome, you can add this command to <code>~/.bashrc</code>.</li>
</ol>
<h2 id="install-zcu102-bsp"><a class="header" href="#install-zcu102-bsp">Install ZCU102 BSP</a></h2>
<ol>
<li>Download the BSP corresponding to the Petalinux version, in the example it is <a href="https://china.xilinx.com/support/download/index.html/content/xilinx/zh/downloadNav/embedded-design-tools/2024-1.html">ZCU102 BSP 2024.1</a></li>
<li>Activate the Petalinux environment, i.e., <code>source settings.sh</code> in the Petalinux installation directory.</li>
<li>Create a Petalinux Project based on BSP: <code>petalinux-create -t project -s xilinx-zcu102-v2024.1-05230256.bsp</code></li>
<li>This will create a <code>xilinx-zcu102-2024.1</code> folder, which contains the parameters needed for QEMU to simulate ZCU102 (device tree), as well as pre-compiled Linux images, device trees, Uboot, etc. that can be directly uploaded to the board.</li>
</ol>
<h2 id="compile-hvisor"><a class="header" href="#compile-hvisor">Compile Hvisor</a></h2>
<p>Refer to "Running Hvisor on Qemu" for setting up the environment required to compile Hvisor, then in the hvisor directory, execute:</p>
<pre><code>make ARCH=aarch64 LOG=info FEATURES=platform_zcu102,gicv2 cp
</code></pre>
<p>to compile. The directory <code>/target/aarch64-unknown-none (may vary)/debug/hvisor</code> is the required hvisor image.</p>
<h2 id="prepare-device-tree"><a class="header" href="#prepare-device-tree">Prepare Device Tree</a></h2>
<h3 id="use-existing-device-tree"><a class="header" href="#use-existing-device-tree">Use Existing Device Tree</a></h3>
<p>In the image/devicetree directory of Hvisor, there is zcu102-root-aarch64.dts, which is a device tree file that has been tested to boot RootLinux, compile it.</p>
<pre><code>dtc -I dts -O dtb -o zcu102-root-aarch64.dtb zcu102-root-aarch64.dts
</code></pre>
<p>If the dtc command is invalid, install device-tree-compiler.</p>
<pre><code>sudo apt-get install device-tree-compiler
</code></pre>
<h3 id="prepare-device-tree-yourself"><a class="header" href="#prepare-device-tree-yourself">Prepare Device Tree Yourself</a></h3>
<p>If you have custom requirements for the device, it is recommended to prepare the device tree yourself. You can decompile the <code>pre-built/linux/images/system.dtb</code> in the ZCU102 BSP to get the complete device tree, based on <code>zcu102-root-aarch64.dts</code> for additions and deletions.</p>
<h2 id="prepare-image"><a class="header" href="#prepare-image">Prepare Image</a></h2>
<h3 id="use-existing-image"><a class="header" href="#use-existing-image">Use Existing Image</a></h3>
<p>It is recommended to use the <code>pre-built/linux/images/Image</code> from the ZCU102 BSP as the Linux kernel to boot on ZCU102, as its driver configuration is complete.</p>
<h3 id="compile-yourself"><a class="header" href="#compile-yourself">Compile Yourself</a></h3>
<p>After testing, the support for ZYNQMP in the Linux source code before version 5.15 is not comprehensive, it is not recommended to use versions before this for compilation, you can compile directly according to the general compilation process in later versions, as the basic support for ZYNQMP in the source code is enabled by default. Specific compilation operations are as follows:</p>
<ol>
<li>Visit the <a href="https://github.com/Xilinx/linux-xlnx/tags?after=xilinx-v2023.1">linux-xlnx</a> official website to download the Linux source code, it is best to download <code>zynqmp-soc-for-v6.3</code>.</li>
<li><code>tar -xvf zynqmp-soc-for-v6.3</code> to unzip the source code</li>
<li>Enter the unzipped directory, execute the following command to use the default configuration, <code>make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- defconfig</code></li>
<li>Compile: <code>make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- Image -j$(nproc)</code></li>
<li>After compilation, the directory <code>arch/arm64/boot/Image</code> is the required image.</li>
</ol>
<h2 id="enable-qemu-simulation"><a class="header" href="#enable-qemu-simulation">Enable QEMU Simulation</a></h2>
<ol>
<li>Activate the Petalinux environment, i.e., <code>source settings.sh</code> in the Petalinux installation directory.</li>
<li>Enter the <code>xilinx-zcu102-2024.1</code> folder, use the following command to start hvisor on the QEMU-simulated ZCU102, the file paths need to be modified according to your actual situation.</li>
</ol>
<pre><code># QEMU parameter passing
petalinux-boot --qemu --prebuilt 2 --qemu-args '-device loader,file=hvisor,addr=0x40400000,force-raw=on -device loader,
file=zcu102-root-aarch64.dtb,addr=0x40000000,force-raw=on -device loader,file=zcu102-root-aarch64.dtb,addr=0x04000000,
force-raw=on -device loader,file=/home/hangqi-ren/Image,addr=0x00200000,force-raw=on -drive if=sd,format=raw,index=1,
file=rootfs.ext4' 
# Start hvisor
bootm 0x40400000 - 0x40000000
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zcu102-hvisor-multi-mode-boot"><a class="header" href="#zcu102-hvisor-multi-mode-boot">ZCU102 hvisor Multi-mode Boot</a></h1>
<h2 id="booting-hvisor-on-zcu102-development-board-in-sd-mode"><a class="header" href="#booting-hvisor-on-zcu102-development-board-in-sd-mode">Booting Hvisor on ZCU102 Development Board in SD Mode</a></h2>
<h3 id="preparing-the-sd-card"><a class="header" href="#preparing-the-sd-card">Preparing the SD Card</a></h3>
<ol>
<li>Prepare a standard SD card, partition it into a Boot partition (FAT32) and the rest as filesystem partitions (EXT4). For Windows partitions, you can use <a href="https://www.diskgenius.cn/download.php">DiskGenius</a>, and for Linux partitions, you can use <a href="https://www.cnblogs.com/renshengdezheli/p/13941563.html">fdisk</a> or <a href="https://blog.csdn.net/linkedin_35878439/article/details/82020925">mkfs</a>.</li>
<li>Prepare a filesystem and copy its contents to any filesystem partition. You can refer to "NXPIMX8" for creating an Ubuntu filesystem or directly use the filesystem from the ZCU102 BSP.</li>
<li>Copy <code>zcu102-root-aarch64.dtb</code>, <code>Image</code>, and <code>hvisor</code> to the Boot partition.</li>
<li>In SD mode, it is necessary to provide ATF and Uboot from the SD card, so copy <code>boot.scr</code> and <code>BOOT.BIN</code> from the ZCU102 BSP to the BOOT partition.</li>
</ol>
<h4 id="booting-zcu102"><a class="header" href="#booting-zcu102">Booting ZCU102</a></h4>
<ol>
<li>Set ZCU102 to SD mode, insert the SD card, connect the serial port, and power on.</li>
<li>Press any key to interrupt the Uboot auto script and run the following commands to boot hvisor and root Linux:</li>
</ol>
<pre><code>fatload mmc 0:1 0x40400000 hvisor;fatload mmc 0:1 0x40000000 zcu102-root-aarch64.dtb
fatload mmc 0:1 0x04000000 zcu102-root-aarch64.dtb;fatload mmc 0:1 0x00200000 Image;bootm 0x40400000 - 0x40000000
</code></pre>
<ol start="3">
<li>If successful, you will see hvisor and Linux information on the serial port and finally enter the filesystem.</li>
</ol>
<h2 id="booting-hvisor-on-zcu102-development-board-in-jtag-mode"><a class="header" href="#booting-hvisor-on-zcu102-development-board-in-jtag-mode">Booting Hvisor on ZCU102 Development Board in Jtag Mode</a></h2>
<p>First, connect the two cables that come with the board to the JTAG and UART interfaces of the board, and connect the other end to the PC via USB.</p>
<p>Then open a petalinux project in the command line, ensure the project has been compiled and generated the corresponding boot files (vmlinux, BOOT.BIN, etc.), and then enter the project root directory and run:</p>
<pre><code class="language-bash">petalinux-boot --jtag --prebuilt 2
</code></pre>
<p>Where prebuilt represents the boot level:</p>
<ul>
<li><strong>Level 1</strong>: Only download the FPGA bitstream, boot FSBL and PMUFW</li>
<li><strong>Level 2</strong>: Download FPGA bitstream and boot UBOOT, and start FSBL, PMUFW, and TF-A (Trusted Firmware-A)</li>
<li><strong>Level 3</strong>: Download and boot Linux, and load or boot FPGA bitstream, FSBL, PMUFW, TF-A, UBOOT</li>
</ul>
<p>Afterward, JTAG will download the corresponding files to the board (saving to the specified memory address) and start the corresponding bootloader. For the specific official UBOOT default script, see the boot.scr file in the project image directory.</p>
<p>Since hvisor requires separate UBOOT commands and a custom-made fitImage to boot, please refer to <a href="chap02/subchap01/../../chap02/subchap01/UbootFitImage-ZCU102.html">UBOOT FIT Image Creation, Loading, and Booting</a>.</p>
<p>After making the fitImage, replace the files in the petalinux images generation directory (Image.ub), so that JTAG loads our custom-made fitImage to the default FIT image load address configured in the petalinux project. This way, when JTAG starts, it will load our fitImage via the JTAG line to the corresponding address in the board memory, and then extract and bootm through the uboot command line.</p>
<p>Another UART line can be used to observe the output of the ZCU102 board (including FSBL, UBOOT, Linux, etc.), which can be viewed through serial port tools such as <code>screen</code>, <code>gtkterm</code>, <code>termius</code>, <code>minicom</code>.</p>
<div class="warning">
    <h3 id="please-note"><a class="header" href="#please-note">Please Note</a></h3>
    <p> Since petalinux specifies some fixed memory addresses, such as the default loading addresses for the Linux kernel, fitImage, and DTB (configurable during petalinux project compilation), there is an issue where if the root Linux dtb in its has the same loading address as during petalinux compilation, it will cause the dtb to be overwritten with the default petalinux dtb, preventing root Linux from booting correctly. Therefore, it is necessary to specify different addresses from the default petalinux dtb/fitImage loading addresses during compilation to prevent other issues.
</div>
<h1 id="references"><a class="header" href="#references">References</a></h1>
<p>[1] PetaLinux Tools Documentation: Reference Guide (UG1144).<a href="https://docs.amd.com/r/2023.1-English/ug1144-petalinux-tools-reference-guide/Booting-a-PetaLinux-Image-on-Hardware-with-JTAG">https://docs.amd.com/r/2023.1-English/ug1144-petalinux-tools-reference-guide/Booting-a-PetaLinux-Image-on-Hardware-with-JTAG</a>
[2] Trusted Firmware-A Documentation.<a href="https://trustedfirmware-a.readthedocs.io/en/latest/">https://trustedfirmware-a.readthedocs.io/en/latest/</a></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="zcu102-nonroot-boot"><a class="header" href="#zcu102-nonroot-boot">ZCU102 NonRoot Boot</a></h2>
<ol>
<li>Compile <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a> using the Linux kernel source code used during the Root boot. The detailed compilation process can be referred to in <a href="https://github.com/syswonder/hvisor-tool/blob/main/README-zh.md">Readme</a>.</li>
<li>Prepare the required <code>virtio_cfg.json</code> and <code>zone1_linux.json</code> for NonRoot boot. You can directly use the <code>example/zcu102-aarch64</code> in the hvisor-tool directory, which has been verified to ensure it can boot.</li>
<li>Prepare the Linux kernel Image, filesystem rootfs, and device tree linux1.dtb needed for NonRoot. The kernel and filesystem can be the same as Root, while Linux1.dtb should be configured as needed, or you can use <code>images/aarch64/devicetree/zcu102-nonroot-aarch64.dts</code> from the hvisor directory.</li>
<li>Copy <code>hvisor.ko, hvisor, virtio_cfg, zone1_linux.json, linux1.dtb, Image, rootfs.ext4</code> to the filesystem used by Root Linux.</li>
<li>Enter the following commands in RootLinux to start NonRoot:</li>
</ol>
<pre><code># Load kernel module
insmod hvisor.ko
# Create virtio device
nohup ./hvisor virtio start virtio_cfg.json &amp;
# Start NonRoot based on the json configuration file
./hvisor zone start zone1_linux.json
# View the output of NonRoot and interact.
screen /dev/pts/0
</code></pre>
<p>For more operation details, refer to <a href="https://github.com/syswonder/hvisor-tool/blob/main/README-zh.md">hvisor-tool Readme</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="uboot-fit-image-creation-loading-and-booting"><a class="header" href="#uboot-fit-image-creation-loading-and-booting">UBOOT FIT Image Creation, Loading, and Booting</a></h1>
<p>wheatfox (enkerewpo@hotmail.com)</p>
<p>This article introduces the basic knowledge related to FIT images, as well as how to create, load, and boot FIT images.</p>
<h2 id="its-source-file"><a class="header" href="#its-source-file">ITS Source File</a></h2>
<p>ITS is the source code for generating FIT images (FIT Image) in uboot, namely Image Tree Source, which uses the Device Tree Source (DTS) syntax format. FIT images can be generated using the mkimage tool provided by uboot.
In the ZCU102 port of hvisor, a FIT image is used to package hvisor, root linux, root dtb, and other files into one fitImage, facilitating booting on QEMU and actual hardware.
The ITS file for the ZCU102 platform is located at <code>scripts/zcu102-aarch64-fit.its</code>:</p>
<pre><code class="language-c">/dts-v1/;
/ {
    description = "FIT image for HVISOR with Linux kernel, root filesystem, and DTB";
    images {
        root_linux {
            description = "Linux kernel";
            data = /incbin/("__ROOT_LINUX_IMAGE__");
            type = "kernel";
            arch = "arm64";
            os = "linux";
            ...
        };
        ...
        root_dtb {
            description = "Device Tree Blob";
            data = /incbin/("__ROOT_LINUX_DTB__");
            type = "flat_dt";
            ...
        };
        hvisor {
            description = "Hypervisor";
            data = /incbin/("__HVISOR_TMP_PATH__");
            type = "kernel";
            arch = "arm64";
            ...
        };
    };

    configurations {
        default = "config@1";
        config@1 {
            description = "default";
            kernel = "hvisor";
            fdt = "root_dtb";
        };
    };
};
</code></pre>
<p>Here, <code>__ROOT_LINUX_IMAGE__</code>, <code>__ROOT_LINUX_DTB__</code>, and <code>__HVISOR_TMP_PATH__</code> will be replaced with actual paths by the <code>sed</code> command in the Makefile. In the ITS source code, it is mainly divided into images and configurations sections. The images section defines the files to be packaged, and the configurations section defines how to combine these files. During UBOOT boot, it will automatically load the corresponding files to the specified address according to the default configuration in configurations, and multiple configurations can be set to support loading different configuration images at boot time.</p>
<p>Makefile corresponding command for mkimage:</p>
<pre><code class="language-Makefile">.PHONY: gen-fit
gen-fit: $(hvisor_bin) dtb
	@if [ ! -f scripts/zcu102-aarch64-fit.its ]; then \
		echo "Error: ITS file scripts/zcu102-aarch64-fit.its not found."; \
		exit 1; \
	fi
	$(OBJCOPY) $(hvisor_elf) --strip-all -O binary $(HVISOR_TMP_PATH)
# now we need to create the vmlinux.bin
	$(GCC_OBJCOPY) $(ROOT_LINUX_IMAGE) --strip-all -O binary $(ROOT_LINUX_IMAGE_BIN)
	@sed \
		-e "s|__ROOT_LINUX_IMAGE__|$(ROOT_LINUX_IMAGE_BIN)|g" \
		-e "s|__ROOT_LINUX_ROOTFS__|$(ROOT_LINUX_ROOTFS)|g" \
		-e "s|__ROOT_LINUX_DTB__|$(ROOT_LINUX_DTB)|g" \
		-e "s|__HVISOR_TMP_PATH__|$(HVISOR_TMP_PATH)|g" \
		scripts/zcu102-aarch64-fit.its &gt; temp-fit.its
	@mkimage -f temp-fit.its $(TARGET_FIT_IMAGE)
	@echo "Generated FIT image: $(TARGET_FIT_IMAGE)"
</code></pre>
<div class="warning">
    <h3 id="please-note-1"><a class="header" href="#please-note-1">Please Note</a></h3>
    <p> Do not input an Image already packaged by UBOOT into the ITS source file, otherwise it will lead to <b>repackaging</b>! The files pointed to in ITS should be original files (vmlinux, etc.), and mkimage processes each file individually when importing ITS (vmlinux->"Image", then embedded into fitImage)
</div>
<h2 id="booting-hvisor-and-root-linux-through-fit-image-in-petalinux-qemu"><a class="header" href="#booting-hvisor-and-root-linux-through-fit-image-in-petalinux-qemu">Booting hvisor and root linux through FIT image in petalinux qemu</a></h2>
<p>Since a fitImage includes all the necessary files, for qemu, you only need to load this file into an appropriate position in memory through the loader.</p>
<p>Then, when qemu starts and enters UBOOT, you can use the following command to boot (modify the specific address according to the actual situation, you can write all lines into one line and copy to UBOOT for booting, or save it to the environment variable <code>bootcmd</code>, UBOOT needs to mount a persistent flash for environment variable storage):</p>
<pre><code class="language-bash">setenv fit_addr 0x10000000; setenv root_linux_load 0x200000;
imxtract ${fit_addr} root_linux ${root_linux_load}; bootm ${fit_addr};
</code></pre>
<h1 id="references-1"><a class="header" href="#references-1">References</a></h1>
<p>[1] Flat Image Tree (FIT). <a href="https://docs.u-boot.org/en/stable/usage/fit/">https://docs.u-boot.org/en/stable/usage/fit/</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-compile"><a class="header" href="#how-to-compile">How to Compile</a></h1>
<h2 id="compiling-with-docker"><a class="header" href="#compiling-with-docker">Compiling with Docker</a></h2>
<h3 id="1-install-docker"><a class="header" href="#1-install-docker">1. Install Docker</a></h3>
<pre><code class="language-bash">sudo snap install docker
</code></pre>
<p>You can also refer to the <a href="https://docs.docker.com/install/">Docker official documentation</a> to install Docker.</p>
<h3 id="2-build-the-image"><a class="header" href="#2-build-the-image">2. Build the image</a></h3>
<pre><code class="language-bash">make build_docker
</code></pre>
<p>This step builds a Docker image, automatically creating all the dependencies required for compilation.</p>
<h3 id="3-run-the-container"><a class="header" href="#3-run-the-container">3. Run the container</a></h3>
<pre><code class="language-bash">make docker
</code></pre>
<p>This step starts a container, mounts the current directory into the container, and enters the container's shell.</p>
<h3 id="4-compile"><a class="header" href="#4-compile">4. Compile</a></h3>
<p>Execute the following command in the container to compile.</p>
<pre><code class="language-bash">make all
</code></pre>
<h2 id="compiling-with-local-environment"><a class="header" href="#compiling-with-local-environment">Compiling with Local Environment</a></h2>
<h3 id="1-install-rustup-and-cargo"><a class="header" href="#1-install-rustup-and-cargo">1. Install RustUp and Cargo</a></h3>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | \
    sh -s -- -y --no-modify-path --profile minimal --default-toolchain nightly
</code></pre>
<h3 id="2-install-the-toolchain"><a class="header" href="#2-install-the-toolchain">2. Install the toolchain</a></h3>
<p>The toolchain currently used by the project is as follows:</p>
<ul>
<li>Rust nightly 2023-07-12</li>
<li><a href="https://crates.io/crates/rustfmt">rustfmt</a></li>
<li><a href="https://crates.io/crates/clippy">clippy</a></li>
<li><a href="https://crates.io/crates/cargo-binutils/0.3.6">cargo-binutils</a></li>
<li>rust-src</li>
<li>llvm-tools-preview</li>
<li>target: aarch64-unknown-none</li>
</ul>
<p>You can check if these tools are installed on your own, or use the following commands to install:</p>
<h4 id="1-install-toml-cli-and-cargo-binutils"><a class="header" href="#1-install-toml-cli-and-cargo-binutils">(1) Install toml-cli and cargo-binutils</a></h4>
<pre><code class="language-bash">cargo install toml-cli cargo-binutils
</code></pre>
<h4 id="2-install-the-target-platform-cross-compilation-toolchain"><a class="header" href="#2-install-the-target-platform-cross-compilation-toolchain">(2) Install the target platform cross-compilation toolchain</a></h4>
<pre><code class="language-bash">rustup target add aarch64-unknown-none
</code></pre>
<h4 id="3-parse-rust-toolchaintoml-to-install-the-rust-toolchain"><a class="header" href="#3-parse-rust-toolchaintoml-to-install-the-rust-toolchain">(3) Parse rust-toolchain.toml to install the Rust toolchain</a></h4>
<pre><code class="language-bash">RUST_VERSION=$(toml get -r rust-toolchain.toml toolchain.channel) &amp;&amp; \
Components=$(toml get -r rust-toolchain.toml toolchain.components | jq -r 'join(" ")') &amp;&amp; \
rustup install $RUST_VERSION &amp;&amp; \
rustup component add --toolchain $RUST_VERSION $Components
</code></pre>
<h4 id="4-compile-1"><a class="header" href="#4-compile-1">(4) Compile</a></h4>
<pre><code class="language-bash">make all
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-start-root-linux"><a class="header" href="#how-to-start-root-linux">How to Start Root Linux</a></h1>
<h2 id="qemu"><a class="header" href="#qemu">QEMU</a></h2>
<h3 id="install-dependencies"><a class="header" href="#install-dependencies">Install Dependencies</a></h3>
<h4 id="1-install-dependencies"><a class="header" href="#1-install-dependencies">1. Install Dependencies</a></h4>
<pre><code class="language-bash">apt-get install -y jq wget build-essential \
 libglib2.0-0 libfdt1 libpixman-1-0 zlib1g \
 libfdt-dev libpixman-1-dev libglib2.0-dev \
 zlib1g-dev ninja-build
</code></pre>
<h4 id="1-download-and-extract-qemu"><a class="header" href="#1-download-and-extract-qemu">1. Download and Extract QEMU</a></h4>
<pre><code class="language-bash">wget https://download.qemu.org/qemu-7.0.0.tar.xz
tar -xvf qemu-${QEMU_VERSION}.tar.xz
</code></pre>
<h4 id="2-conditionally-compile-and-install-qemu"><a class="header" href="#2-conditionally-compile-and-install-qemu">2. Conditionally Compile and Install QEMU</a></h4>
<p>Here we only compile QEMU for simulating aarch64. If you need QEMU for other architectures, you can refer to <a href="https://wiki.qemu.org/Hosts/Linux">QEMU Official Documentation</a>.</p>
<pre><code class="language-bash">cd qemu-7.0.0 &amp;&amp; \
./configure --target-list=aarch64-softmmu,aarch64-linux-user &amp;&amp; \
make -j$(nproc) &amp;&amp; \
make install
</code></pre>
<h4 id="3-test-if-qemu-is-successfully-installed"><a class="header" href="#3-test-if-qemu-is-successfully-installed">3. Test if QEMU is Successfully Installed</a></h4>
<pre><code class="language-bash">qemu-system-aarch64 --version
</code></pre>
<h3 id="start-root-linux"><a class="header" href="#start-root-linux">Start Root Linux</a></h3>
<h4 id="1-prepare-root-file-system-and-kernel-image"><a class="header" href="#1-prepare-root-file-system-and-kernel-image">1. Prepare Root File System and Kernel Image</a></h4>
<p>Place the image file in <code>hvisor/images/aarch64/kernel/</code>, named <code>Image</code>.</p>
<p>Place the Root file system in <code>hvisor/images/aarch64/virtdisk/</code>, named <code>rootfs1.ext4</code>.</p>
<h4 id="2-start-qemu"><a class="header" href="#2-start-qemu">2. Start QEMU</a></h4>
<p>Execute the following command in the hviosr directory:</p>
<pre><code class="language-bash">make run
</code></pre>
<h4 id="3-enter-qemu"><a class="header" href="#3-enter-qemu">3. Enter QEMU</a></h4>
<p>It will automatically load uboot. After uboot loading is complete, enter <code>bootm 0x40400000 - 0x40000000</code> to enter Root Linux.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-boot-nonroot-linux"><a class="header" href="#how-to-boot-nonroot-linux">How to Boot NonRoot Linux</a></h1>
<p>Hvisor has properly handled the booting of NonRoot, making it relatively simple to do so, as follows:</p>
<ol>
<li>
<p>Prepare the kernel image, device tree, and file system for NonRoot Linux. Place the kernel and device tree in the file system of Root Linux.</p>
</li>
<li>
<p>Specify the serial port and file system to be mounted for NonRoot Linux in the device tree file, as shown below:</p>
</li>
</ol>
<pre><code>	chosen {
		bootargs = "clk_ignore_unused console=ttymxc3,115200 earlycon=ec_imx6q3,0x30a60000,115200 root=/dev/mmcblk3p2 rootwait rw";
		stdout-path = "/soc@0/bus@30800000/serial@30a60000";
	};
</code></pre>
<ol start="3">
<li>
<p>Compile the <a href="https://github.com/syswonder/hvisor-tool?tab=readme-ov-file">kernel module and command line tool</a> for Hvisor and place it in the file system of Root Linux.</p>
</li>
<li>
<p>Boot Hvisor's Root Linux and inject the kernel module that was just compiled:</p>
</li>
</ol>
<pre><code>insmod hvisor.ko
</code></pre>
<ol start="5">
<li>Use the command line tool, here assumed to be named <code>hvisor</code>, to boot NonRoot Linux.</li>
</ol>
<pre><code>./hvisor zone start --kernel kernel image,addr=0x70000000 --dtb device tree file,addr=0x91000000 --id virtual machine number (starting from 1)
</code></pre>
<ol start="6">
<li>Once NonRoot Linux has booted, open the specified serial port to use it.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-and-management-of-zones"><a class="header" href="#configuration-and-management-of-zones">Configuration and Management of Zones</a></h1>
<p>The hvisor project, as a lightweight hypervisor, uses a Type-1 architecture that allows multiple virtual machines (zones) to run directly on top of hardware. Below is a detailed explanation of the key points of zone configuration and management:</p>
<h2 id="resource-allocation"><a class="header" href="#resource-allocation">Resource Allocation</a></h2>
<p>Resources such as CPU, memory, devices, and interrupts are statically allocated to each zone, meaning that once allocated, these resources will not be dynamically scheduled between zones.</p>
<h2 id="root-zone-configuration"><a class="header" href="#root-zone-configuration">Root Zone Configuration</a></h2>
<p>The configuration of the root zone is hardcoded inside hvisor, written in Rust language, and presented as a C-style structure HvZoneConfig. This structure contains key information such as zone ID, number of CPUs, memory regions, interrupt information, physical addresses and sizes of the kernel and device tree binaries (DTB).</p>
<h2 id="non-root-zones-configuration"><a class="header" href="#non-root-zones-configuration">Non-root Zones Configuration</a></h2>
<p>The configuration of non-root zones is stored in the file system of root Linux, usually represented in JSON format. For example:</p>
<pre><code class="language-json">    {
        "arch": "arm64",
        "zone_id": 1,
        "cpus": [2, 3],
        "memory_regions": [
            {
                "type": "ram",
                "physical_start": "0x50000000",
                "virtual_start":  "0x50000000",
                "size": "0x30000000"
            },
            {
                "type": "io",
                "physical_start": "0x30a60000",
                "virtual_start":  "0x30a60000",
                "size": "0x1000"
            },
            {
                "type": "virtio",
                "physical_start": "0xa003c00",
                "virtual_start":  "0xa003c00",
                "size": "0x200"
            }
        ],
        "interrupts": [61, 75, 76, 78],
        "kernel_filepath": "./Image",
        "dtb_filepath": "./linux2.dtb",
        "kernel_load_paddr": "0x50400000",
        "dtb_load_paddr":   "0x50000000",
        "entry_point":      "0x50400000"
    }
</code></pre>
<ul>
<li>The <code>arch</code> field specifies the target architecture (e.g., arm64).</li>
<li><code>cpus</code> is a list indicating the CPU core IDs allocated to the zone.</li>
<li><code>memory_regions</code> describes different types of memory regions along with their physical and virtual start addresses and sizes.</li>
<li><code>interrupts</code> lists the interrupt numbers allocated to the zone.</li>
<li><code>kernel_filepath</code> and <code>dtb_filepath</code> indicate the paths of the kernel and device tree binary files.</li>
<li><code>kernel_load_paddr</code> and <code>dtb_load_paddr</code> are the physical memory load addresses for the kernel and device tree binaries.</li>
<li><code>entry_point</code> specifies the entry point address of the kernel.</li>
</ul>
<p>The management tool of root Linux is responsible for reading the JSON configuration file and converting it into a C-style structure, which is then passed to hvisor to start non-root zones.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="command-line-tools"><a class="header" href="#command-line-tools">Command Line Tools</a></h1>
<p>Command line tools are management tools affiliated with hvisor, used to create and close other virtual machines on the management virtual machine Root Linux, and are responsible for starting the Virtio daemon, providing Virtio device emulation. The repository address is located at <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a>.</p>
<h2 id="how-to-compile-1"><a class="header" href="#how-to-compile-1">How to Compile</a></h2>
<p>The command line tool currently supports two architectures: arm64 and riscv, and requires a kernel module to be used. Cross-compilation on an x86 host can be used to compile for different architectures.</p>
<ul>
<li>arm64 compilation</li>
</ul>
<p>Execute the following command in the hvisor-tool directory to obtain the command line tool hvisor and kernel module hvisorl.ko for the arm64 architecture.</p>
<pre><code>make all ARCH=arm64 KDIR=xxx
</code></pre>
<p>Where KDIR is the source path of Root Linux, used for the compilation of the kernel module.</p>
<ul>
<li>riscv compilation</li>
</ul>
<p>Compile the command line tool and kernel module for the riscv architecture:</p>
<pre><code>make all ARCH=riscv KDIR=xxx
</code></pre>
<h2 id="managing-virtual-machines"><a class="header" href="#managing-virtual-machines">Managing Virtual Machines</a></h2>
<h3 id="loading-the-kernel-module"><a class="header" href="#loading-the-kernel-module">Loading the Kernel Module</a></h3>
<p>Before using the command line tool, you need to load the kernel module to facilitate interaction between the user-mode program and Hypervisor:</p>
<pre><code>insmod hvisor.ko
</code></pre>
<p>The operation to unload the kernel module is:</p>
<pre><code>rmmod hvisor.ko
</code></pre>
<p>Where hvisor.ko is located in the hvisor-tool/driver directory.</p>
<h3 id="starting-a-virtual-machine"><a class="header" href="#starting-a-virtual-machine">Starting a Virtual Machine</a></h3>
<p>On Root Linux, you can create a virtual machine with id 1 using the following command. This command will load the virtual machine's operating system image file <code>Image</code> into the real physical address <code>xxxa</code>, load the virtual machine's device tree file <code>linux2.dtb</code> into the real physical address <code>xxxb</code>, and start it.</p>
<pre><code>./hvisor zone start --kernel Image,addr=xxxa --dtb linux2.dtb,addr=xxxb --id 1
</code></pre>
<h3 id="shutting-down-a-virtual-machine"><a class="header" href="#shutting-down-a-virtual-machine">Shutting Down a Virtual Machine</a></h3>
<p>Shut down the virtual machine with id 1:</p>
<pre><code>./hvisor zone shutdown -id 1
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="use-of-virtio-devices"><a class="header" href="#use-of-virtio-devices">Use of VirtIO Devices</a></h1>
<p>Currently, hvisor supports three types of Virtio devices: Virtio block, Virtio net, and Virtio Console, presented to virtual machines other than Root Linux via MMIO. The Virtio device source repository is located at <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a>, compiled and used together with the command line tool. After creating Virtio devices through the command line tool, the Virtio device will become a daemon on Root Linux, and its log information will be output to the nohup.out file.</p>
<h2 id="creating-and-starting-virtio-devices"><a class="header" href="#creating-and-starting-virtio-devices">Creating and Starting Virtio Devices</a></h2>
<p>Before creating Virtio devices through the command line, execute <code>insmod hvisor.ko</code> to load the kernel module.</p>
<h3 id="virtio-blk-device"><a class="header" href="#virtio-blk-device">Virtio blk Device</a></h3>
<p>Execute the following example command on the Root Linux console to create a Virtio blk device:</p>
<pre><code class="language-shell">nohup ./hvisor virtio start \
	--device blk,addr=0xa003c00,len=0x200,irq=78,zone_id=1,img=rootfs2.ext4 &amp;
</code></pre>
<p>Here <code>--device blk</code> indicates creating a Virtio disk device for use by the virtual machine with id <code>zone_id</code>. This virtual machine will interact with the device through an MMIO region, which starts at <code>addr</code>, with a length of <code>len</code>, the device interrupt number is <code>irq</code>, and the corresponding disk image path is <code>img</code>.</p>
<blockquote>
<p>Virtual machines using Virtio devices need to add information about the Virtio mmio node in the device tree.</p>
</blockquote>
<h3 id="virtio-net-device"><a class="header" href="#virtio-net-device">Virtio net Device</a></h3>
<h4 id="creating-network-topology"><a class="header" href="#creating-network-topology">Creating Network Topology</a></h4>
<p>Before using the Virtio net device, a network topology needs to be created in root Linux so that the Virtio net device can connect to the real network card through the Tap device and bridge device. Execute the following commands in root Linux:</p>
<pre><code class="language-shell">mount -t proc proc /proc
mount -t sysfs sysfs /sys
ip link set eth0 up
dhclient eth0
brctl addbr br0
brctl addif br0 eth0
ifconfig eth0 0
dhclient br0
ip tuntap add dev tap0 mode tap
brctl addif br0 tap0
ip link set dev tap0 up
</code></pre>
<p>This will create a network topology of <code>tap0 device&lt;--&gt;bridge device&lt;--&gt;real network card</code>.</p>
<h4 id="starting-virtio-net"><a class="header" href="#starting-virtio-net">Starting Virtio net</a></h4>
<p>Execute the following example command on the Root Linux console to create a Virtio net device:</p>
<pre><code class="language-shell">nohup ./hvisor virtio start \
	--device net,addr=0xa003600,len=0x200,irq=75,zone_id=1,tap=tap0 &amp;
</code></pre>
<p><code>--device net</code> indicates creating a Virtio network device for use by the virtual machine with id <code>zone_id</code>. This virtual machine will interact with the device through an MMIO region, which starts at <code>addr</code>, with a length of <code>len</code>, the device interrupt number is <code>irq</code>, and it connects to the Tap device named <code>tap</code>.</p>
<h3 id="virtio-console-device"><a class="header" href="#virtio-console-device">Virtio console Device</a></h3>
<p>Execute the following example command on the Root Linux console to create a Virtio console device:</p>
<pre><code class="language-shell">nohup ./hvisor virtio start \
	--device console,addr=0xa003800,len=0x200,irq=76,zone_id=1 &amp;
</code></pre>
<p><code>--device console</code> indicates creating a Virtio console for use by the virtual machine with id <code>zone_id</code>. This virtual machine will interact with the device through an MMIO region, which starts at <code>addr</code>, with a length of <code>len</code>, the device interrupt number is <code>irq</code>.</p>
<p>Execute <code>cat nohup.out | grep "char device"</code>, and you will observe the output <code>char device redirected to /dev/pts/xx</code>. Execute on Root Linux:</p>
<pre><code>screen /dev/pts/xx
</code></pre>
<p>This will enter the virtual console and interact with the virtual machine. Press the shortcut key <code>Ctrl +a d</code> to return to the Root Linux terminal. Execute <code>screen -r [session_id]</code> to re-enter the virtual console.</p>
<h3 id="creating-multiple-virtio-devices"><a class="header" href="#creating-multiple-virtio-devices">Creating Multiple Virtio Devices</a></h3>
<p>Execute the following command to create Virtio blk, net, and console devices simultaneously, all devices are within one daemon process.</p>
<pre><code class="language-shell">nohup ./hvisor virtio start \
	--device blk,addr=0xa003c00,len=0x200,irq=78,zone_id=1,img=rootfs2.ext4 \
	--device net,addr=0xa003600,len=0x200,irq=75,zone_id=1,tap=tap0 \
	--device console,addr=0xa003800,len=0x200,irq=76,zone_id=1 &amp;
</code></pre>
<h2 id="closing-virtio-devices"><a class="header" href="#closing-virtio-devices">Closing Virtio Devices</a></h2>
<p>Execute the following command to close the Virtio daemon and all created devices:</p>
<pre><code>pkill hvisor
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hvisor-overall-architecture"><a class="header" href="#hvisor-overall-architecture">hvisor Overall Architecture</a></h1>
<ul>
<li>
<p>CPU Virtualization</p>
<ul>
<li>Architectural Compatibility: Supports aarch64, riscv64, and loongarch architectures, each with a dedicated CPU virtualization component.</li>
<li>CPU Allocation: Uses static allocation, pre-determining the CPU resources for each virtual machine.</li>
</ul>
</li>
<li>
<p>Memory Virtualization</p>
<ul>
<li>Two-Level Page Table: Utilizes two-level page table technology to optimize the memory virtualization process.</li>
</ul>
</li>
<li>
<p>Interrupt Virtualization</p>
<ul>
<li>Interrupt Controller Virtualization: Supports ARM GIC, RISC-V PLIC, and other architecture-specific interrupt controller virtualizations.</li>
<li>Interrupt Handling: Manages the transmission and processing of interrupt signals.</li>
</ul>
</li>
<li>
<p>I/O Virtualization</p>
<ul>
<li>IOMMU Integration: Supports IOMMU, enhancing the efficiency and security of DMA virtualization.</li>
<li>VirtIO Standard: Adheres to the VirtIO specification, providing high-performance virtual devices.</li>
<li>PCI Virtualization: Implements PCI virtualization, ensuring virtual machines can access physical or virtual I/O devices.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="initialization-process-of-hvisor"><a class="header" href="#initialization-process-of-hvisor">Initialization process of hvisor</a></h1>
<p>Abstract: Introduces the relevant knowledge involved in running hvisor on qemu and the initialization process of hvisor. Starting from the start of qemu, track the entire process, and you will have a general understanding of the initialization process of hvisor after reading this article.</p>
<h2 id="qemu-startup-process"><a class="header" href="#qemu-startup-process">qemu startup process</a></h2>
<p>The startup process of the computer simulated by qemu: After the necessary files are loaded into memory, the PC register is initialized to 0x1000, and a few instructions are executed from here before jumping to 0x80000000 to start executing the bootloader (hvsior arm part uses Uboot), after executing a few instructions, it jumps to the starting address of the kernel that uboot can recognize and executes.</p>
<h3 id="generate-the-executable-file-of-hvisor"><a class="header" href="#generate-the-executable-file-of-hvisor">Generate the executable file of hvisor</a></h3>
<pre><code>rust-objcopy --binary-architecture=aarch64 target/aarch64-unknown-none/debug/hvisor --strip-all -O binary target/aarch64-unknown-none/debug/hvisor.bin.tmp
</code></pre>
<p>Convert the executable file of hvisor into logical binary and save it as <code>hvisor.bin.tmp</code>.</p>
<h3 id="generate-an-image-file-that-uboot-can-recognize"><a class="header" href="#generate-an-image-file-that-uboot-can-recognize">Generate an image file that uboot can recognize</a></h3>
<p>uboot is a bootloader, its main task is to jump to the first instruction of the hvisor image to start execution, so it is necessary to ensure that the generated hvisor image is recognizable by uboot, here you need to use the <code>mkimage</code> tool.</p>
<pre><code>mkimage -n hvisor_img -A arm64 -O linux -C none -T kernel -a 0x40400000 -e 0x40400000 -d target/aarch64-unknown-none/debug/hvisor.bin.tmp target/aarch64-unknown-none/debug/hvisor.bin
</code></pre>
<ul>
<li><code>-n hvisor_img</code>: Specify the name of the kernel image.</li>
<li><code>-A arm64</code>: Specify the architecture as ARM64.</li>
<li><code>-O linux</code>: Specify the operating system as Linux.</li>
<li><code>-C none</code>: Do not use compression algorithms.</li>
<li><code>-T kernel</code>: Specify the type as kernel.</li>
<li><code>-a 0x40400000</code>: Specify the loading address as <code>0x40400000</code>.</li>
<li><code>-e 0x40400000</code>: Specify the entry address as <code>0x40400000</code>.</li>
<li><code>-d target/aarch64-unknown-none/debug/hvisor.bin.tmp</code>: Specify the input file as the previously generated temporary binary file.</li>
<li>The last parameter is the generated output file name, which is the final kernel image file <code>hvisor.bin</code>.</li>
</ul>
<h2 id="initialization-process"><a class="header" href="#initialization-process">Initialization process</a></h2>
<h3 id="aarch64ld-link-script"><a class="header" href="#aarch64ld-link-script">aarch64.ld link script</a></h3>
<p>To understand how hvisor executes, we first look at the link script <code>aarch64.ld</code>, which gives us a general understanding of the execution process of hvisor.</p>
<pre><code>ENTRY(arch_entry)
BASE_ADDRESS = 0x40400000;
</code></pre>
<p>The first line sets the program entry <code>arch_entry</code>, which can be found in <code>arch/aarch64/entry.rs</code>, introduced later.</p>
<pre><code>.text : {
        *(.text.entry)
        *(.text .text.*)
    }
</code></pre>
<p>We make the <code>.text</code> section the very beginning section, and put the <code>.text.entry</code> containing the first entry instruction at the beginning of the <code>.text</code> section, ensuring that hvisor indeed starts executing from the 0x40400000 location agreed with qemu.</p>
<p>Here we also need to remember something called <code>__core_end</code>, which is the address of the end position of the link script, which can be known during the startup process.</p>
<h3 id="arch_entry"><a class="header" href="#arch_entry">arch_entry</a></h3>
<p>With the above prerequisites, we can step into the first instruction of hvisor, which is <code>arch_entry()</code>.</p>
<pre><code>// src/arch/aarch64/entry.rs

pub unsafe extern "C" fn arch_entry() -&gt; i32 {
    unsafe {
        core::arch::asm!(
            "
            // x0 = dtbaddr
            mov x1, x0
            mrs x0, mpidr_el1
            and x0, x0, #0xff
            ldr x2, =__core_end          // x2 = &amp;__core_end
            mov x3, {per_cpu_size}      // x3 = per_cpu_size
            madd x4, x0, x3, x3       // x4 = cpuid * per_cpu_size + per_cpu_size
            add x5, x2, x4
            mov sp, x5           // sp = &amp;__core_end + (cpuid + 1) * per_cpu_size
            b {rust_main}             // x0 = cpuid, x1 = dtbaddr
            ",
            options(noreturn),
            per_cpu_size=const PER_CPU_SIZE,
            rust_main = sym crate::rust_main,
        );
    }
}
</code></pre>
<p>First, look at the embedded assembly part. The first instruction <code>mov x1,x0</code>, passes the value of the <code>x0</code> register into the <code>x1</code> register, where x0 contains the address of the device tree. qemu simulates an arm architecture computer, which also has various devices such as input/output devices like mice and displays, as well as various storage devices. When we want to get input from the keyboard and output to the display, we need to get input from somewhere or put the output data somewhere. In the computer, we use specific addresses to access these devices. The device tree saves the access addresses of these devices. As the supervisor of all software, the hypervisor naturally needs to know the information of the device tree, so Uboot will put this information in <code>x0</code> before entering the kernel, which is a convention.</p>
<p><code>mrs x0, mpidr_el1</code> is an instruction to access system-level registers, that is, to send the contents of the system register <code>mpidr_el1</code> to <code>x0</code>, and <code>mpidr_el1</code> contains information about which CPU we are currently dealing with (the computer supports multi-core CPUs), and there are many cooperations with the CPU later, so we need to know which CPU is currently in use. This register contains a lot of information about the CPU, and what we currently need is the lower 8 bits, which extract the corresponding CPU id, which is what the sentence <code>and x0, x0, #0xff</code> is doing.</p>
<p><code>ldr x2, = __core_end</code>, we set a symbol <code>__core_end</code> at the end of the link script, as the end address of the entire hvisor program space, and put this address into <code>x2</code>.</p>
<p><code>mov x3,{per_cpu_size}</code> puts the size of each CPU's stack into <code>x3</code>, this <code>{xxx}</code> is to replace the value of <code>xxx</code> with the assembly code, you can see below <code>per_cpu_size=const PER_CPU_SIZE</code> The external variable is renamed as a parameter. Another parameter with <code>sym</code> indicates that what follows is a symbol defined elsewhere.</p>
<p><code>per_cpu_size</code> In this size space, related registers can be saved and restored, including the CPU's stack space.</p>
<p><code>madd x4, x0, x3, x3</code> is a multiply-add instruction, cpu_id * per_cpu_size + per_cpu_size, the result is put into <code>x4</code>, at this time <code>x4</code> contains how much space the current number of CPUs needs. (The sequence starts from 0, so add per_cpu_size one more time).</p>
<p><code>add x5,x2,x4</code> means to add the end address of hvisor to the total space required by the CPU and put it into <code>x5</code>.</p>
<p><code>mov sp,x5</code> is to find the top of the current CPU's stack.</p>
<p><code>b {rust_main}</code> means to jump to <code>rust_main</code> to start execution, which also indicates that this piece of assembly code will not return, corresponding to <code>option(noreturn)</code>.</p>
<h2 id="enter-rust_main"><a class="header" href="#enter-rust_main">Enter rust_main()</a></h2>
<h3 id="fn-rust_maincpuidusize-host_dtbusize"><a class="header" href="#fn-rust_maincpuidusize-host_dtbusize">fn rust_main(cpuid:usize, host_dtb:usize)</a></h3>
<p>Entering <code>rust_main</code> requires two parameters, which are passed through <code>x0</code> and <code>x1</code>. Remember that in the previous entry, our <code>x0</code> stored the cpu_id and <code>x1</code> stored the device tree related information.</p>
<h3 id="install_trap_vector"><a class="header" href="#install_trap_vector">install_trap_vector()</a></h3>
<p>When the processor encounters an exception or interrupt, it needs to jump to the corresponding location for processing. Here, these corresponding jump addresses are set (can be regarded as setting a table) for handling exceptions at the Hypervisor level. Each privilege level has its own corresponding exception vector table, except for EL0, the application privilege level, which must jump to other privilege levels to handle exceptions. The <code>VBAR_ELn</code> register is used to store the base address of the exception vector table under the ELn privilege level.</p>
<pre><code>extern "C" {
    fn _hyp_trap_vector();
}

pub fn install_trap_vector() {
    // Set the trap vector.
    VBAR_EL2.set(_hyp_trap_vector as _)
}

</code></pre>
<p><code>VBAR_EL2.set()</code> sets the address of <code>_hyp_trap_vector()</code> as the base address of the exception vector table for EL2 privilege level.</p>
<p><code>_hyp_trap_vector()</code> This assembly code is constructing the exception vector table.</p>
<p><strong>Simple introduction to the format of the exception vector table</strong></p>
<p>According to the level of the exception and whether the level of handling the exception is the same, it is divided into two categories. If the level remains unchanged, it is divided into two groups according to whether the current level's SP is used. If the exception level changes, it is divided into two groups according to whether the execution mode is 64-bit/32-bit. At this point, the exception vector table is divided into 4 groups. In each group, each table entry represents an entry for handling a certain type of exception.</p>
<h3 id="main-cpu"><a class="header" href="#main-cpu">Main CPU</a></h3>
<pre><code>static MASTER_CPU: AtomicI32 = AtomicI32::new(-1);

let mut is_primary = false;
    if</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="percpu-structure"><a class="header" href="#percpu-structure">PerCPU Structure</a></h1>
<p>In the architecture of hvisor, the PerCpu structure plays a core role, used to implement local state management for each CPU core and to support CPU virtualization. Below is a detailed introduction to the PerCpu structure and related functions:</p>
<h2 id="percpu-structure-definition"><a class="header" href="#percpu-structure-definition">PerCpu Structure Definition</a></h2>
<p>The PerCpu structure is designed as a container for each CPU core to store its specific data and state. Its layout is as follows:</p>
<pre><code>#[repr(C)]
pub struct PerCpu {
    pub id: usize,
    pub cpu_on_entry: usize,
    pub dtb_ipa: usize,
    pub arch_cpu: ArchCpu,
    pub zone: Option&lt;Arc&lt;RwLock&lt;Zone&gt;&gt;&gt;,
    pub ctrl_lock: Mutex&lt;()&gt;,
    pub boot_cpu: bool,
    // percpu stack
}
</code></pre>
<p>The definitions of each field are as follows:</p>
<pre><code>    id: Identifier of the CPU core.
    cpu_on_entry: An address used to track the CPU entry state, initialized to INVALID_ADDRESS, indicating an invalid address.
    dtb_ipa: The physical address of the device tree binary, also initialized to INVALID_ADDRESS.
    arch_cpu: A reference to the ArchCpu type, which contains architecture-specific CPU information and functions.
    zone: An optional Arc&lt;RwLock&lt;Zone&gt;&gt; type, representing the virtual machine (zone) that the current CPU core is running.
    ctrl_lock: A mutex used to control access and synchronize PerCpu data.
    boot_cpu: A boolean value indicating whether it is the boot CPU.
</code></pre>
<h2 id="construction-and-operation-of-percpu"><a class="header" href="#construction-and-operation-of-percpu">Construction and Operation of PerCpu</a></h2>
<pre><code>    PerCpu::new: This function creates and initializes the PerCpu structure. It first calculates the virtual address of the structure, then safely writes the initialization data. For the RISC-V architecture, it also updates the CSR_SSCRATCH register to store the pointer to ArchCpu.
    run_vm: When this method is called, if the current CPU is not the boot CPU, it will first put it in an idle state, then run the virtual machine.
    entered_cpus: Returns the number of CPU cores that have entered the virtual machine running state.
    activate_gpm: Activates the GPM (Guest Page Management) of the associated zone.
</code></pre>
<h2 id="obtaining-percpu-instances"><a class="header" href="#obtaining-percpu-instances">Obtaining PerCpu Instances</a></h2>
<pre><code>    get_cpu_data: Provides a method to obtain PerCpu instances based on CPU ID.
    this_cpu_data: Returns the PerCpu instance of the currently executing CPU.
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu-virtualization-on-aarch64"><a class="header" href="#cpu-virtualization-on-aarch64">CPU Virtualization on AArch64</a></h1>
<h2 id="cpu-boot-mechanism"><a class="header" href="#cpu-boot-mechanism">CPU Boot Mechanism</a></h2>
<p>Under the AArch64 architecture, hvisor uses the <code>psci::cpu_on()</code> function to wake up a specified CPU core, bringing it from an off state to a running state. This function receives the CPU's ID, boot address, and an opaque parameter as input. If an error occurs, such as the CPU already being awake, the function handles the error appropriately to avoid reawakening.</p>
<h2 id="cpu-virtualization-initialization-and-operation"><a class="header" href="#cpu-virtualization-initialization-and-operation">CPU Virtualization Initialization and Operation</a></h2>
<p>The <code>ArchCpu</code> structure encapsulates architecture-specific CPU information and functionality, with its <code>reset()</code> method responsible for setting the CPU to the initial state of virtualization mode. This includes:</p>
<ul>
<li>Setting the ELR_EL2 register to the specified entry point</li>
<li>Configuring the SPSR_EL2 register</li>
<li>Clearing general registers</li>
<li>Resetting virtual machine-related registers</li>
<li><code>activate_vmm()</code>, activating the Virtual Memory Manager (VMM)</li>
</ul>
<p>The <code>activate_vmm()</code> method is used to configure the VTCR_EL2 and HCR_EL2 registers, enabling the virtualization environment.</p>
<p>The <code>run()</code> and <code>idle()</code> methods of <code>ArchCpu</code> are used to start and idle the CPU, respectively. When starting, it activates the zone's GPM (Guest Page Management), resets to the specified entry point and device tree binary (DTB) address, and then jumps to the EL2 entry point through the <code>vmreturn</code> macro. In idle mode, the CPU is reset to a wait state (WFI) and prepares a <code>parking</code> instruction page for use during idle periods.</p>
<h2 id="switching-between-el1-and-el2"><a class="header" href="#switching-between-el1-and-el2">Switching between EL1 and EL2</a></h2>
<p>hvisor uses EL2 as the hypervisor mode and EL1 for the guest OS in the AArch64 architecture. The <code>handle_vmexit</code> macro handles the context switch from EL1 to EL2 (VMEXIT event), saves the user mode register context, calls an external function to handle the exit reason, and then returns to continue executing hypervisor code. The <code>vmreturn</code> function is used to return from EL2 mode to EL1 mode (VMENTRY event), restores the user mode register context, and returns to the guest OS's code segment through the <code>eret</code> instruction.</p>
<h2 id="mmu-configuration-and-enablement"><a class="header" href="#mmu-configuration-and-enablement">MMU Configuration and Enablement</a></h2>
<p>To support virtualization, the <code>enable_mmu()</code> function configures MMU mapping in EL2 mode, including setting the MAIR_EL2, TCR_EL2, and SCTLR_EL2 registers, enabling instruction and data caching capabilities, and ensuring the virtual range covers the entire 48-bit address space.</p>
<p>Through these mechanisms, hvisor achieves efficient CPU virtualization on the AArch64 architecture, allowing multiple independent zones to operate under statically allocated resources while maintaining system stability and performance.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu-virtualization-under-riscv"><a class="header" href="#cpu-virtualization-under-riscv">CPU Virtualization under RISCV</a></h1>
<p>Abstract: Introducing the CPU virtualization work under the RISCV architecture centered around the ArchCpu structure.</p>
<h2 id="two-data-structures-involved"><a class="header" href="#two-data-structures-involved">Two Data Structures Involved</a></h2>
<p>Hvisor supports multiple architectures, and the work required for CPU virtualization varies for each architecture. However, a unified interface should be provided within a system. Therefore, we split the CPU into two data structures: <code>PerCpu</code> and <code>ArchCpu</code>.</p>
<h3 id="percpu"><a class="header" href="#percpu">PerCpu</a></h3>
<p>This is a general description of the CPU, which has already been introduced in the <code>PerCpu</code> documentation.</p>
<h3 id="archcpu"><a class="header" href="#archcpu">ArchCpu</a></h3>
<p><code>ArchCpu</code> is a CPU structure specific to a particular architecture (<strong>RISCV architecture is discussed in this article</strong>). This structure undertakes the specific behavior of the CPU.</p>
<p>In the ARM architecture, there is also a corresponding <code>ArchCpu</code>, which has a slightly different structure from the <code>ArchCpu</code> introduced in this section, but they have the same interface (i.e., both have initialization behaviors).</p>
<p>The fields included are as follows:</p>
<pre><code>pub struct ArchCpu {
    pub x: [usize; 32], //x0~x31
    pub hstatus: usize,
    pub sstatus: usize,
    pub sepc: usize,
    pub stack_top: usize,
    pub cpuid: usize,
    // pub first_cpu: usize,
    pub power_on: bool,
    pub init: bool,
    pub sstc: bool,
}
</code></pre>
<p>Explanation of each field:</p>
<ul>
<li><code>x</code>: Values of general-purpose registers</li>
<li><code>hstatus</code>: Stores the value of the Hypervisor status register</li>
<li><code>sstatus</code>: Stores the Supervisor status register value, managing S-mode state information such as interrupt enable flags</li>
<li><code>sepc</code>: The return address at the end of exception handling</li>
<li><code>stack_top</code>: Stack top of the corresponding CPU stack</li>
<li><code>power_on</code>: Whether this CPU is powered on</li>
<li><code>init</code>: Whether this CPU has been initialized</li>
<li><code>sstc</code>: Whether a timer interrupt has been configured</li>
</ul>
<h2 id="related-methods"><a class="header" href="#related-methods">Related Methods</a></h2>
<p>This part explains the involved methods.</p>
<h3 id="archcpuinit"><a class="header" href="#archcpuinit">ArchCpu::init</a></h3>
<p>This method mainly initializes the CPU, sets the context when first entering the VM, and some CSR initializations.</p>
<pre><code>pub fn init(&amp;mut self, entry: usize, cpu_id: usize, dtb: usize) {
        write_csr!(CSR_SSCRATCH, self as *const _ as usize); //arch cpu pointer
        self.sepc = entry;
        self.hstatus = 1 &lt;&lt; 7 | 2 &lt;&lt; 32; //HSTATUS_SPV | HSTATUS_VSXL_64
        self.sstatus = 1 &lt;&lt; 8 | 1 &lt;&lt; 63 | 3 &lt;&lt; 13 | 3 &lt;&lt; 15; //SPP
        self.stack_top = self.stack_top() as usize;
        self.x[10] = cpu_id; //cpu id
        self.x[11] = dtb; //dtb addr

        set_csr!(CSR_HIDELEG, 1 &lt;&lt; 2 | 1 &lt;&lt; 6 | 1 &lt;&lt; 10); //HIDELEG_VSSI | HIDELEG_VSTI | HIDELEG_VSEI
        set_csr!(CSR_HEDELEG, 1 &lt;&lt; 8 | 1 &lt;&lt; 12 | 1 &lt;&lt; 13 | 1 &lt;&lt; 15); //HEDELEG_ECU | HEDELEG_IPF | HEDELEG_LPF | HEDELEG_SPF
        set_csr!(CSR_HCOUNTEREN, 1 &lt;&lt; 1); //HCOUNTEREN_TM
                                          //In VU-mode, a counter is not readable unless the applicable bits are set in both hcounteren and scounteren.
        set_csr!(CSR_SCOUNTEREN, 1 &lt;&lt; 1);
        write_csr!(CSR_HTIMEDELTA, 0);
        set_csr!(CSR_HENVCFG, 1 &lt;&lt; 63);
        //write_csr!(CSR_VSSTATUS, 1 &lt;&lt; 63 | 3 &lt;&lt; 13 | 3 &lt;&lt; 15); //SSTATUS_SD | SSTATUS_FS_DIRTY | SSTATUS_XS_DIRTY

        // enable all interupts
        set_csr!(CSR_SIE, 1 &lt;&lt; 9 | 1 &lt;&lt; 5 | 1 &lt;&lt; 1); //SEIE STIE SSIE
                                                     // write_csr!(CSR_HIE, 1 &lt;&lt; 12 | 1 &lt;&lt; 10 | 1 &lt;&lt; 6 | 1 &lt;&lt; 2); //SGEIE VSEIE VSTIE VSSIE
        write_csr!(CSR_HIE, 0);
        write_csr!(CSR_VSTVEC, 0);
        write_csr!(CSR_VSSCRATCH, 0);
        write_csr!(CSR_VSEPC, 0);
        write_csr!(CSR_VSCAUSE, 0);
        write_csr!(CSR_VSTVAL, 0);
        write_csr!(CSR_HVIP, 0);
        write_csr!(CSR_VSATP, 0);
    }
</code></pre>
<p><code>write_csr!(CSR_SSCRATCH, self as *const _ as usize)</code> continues the content of the previous method, writing the address of <code>ArchCpu</code> into <code>sscratch</code>. The return address is set as the entry, the <code>SPV</code> field of <code>hstatus</code> is set to 1, representing that when returning to the VM, the VM runs under VS mode (or understood as the VM was running in VS mode before the exception occurred); the <code>VSXL</code> field sets the length of the registers under VS mode. The <code>SPP</code> and other fields of <code>sstatus</code> provide information about which privilege level the CPU was in before the Trap occurred. <code>SPP</code> and <code>SPV</code> fields, used in combination, determine which privilege level should be returned to when executing the <code>sret</code> instruction under HS mode, with the return address set by <code>spec</code>.</p>
<p><code>HIDELEG</code> and <code>CSR_HEDELEG</code> settings delegate certain interrupts to VS mode for handling. <code>HCOUNTEREN</code> and <code>SCOUNTEREN</code> are used to restrict the performance counters that the VM can access, in this case enabling the <code>TM</code> field, allowing access to the <code>time</code> register. <code>HTIMEDELTA</code> is used to adjust the value read from the <code>time</code> register by the VM, returning the sum of <code>HTIMEDELTA</code> and <code>time</code> in VS or VU mode. <code>SIE</code> enables interrupts, and we have enabled all interrupts.</p>
<p>In the code, note the difference between <code>write_csr!</code> and <code>set_csr!</code>; <code>write_csr!</code> uses direct writing, which is an overwrite method, while <code>set_csr!</code> uses the "or" method, setting certain bits.</p>
<h3 id="archcpuidle"><a class="header" href="#archcpuidle">ArchCpu::idle</a></h3>
<p>By executing the wfi instruction, non-primary CPUs are set to a low-power idle state.</p>
<p>Set a special memory page that contains instructions to put the CPU into a low-power waiting state, allowing them to be placed in a low-power waiting state when no tasks are allocated to certain CPUs in the system, until an interrupt occurs.</p>
<pre><code>pub fn idle(&amp;mut self) -&gt; ! {
        extern "C" {
            fn vcpu_arch_entry() -&gt; !;
        }
        assert!(this_cpu_id() == self.cpuid);
        self.init(0, this_cpu_data().id, this_cpu_data().opaque);
        // reset current cpu -&gt; pc = 0x0 (wfi)
        PARKING_MEMORY_SET.call_once(|| {
            let parking_code: [u8; 4] = [0x73, 0x00, 0x50, 0x10]; // 1: wfi; b 1b
            unsafe {
                PARKING_INST_PAGE[..4].copy_from_slice(&amp;parking_code);
            }

            let mut gpm = MemorySet::&lt;Stage2PageTable&gt;::new();
            gpm.insert(MemoryRegion::new_with_offset_mapper(
                0 as GuestPhysAddr,
                unsafe { &amp;PARKING_INST_PAGE as *const _ as HostPhysAddr - PHYS_VIRT_OFFSET },
                PAGE_SIZE,
                MemFlags::READ | MemFlags::WRITE | MemFlags::EXECUTE,
            ))
            .unwrap();
            gpm
        });
        unsafe {
            PARKING_MEMORY_SET.get().unwrap().activate();
            vcpu_arch_entry();
        }
}
</code></pre>
<p>Set the CPU's entry address to 0, and the address 0 will be mapped to the <code>parking page</code>, which has some wfi instruction encodings set. The <code>wfi</code> instruction puts the CPU into a waiting state until an interrupt occurs.</p>
<p>Then enter <code>vcpu_arch_entry</code>, <code>vcpu_arch_entry</code> points to a piece of assembly code, which is to find <code>ArchCpu</code> based on <code>sscratch</code> for context recovery, then execute <code>sret</code>, return to the address set by <code>spec</code> to execute, that is, execute the wfi instruction just set (<strong>not kernel code</strong>), and enter low power mode.</p>
<p>Although some initialization work is also done here, the CPU's initialization flag <code>init</code> is not set to <code>true</code>, so when the CPU is truly awakened and run later, it will be re-initialized (reflected in the run method).</p>
<h3 id="archcpurun"><a class="header" href="#archcpurun">ArchCpu::run</a></h3>
<p>The main content of this method is some initialization, setting the correct CPU execution entry, and modifying the flag that the CPU has been initialized.</p>
<pre><code>pub fn run(&amp;mut self) -&gt; ! {
        extern "C" {
            fn vcpu_arch_entry() -&gt; !;
        }

        assert!(this_cpu_id() == self.cpuid);
        //change power_on
        this_cpu_data().activate</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loongarch-processor-virtualization"><a class="header" href="#loongarch-processor-virtualization">LoongArch Processor Virtualization</a></h1>
<p>The LoongArch instruction set is an independent RISC instruction set released by China's Loongson Zhongke Company in 2020, which includes five modules: the basic instruction set, binary translation extension (LBT), vector extension (LSX), advanced vector extension (LASX), and virtualization extension (LVZ).</p>
<p>This article will briefly introduce the CPU virtualization design of the LoongArch instruction set, with related explanations from the publicly available KVM source code and code comments.</p>
<h2 id="introduction-to-loongarch-registers"><a class="header" href="#introduction-to-loongarch-registers">Introduction to LoongArch Registers</a></h2>
<h3 id="general-register-usage-convention"><a class="header" href="#general-register-usage-convention">General Register Usage Convention</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Alias</th><th>Usage</th><th>Preserved in Call</th></tr></thead><tbody>
<tr><td><code>$r0</code></td><td><code>$zero</code></td><td>Constant 0</td><td>(constant)</td></tr>
<tr><td><code>$r1</code></td><td><code>$ra</code></td><td>Return address</td><td>No</td></tr>
<tr><td><code>$r2</code></td><td><code>$tp</code></td><td>Thread pointer</td><td>(non-assignable)</td></tr>
<tr><td><code>$r3</code></td><td><code>$sp</code></td><td>Stack pointer</td><td>Yes</td></tr>
<tr><td><code>$r4 - $r5</code></td><td><code>$a0 - $a1</code></td><td>Argument/return registers</td><td>No</td></tr>
<tr><td><code>$r6 - $r11</code></td><td><code>$a2 - $a7</code></td><td>Argument registers</td><td>No</td></tr>
<tr><td><code>$r12 - $r20</code></td><td><code>$t0 - $t8</code></td><td>Temporary registers</td><td>No</td></tr>
<tr><td><code>$r21</code></td><td>Reserved</td><td>(non-assignable)</td><td></td></tr>
<tr><td><code>$r22</code></td><td><code>$fp / $s9</code></td><td>Frame pointer / static reg</td><td>Yes</td></tr>
<tr><td><code>$r23 - $r31</code></td><td><code>$s0 - $s8</code></td><td>Static registers</td><td>Yes</td></tr>
</tbody></table>
</div>
<h3 id="floating-point-register-usage-convention"><a class="header" href="#floating-point-register-usage-convention">Floating Point Register Usage Convention</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Alias</th><th>Usage</th><th>Preserved in Call</th></tr></thead><tbody>
<tr><td><code>$f0 - $f1</code></td><td><code>$fa0 - $fa1</code></td><td>Argument/return registers</td><td>No</td></tr>
<tr><td><code>$f2 - $f7</code></td><td><code>$fa2 - $fa7</code></td><td>Argument registers</td><td>No</td></tr>
<tr><td><code>$f8 - $f23</code></td><td><code>$ft0 - $ft15</code></td><td>Temporary registers</td><td>No</td></tr>
<tr><td><code>$f24 - $f31</code></td><td><code>$fs0 - $fs7</code></td><td>Static registers</td><td>Yes</td></tr>
</tbody></table>
</div>
<p>Temporary registers are also known as caller-saved registers. Static registers are also known as callee-saved registers.</p>
<h3 id="csr-registers"><a class="header" href="#csr-registers">CSR Registers</a></h3>
<p><strong>Control and Status Register (CSR)</strong> is a special type of register in the LoongArch architecture used to control the processor's operational state.</p>
<p>For processors that have implemented the LVZ virtualization extension, there is also a set of CSRs for controlling virtualization.</p>
<h3 id="gcsr-register-set"><a class="header" href="#gcsr-register-set">GCSR Register Set</a></h3>
<p>In LoongArch processors that implement virtualization, there is an additional set of <strong>Guest Control and Status Registers (GCSR)</strong>.</p>
<h3 id="entering-guest-mode-process-from-linux-kvm-source-code"><a class="header" href="#entering-guest-mode-process-from-linux-kvm-source-code">Entering Guest Mode Process (from Linux KVM source code)</a></h3>
<ol>
<li><strong><code>switch_to_guest</code></strong>:</li>
<li>Clear the <code>CSR.ECFG.VS</code> field (set to 0, i.e., all exceptions use one entry address)</li>
<li>Read the guest eentry (guest OS interrupt vector address) saved in Hypervisor -&gt; GEENTRY, then write GEENTRY to <code>CSR.EENTRY</code></li>
<li>Read the guest era (guest OS exception return address) saved in Hypervisor -&gt; GPC, then write GPC to <code>CSR.ERA</code></li>
<li>Read <code>CSR.PGDL</code> global page table address, store in Hypervisor</li>
<li>Load guest pgdl from Hypervisor to <code>CSR.PGDL</code></li>
<li>Read out <code>CSR.GSTAT.GID</code> and <code>CSR.GTLBC.TGID</code>, write to <code>CSR.GTLBC</code></li>
<li>Set <code>CSR.PRMD.PIE</code> to 1, enabling global interrupts at the Hypervisor level</li>
<li>Set <code>CSR.GSTAT.PGM</code> to 1, aiming to make the ertn instruction enter guest mode</li>
<li>Hypervisor restores the guest's general-purpose registers (GPRS) saved earlier to the hardware registers (restoring the context)</li>
<li><strong>Execute the <code>ertn</code> instruction, entering guest mode</strong></li>
</ol>
<h2 id="virtualization-related-exceptions"><a class="header" href="#virtualization-related-exceptions">Virtualization-related Exceptions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>code</th><th>subcode</th><th>abbreviation</th><th>description</th></tr></thead><tbody>
<tr><td>22</td><td>-</td><td>GSPR</td><td>Guest-sensitive privilege resource exception, triggered by <code>cpucfg</code>, <code>idle</code>, <code>cacop</code> instructions, and when a virtual machine accesses non-existent GCSR and IOCSR, forcing a trap into Hypervisor for handling (e.g., software emulation)</td></tr>
<tr><td>23</td><td>-</td><td>HVC</td><td>Exception triggered by hvcl supercall instruction</td></tr>
<tr><td>24</td><td>0</td><td>GCM</td><td>Guest GCSR software modification exception</td></tr>
<tr><td>24</td><td>1</td><td>GCHC</td><td>Guest GCSR hardware modification exception</td></tr>
</tbody></table>
</div>
<h3 id="handling-exceptions-in-guest-mode-process-from-linux-kvm-source-code"><a class="header" href="#handling-exceptions-in-guest-mode-process-from-linux-kvm-source-code">Handling Exceptions in Guest Mode Process (from Linux KVM source code)</a></h3>
<ol>
<li>
<p><strong><code>kvm_exc_entry</code></strong>:</p>
</li>
<li>
<p>Hypervisor first saves the guest's general-purpose registers (GPRS), protecting the context.</p>
</li>
<li>
<p>Hypervisor saves <code>CSR.ESTAT</code> -&gt; host ESTAT</p>
</li>
<li>
<p>Hypervisor saves <code>CSR.ERA</code> -&gt; GPC</p>
</li>
<li>
<p>Hypervisor saves <code>CSR.BADV</code> -&gt; host BADV, which records the erroneous virtual address when an address error exception is triggered</p>
</li>
<li>
<p>Hypervisor saves <code>CSR.BADI</code> -&gt; host BADI, which records the opcode of the instruction that triggered the synchronous class exception, excluding interrupts (INT), guest CSR hardware modification exceptions (GCHC), and machine error exceptions (MERR).</p>
</li>
<li>
<p>Read the host ECFG saved by Hypervisor, write to <code>CSR.ECFG</code> (i.e., switch to host's exception configuration)</p>
</li>
<li>
<p>Read the host EENTRY saved by Hypervisor, write to <code>CSR.EENTRY</code></p>
</li>
<li>
<p>Read the host PGD saved by Hypervisor, write to <code>CSR.PGDL</code> (restoring host page table global directory base address, lower half space)</p>
</li>
<li>
<p>Set <code>CSR.GSTAT.PGM</code> off</p>
</li>
<li>
<p>Clear <code>GTLBC.TGID</code> field</p>
</li>
<li>
<p>Restore kvm per CPU registers</p>
<ol>
<li>Involving KVM_ARCH_HTP, KVM_ARCH_HSP, KVM_ARCH_HPERCPU in kvm assembly</li>
</ol>
</li>
<li>
<p><strong>Jump to KVM_ARCH_HANDLE_EXIT position to handle the exception</strong></p>
</li>
<li>
<p>Determine if the recent function ret is &lt;=0</p>
<ol>
<li>If &lt;=0, continue running host</li>
<li>Otherwise, continue running guest, save percpu registers, as it may switch to a different CPU to continue running guest. Save host percpu registers to <code>CSR.KSAVE</code> register</li>
</ol>
</li>
<li>
<p>Jump to <code>switch_to_guest</code></p>
</li>
</ol>
<h2 id="vcpu-context-registers-to-be-saved"><a class="header" href="#vcpu-context-registers-to-be-saved">vCPU Context Registers to be Saved</a></h2>
<p>According to the LoongArch function call standard, the registers to be saved when manually switching CPU function running context are (excluding floating point registers): <code>$s0</code>-<code>$s9</code>, <code>$sp</code>, <code>$ra</code></p>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<p>[1] Loongson Zhongke Technology Co., Ltd. Loongson Architecture ELF psABI Specification. Version 2.01.</p>
<p>[2] Loongson Zhongke Technology Co., Ltd. Loongson Architecture Reference Manual. Volume One: Basic Architecture.</p>
<p>[3] <a href="https://github.com/torvalds/linux/blob/master/arch/loongarch/kvm/switch.S">https://github.com/torvalds/linux/blob/master/arch/loongarch/kvm/switch.S</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h1>
<h2 id="heap-memory-allocation"><a class="header" href="#heap-memory-allocation">Heap Memory Allocation</a></h2>
<h3 id="initializing-the-allocator"><a class="header" href="#initializing-the-allocator">Initializing the Allocator</a></h3>
<p>When using programming languages, we often encounter dynamic memory allocation, such as allocating a block of memory in C using <code>malloc</code> or <code>new</code>, or using <code>Vec</code>, <code>String</code>, etc., in Rust, which are allocated on the heap.</p>
<p>To allocate memory on the heap, we need to do the following:</p>
<ul>
<li>Provide a large block of memory space at initialization</li>
<li>Provide interfaces for allocation and release</li>
<li>Manage free blocks</li>
</ul>
<p>In summary, we need to allocate a large space and set up an allocator to manage this space. We then inform Rust that we now have an allocator, asking it to use it, allowing us to use variables like <code>Vec</code>, <code>String</code> that allocate memory on the heap. This is what the following lines of code do.</p>
<pre><code>use buddy_system_allocator::LockedHeap;

use crate::consts::HV_HEAP_SIZE;

#[cfg_attr(not(test), global_allocator)]
static HEAP_ALLOCATOR: LockedHeap&lt;32&gt; = LockedHeap::&lt;32&gt;::new();

/// Initialize the global heap allocator.
pub fn init() {
    const MACHINE_ALIGN: usize = core::mem::size_of::&lt;usize&gt;();
    const HEAP_BLOCK: usize = HV_HEAP_SIZE / MACHINE_ALIGN;
    static mut HEAP: [usize; HEAP_BLOCK] = [0; HEAP_BLOCK];
    let heap_start = unsafe { HEAP.as_ptr() as usize };
    unsafe {
        HEAP_ALLOCATOR
            .lock()
            .init(heap_start, HEAP_BLOCK * MACHINE_ALIGN);
    }
    info!(
        "Heap allocator initialization finished: {:#x?}",
        heap_start..heap_start + HV_HEAP_SIZE
    );
}
</code></pre>
<p><code>#[cfg_attr(not(test), global_allocator)]</code> is a conditional compilation attribute, setting the <code>HEAP_ALLOCATOR</code> defined in the next line as Rust's global memory allocator when not in a test environment. Now Rust knows we can perform dynamic allocations.</p>
<p><code>HEAP_ALLOCATOR.lock().init(heap_start, HEAP_BLOCK * MACHINE_ALIGN)</code> manages the large space we allocated with the allocator.</p>
<h3 id="testing"><a class="header" href="#testing">Testing</a></h3>
<pre><code>pub fn test() {
    use alloc::boxed::Box;
    use alloc::vec::Vec;
    extern "C" {
        fn sbss();
        fn ebss();
    }
    let bss_range = sbss as usize..ebss as usize;
    let a = Box::new(5);
    assert_eq!(*a, 5);
    assert!(bss_range.contains(&amp;(a.as_ref() as *const _ as usize)));
    drop(a);
    let mut v: Vec&lt;usize&gt; = Vec::new();
    for i in 0..500 {
        v.push(i);
    }
    for (i, val) in v.iter().take(500).enumerate() {
        assert_eq!(*val, i);
    }
    assert!(bss_range.contains(&amp;(v.as_ptr() as usize)));
    drop(v);
    info!("heap_test passed!");
}
</code></pre>
<p>In this test, we use <code>Box</code> and <code>Vec</code> to verify the memory we allocated is within the <code>bss</code> segment.</p>
<p>The large uninitiated global variable we just handed over to the allocator is placed in the <code>bss</code> segment. We only need to test if the addresses of the variables we obtained are within this range.</p>
<h2 id="armv8-memory-management-knowledge"><a class="header" href="#armv8-memory-management-knowledge">Armv8 Memory Management Knowledge</a></h2>
<h3 id="addressing"><a class="header" href="#addressing">Addressing</a></h3>
<p>The address bus is default 48 bits, while the addressing request issued is 64 bits, so based on the high 16 bits, the virtual address can be divided into 2 spaces:</p>
<ul>
<li>High 16 bits as 1: Kernel space</li>
<li>High 16 bits as 0: User space</li>
</ul>
<p>From the perspective of guestVM, when converting virtual addresses to physical addresses, the CPU selects the TTBR register based on the 63rd bit value of the virtual address. TTBR stores the base address of the level-1 page table. If it's user space, select TTBR0; if it's kernel space, select TTBR1.</p>
<h3 id="four-level-page-table-mapping-example-with-4k-page-size"><a class="header" href="#four-level-page-table-mapping-example-with-4k-page-size">Four-Level Page Table Mapping (example with 4K page size)</a></h3>
<p>Besides the high 16 bits used to determine which page table base register to use, the next 36 bits are divided every 9 bits as the index for each level of the page table, with the lower 12 bits as the offset within the page. As shown in the diagram below.</p>
<p><img src="chap04/./img/memory_level4_pagetable.png" alt="Level4_PageTable" /></p>
<h3 id="stage-2-page-table-mechanism"><a class="header" href="#stage-2-page-table-mechanism">Stage-2 Page Table Mechanism</a></h3>
<p>In a virtualized environment, there are two types of address mapping processes:</p>
<ul>
<li>guestVM uses Stage-1 address conversion, using <code>TTBR0_EL1</code> or <code>TTBR1_EL1</code>, to convert the accessed VA to IPA, then through Stage-2 address conversion, using <code>VTTBR0_EL2</code> to convert IPA to PA.</li>
<li>Hypervisor may run its own applications, and the VA to PA conversion of these applications only needs one conversion, using the <code>TTBR0_EL2</code> register.</li>
</ul>
<p><img src="chap04/./img/memory_nested_translation.png" alt="Nested_Address_Translation" /></p>
<h2 id="hvsiors-memory-management"><a class="header" href="#hvsiors-memory-management">hvsior's Memory Management</a></h2>
<h3 id="physical-page-frame-management"><a class="header" href="#physical-page-frame-management">Physical Page Frame Management</a></h3>
<p>Similar to the construction of the heap mentioned above, page frame allocation also requires an allocator, and then we manage the memory used for allocation with the allocator.</p>
<p><strong>Bitmap-based Allocator</strong></p>
<pre><code>use bitmap_allocator::BitAlloc;
type FrameAlloc = bitmap_allocator::BitAlloc1M;

struct FrameAllocator {
    base: PhysAddr,
    inner: FrameAlloc,
}
</code></pre>
<p><code>BitAlloc1M</code> is a bitmap-based allocator, managing page numbers by <strong>providing information on which pages are free and which are occupied</strong>.</p>
<p>Then, the bitmap allocator and the starting address used for page frame allocation are encapsulated into a page frame allocator.</p>
<p>So we see the initialization function as follows:</p>
<pre><code>fn init(&amp;mut self, base: PhysAddr, size: usize) {
        self.base = align_up(base);
        let page_count = align_up(size) / PAGE_SIZE;
        self.inner.insert(0..page_count);
    }
</code></pre>
<p>The starting address of the page frame allocation area and the size of the available space are passed in, calculating the number of page frames available for allocation in this space <code>page_size</code>, and then informing the bitmap allocator of all page frame numbers through the <code>insert</code> function.</p>
<p><strong>Page Frame Structure</strong></p>
<pre><code>pub struct Frame {
    start_paddr: PhysAddr,
    frame_count: usize,
}
</code></pre>
<p>The structure of the page frame includes the starting address of this page frame and the number of page frames corresponding to this frame instance, which may be 0, 1, or more than 1.</p>
<blockquote>
<p>Why are there cases where the frame count is 0?</p>
<p>When hvisor wants to access the content of the page frame through <code>Frame</code>, a temporary instance is needed, which does not involve page frame allocation or recycling, so 0 is used as a flag.</p>
</blockquote>
<blockquote>
<p>Why are there cases where the frame count is more than 1?</p>
<p>In some cases, we are required to allocate continuous memory, and the size exceeds one page, i.e., multiple continuous page frames are allocated.</p>
</blockquote>
<p><strong>Allocation (alloc)</strong></p>
<p>Now we know that the page frame allocator can allocate a number of a free page frame, turning the number into a <code>Frame</code> instance to complete the page frame allocation, as follows for a single page frame allocation:</p>
<pre><code>impl FrameAllocator {
    fn init(&amp;mut self, base: PhysAddr, size: usize) {
        self.base = align_up(base);
        let page_count = align_up(size) / PAGE_SIZE;
        self.inner.insert(0..page_count);
    }
}

impl Frame {
    /// Allocate one physical frame.
    pub fn new() -&gt; HvResult&lt;Self&gt; {
        unsafe {
            FRAME_ALLOCATOR
                .lock()
                .alloc()
                .map(|start_paddr| Self {
                    start_paddr,
                    frame_count: 1,
                })
                .ok_or(hv_err!(ENOMEM))
        }
    }
}
</code></pre>
<p>As seen, the frame allocator helps us allocate a page frame and returns the starting physical address, then creates a <code>Frame</code> instance.</p>
<p><strong>Page Frame Recycling</strong></p>
<p>The <code>Frame</code> structure is tied to the actual physical page, following the RAII design standard, so when a <code>Frame</code> leaves scope, the corresponding memory area also needs to be returned to hvisor. This requires us to implement the <code>Drop Trait</code>'s <code>drop</code> method, as follows:</p>
<pre><code>impl Drop for Frame {
    fn drop(&amp;mut self) {
        unsafe {
            match self.frame_count {
                0 =&gt; {} // Do not deallocate when use Frame::from_paddr()
                1 =&gt; FRAME_ALLOCATOR.lock().dealloc(self.start_paddr),
                _ =&gt; FRAME_ALLOCATOR
                    .lock()
                    .dealloc_contiguous(self.start_paddr, self.frame_count),
            }
        }
    }
}

impl FrameAllocator{
    unsafe fn dealloc(&amp;mut self, target: PhysAddr) {
        trace!("Deallocate frame: {:x}", target);
        self.inner.dealloc((target - self.base) / PAGE_SIZE)
    }
}
</code></pre>
<p>In <code>drop</code>, we can see that page frames with a frame count of 0 do not need to release the corresponding physical pages, and page frames with a frame count greater than 1 indicate continuous allocated page frames, requiring the recycling of more than one physical page.</p>
<h3 id="page-table-related-data-structures"><a class="header" href="#page-table-related-data-structures">Page Table Related Data Structures</a></h3>
<p>With the above knowledge about Armv8 memory management, we know that the process of building page tables is divided into two parts: the page table used by hvisor itself and the Stage-2 conversion page table. We will focus on the Stage-2 page table.</p>
<p>Before that, we need to understand a few data structures that will be used.</p>
<p><strong>Logical Segment MemoryRegion</strong></p>
<p>The description of the logical segment, including the starting address, size, permission flags</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="arm-gicv3-module"><a class="header" href="#arm-gicv3-module">ARM GICv3 Module</a></h1>
<h2 id="1-gicv3-module"><a class="header" href="#1-gicv3-module">1. GICv3 Module</a></h2>
<h3 id="gicv3-initialization-process"><a class="header" href="#gicv3-initialization-process">GICv3 Initialization Process</a></h3>
<p>The GICv3 initialization process in hvisor involves the initialization of the GIC Distributor (GICD) and GIC Redistributor (GICR), as well as the mechanisms for interrupt handling and virtual interrupt injection. Key steps in this process include:</p>
<ul>
<li>SDEI version check: Obtain the version information of the Secure Debug Extensions Interface (SDEI) through smc_arg1!(0xc4000020).</li>
<li>ICCs configuration: Set icc_ctlr_el1 to only provide priority drop functionality, set icc_pmr_el1 to define the interrupt priority mask, and enable Group 1 IRQs.</li>
<li>Clear pending interrupts: Call the gicv3_clear_pending_irqs function to clear all pending interrupts, ensuring the system is in a clean state.</li>
<li>VMCR and HCR configuration: Set ich_vmcr_el2 and ich_hcr_el2 registers to enable the virtualized CPU interface, preparing for virtual interrupt handling.</li>
</ul>
<h3 id="handling-pending-interrupts"><a class="header" href="#handling-pending-interrupts">Handling Pending Interrupts</a></h3>
<ul>
<li>The <code>pending_irq</code> function reads the <code>icc_iar1_el1</code> register, returning the ID of the current interrupt being processed. If the value is greater than or equal to 0x3fe, it is considered an invalid interrupt.</li>
<li>The <code>deactivate_irq</code> function clears the interrupt flag by writing to the <code>icc_eoir1_el1</code> and <code>icc_dir_el1</code> registers, enabling the interrupt.</li>
</ul>
<h3 id="virtual-interrupt-injection"><a class="header" href="#virtual-interrupt-injection">Virtual Interrupt Injection</a></h3>
<ul>
<li>The <code>inject_irq</code> function checks for an available <code>List Register (LR)</code> and writes virtual interrupt information into it. This function distinguishes between hardware interrupts and software-generated interrupts, appropriately setting fields in the LR.</li>
</ul>
<h3 id="gic-data-structure-initialization"><a class="header" href="#gic-data-structure-initialization">GIC Data Structure Initialization</a></h3>
<ul>
<li>GIC is a global Once container used for the lazy initialization of the Gic structure, which includes the base addresses and sizes of GICD and GICR.</li>
<li>The primary_init_early and primary_init_late functions configure the GIC in the early and late initialization phases, enabling interrupts.</li>
</ul>
<h3 id="zone-level-initialization"><a class="header" href="#zone-level-initialization">Zone-Level Initialization</a></h3>
<p>In the Zone structure, the <code>arch_irqchip_reset</code> method is responsible for resetting all interrupts allocated to a specific zone by directly writing to the GICD's ICENABLER and ICACTIVER registers.</p>
<h2 id="2-vgicv3-module"><a class="header" href="#2-vgicv3-module">2. vGICv3 Module</a></h2>
<p>hvisor's VGICv3 (Virtual Generic Interrupt Controller version 3) module provides virtualization support for GICv3 in the ARMv8-A architecture. It controls and coordinates interrupt requests between different zones (virtual machine instances) through MMIO (Memory Mapped I/O) access and interrupt bitmaps management.</p>
<h3 id="mmio-region-registration"><a class="header" href="#mmio-region-registration">MMIO Region Registration</a></h3>
<p>During initialization, the <code>Zone</code> structure's <code>vgicv3_mmio_init</code> method registers the MMIO regions for the GIC Distributor (GICD) and each CPU's GIC Redistributor (GICR). MMIO region registration is completed through the <code>mmio_region_register</code> function, which associates specific processor or interrupt controller addresses with corresponding handler functions <code>vgicv3_dist_handler</code> and <code>vgicv3_redist_handler</code>.</p>
<h3 id="interrupt-bitmap-initialization"><a class="header" href="#interrupt-bitmap-initialization">Interrupt Bitmap Initialization</a></h3>
<p>The <code>Zone</code> structure's <code>irq_bitmap_init</code> method initializes the interrupt bitmap to track which interrupts belong to the current <code>zone</code>. By iterating through the provided list of interrupts, each interrupt is inserted into the bitmap. The <code>insert_irq_to_bitmap</code> function is responsible for mapping specific interrupt numbers to the appropriate positions in the bitmap.</p>
<h3 id="mmio-access-restrictions"><a class="header" href="#mmio-access-restrictions">MMIO Access Restrictions</a></h3>
<p>The <code>restrict_bitmask_access</code> function restricts MMIO access to the <code>GICD</code> registers, ensuring that only interrupts belonging to the current <code>zone</code> can be modified. This function checks whether the access is for the current zone's interrupts and, if so, updates the access mask to allow or restrict specific read/write operations.</p>
<h3 id="vgicv3-mmio-handling"><a class="header" href="#vgicv3-mmio-handling">VGICv3 MMIO Handling</a></h3>
<p>The <code>vgicv3_redist_handler</code> and <code>vgicv3_dist_handler</code> functions handle MMIO access for GICR and GICD, respectively. The <code>vgicv3_redist_handler</code> function handles read and write operations for GICR, checking whether the access is for the current <code>zone</code>'s GICR and allowing access if so; otherwise, the access is ignored. The <code>vgicv3_dist_handler</code> function, depending on the type of GICD register, calls the <code>vgicv3_handle_irq_ops</code> or <code>restrict_bitmask_access</code> functions to appropriately handle interrupt routing and configuration register access.</p>
<p>Through these mechanisms, hvisor effectively manages interrupts across zones, ensuring that each zone can only access and control the interrupts allocated to it, while providing necessary isolation. This allows VGICv3 to work efficiently and securely in a multi-zone environment, supporting complex virtualization scenarios.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="source-of-interruptions"><a class="header" href="#source-of-interruptions">Source of Interruptions</a></h1>
<p>In hvisor, there are three types of interrupts: timer interrupts, software interrupts, and external interrupts.</p>
<p>Timer Interrupt: A timer interrupt is generated when the time register becomes greater than the timecmp register.</p>
<p>Software Interrupt: In a multi-core system, one hart sends an inter-hart interrupt to another hart, which is implemented through an SBI call.</p>
<p>External Interrupt: External devices send interrupt signals to the processor through interrupt lines.</p>
<h1 id="timer-interrupt"><a class="header" href="#timer-interrupt">Timer Interrupt</a></h1>
<p>When a virtual machine needs to trigger a timer interrupt, it traps into hvisor through the ecall instruction.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>        ExceptionType::ECALL_VS =&gt; {
            trace!("ECALL_VS");
            sbi_vs_handler(current_cpu);
            current_cpu.sepc += 4;
        }
        ...
pub fn sbi_vs_handler(current_cpu: &amp;mut ArchCpu) {
    let eid: usize = current_cpu.x[17];
    let fid: usize = current_cpu.x[16];
    let sbi_ret;
    match eid {
        ...
            SBI_EID::SET_TIMER =&gt; {
            sbi_ret = sbi_time_handler(fid, current_cpu);
        }
        ...
    }
}
<span class="boring">}</span></code></pre></pre>
<p>If the sstc extension is not enabled, it is necessary to trap into machine mode through an SBI call, set the mtimecmp register, clear the virtual machine's timer interrupt pending bit, and enable the hvisor's timer interrupt enable bit; if the sstc extension is enabled, stimecmp can be set directly.</p>
<pre><code class="language-rs">pub fn sbi_time_handler(fid: usize, current_cpu: &amp;mut ArchCpu) -&gt; SbiRet {
...
    if current_cpu.sstc {
        write_csr!(CSR_VSTIMECMP, stime);
    } else {
        set_timer(stime);
        unsafe {
            // clear guest timer interrupt pending
            hvip::clear_vstip();
            // enable timer interrupt
            sie::set_stimer();
        }
    }
    return sbi_ret;
}
</code></pre>
<p>When the time register becomes greater than the timecmp register, a timer interrupt is generated.</p>
<p>After the interrupt is triggered, the trap context is saved, and dispatched to the corresponding handler function.</p>
<pre><code class="language-rs">        InterruptType::STI =&gt; {
            unsafe {
                hvip::set_vstip();
                sie::clear_stimer();
            }
        }
</code></pre>
<p>Set the virtual machine's timer interrupt pending bit to 1, inject a timer interrupt into the virtual machine, and clear hvisor's timer interrupt enable bit to complete the interrupt handling.</p>
<h1 id="software-interrupt"><a class="header" href="#software-interrupt">Software Interrupt</a></h1>
<p>When a virtual machine needs to send an IPI, it traps into hvisor through the ecall instruction.</p>
<pre><code class="language-rs">        SBI_EID::SEND_IPI =&gt; {
            ...
            sbi_ret = sbi_call_5(
                eid,
                fid,
                current_cpu.x[10],
                current_cpu.x[11],
                current_cpu.x[12],
                current_cpu.x[13],
                current_cpu.x[14],
            );
        }
</code></pre>
<p>Then, through an SBI call, trap into machine mode to send an IPI to the specified hart by setting the SSIP bit of the mip register to 1 to inject an inter-hart interrupt into hvisor.</p>
<p>After the interrupt is triggered, the trap context is saved, and dispatched to the corresponding handler function.</p>
<pre><code class="language-rs">pub fn handle_ssi(current_cpu: &amp;mut ArchCpu) {
    ...
    clear_csr!(CSR_SIP, 1 &lt;&lt; 1);
    set_csr!(CSR_HVIP, 1 &lt;&lt; 2);
    check_events();
}
</code></pre>
<p>Set the virtual machine's software interrupt pending bit to 1, injecting a software interrupt into the virtual machine. Then determine the type of inter-hart interrupt, wake up or block the CPU, or handle VIRTIO-related interrupt requests.</p>
<h1 id="external-interrupt"><a class="header" href="#external-interrupt">External Interrupt</a></h1>
<h2 id="plic"><a class="header" href="#plic">PLIC</a></h2>
<p>RISC-V implements external interrupt handling through PLIC, which does not support virtualization and does not support MSI.</p>
<p>PLIC architecture diagram</p>
<p>The interrupt process of PLIC is shown in the diagram below.</p>
<p>Interrupt sources send an interrupt signal to PLIC through the interrupt line, and only when the interrupt priority is greater than the threshold, it can pass through the threshold register filter.</p>
<p>Then read the claim register to get the pending highest priority interrupt, then clear the corresponding pending bit. Pass it to the target hart for interrupt handling.</p>
<p>After handling, write the interrupt number to the complete register to receive the next interrupt request.</p>
<h2 id="initialization"><a class="header" href="#initialization">Initialization</a></h2>
<p>The initialization process is similar to AIA.</p>
<h2 id="processing"><a class="header" href="#processing">Processing</a></h2>
<p>When an external interrupt is triggered in the virtual machine, it accesses the vPLIC address space, but since PLIC does not support virtualization, this address space is unmapped. Therefore, a page fault exception is triggered, and it traps into hvisor for handling.</p>
<p>After the exception is triggered, the trap context is saved, and enters the page fault exception handler function.</p>
<pre><code class="language-rs">pub fn guest_page_fault_handler(current_cpu: &amp;mut ArchCpu) {
    ...
    if addr &gt;= host_plic_base &amp;&amp; addr &lt; host_plic_base + PLIC_TOTAL_SIZE {
        let mut inst: u32 = read_csr!(CSR_HTINST) as u32;
        ...
        if let Some(inst) = inst {
            if addr &gt;= host_plic_base + PLIC_GLOBAL_SIZE {
                vplic_hart_emul_handler(current_cpu, addr, inst);
            } else {
                vplic_global_emul_handler(current_cpu, addr, inst);
            }
            current_cpu.sepc += ins_size;
        } 
        ...
    }
}
</code></pre>
<p>Determine whether the address where the page fault occurred is within the PLIC address space, then parse the instruction that caused the exception, and modify the PLIC address space based on the access address and instruction to implement the simulation configuration for vPLIC.</p>
<pre><code class="language-rs">pub fn vplic_hart_emul_handler(current_cpu: &amp;mut ArchCpu, addr: GuestPhysAddr, inst: Instruction) {
    ...
    if offset &gt;= PLIC_GLOBAL_SIZE &amp;&amp; offset &lt; PLIC_TOTAL_SIZE {
        ...
        if index == 0 {
            // threshold
            match inst {
                Instruction::Sw(i) =&gt; {
                    // guest write threshold register to plic core
                    let value = current_cpu.x[i.rs2() as usize] as u32;
                    host_plic.write().set_threshold(context, value);
                }
                _ =&gt; panic!("Unexpected instruction threshold {:?}", inst),
            }
            ...
        }
    }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overall-structure"><a class="header" href="#overall-structure">Overall Structure</a></h1>
<p>AIA mainly includes two parts, the Interrupt Message Service Controller (IMSIC) and the Advanced Platform-Level Interrupt Controller (APLIC), with the overall structure shown in the figure below.</p>
<img src="chap04/subchap02/../img/riscv_aia_struct.jpg"  style="zoom: 50%;" />
<p>Peripherals can choose to send message interrupts or send wired interrupts via line connections.</p>
<p>If peripheral A supports MSI, it only needs to write the specified data to the interrupt file of the designated hart, and then IMSIC will deliver an interrupt to the target processor.</p>
<p>For all devices, they can connect to APLIC via an interrupt line, and APLIC will choose the interrupt delivery mode according to the configuration:</p>
<ul>
<li>Wired interrupt</li>
<li>MSI</li>
</ul>
<p>In hvisor, the interrupt delivery mode is MSI.</p>
<p>After enabling the AIA specification with <code>IRQ=aia</code> in hvisor, the handling of clock interrupts remains consistent, while the handling of software interrupts and external interrupts changes.</p>
<h1 id="external-interrupt-1"><a class="header" href="#external-interrupt-1">External Interrupt</a></h1>
<h2 id="imsic"><a class="header" href="#imsic">IMSIC</a></h2>
<p>In hvisor, a physical CPU corresponds to a virtual CPU, and they both have their own interrupt files.</p>
<img src="chap04/subchap02/../img/riscv_aia_intfile.png"  style="zoom: 80%;" />
<p>Writing to an interrupt file can trigger an external interrupt of the specified privilege level for the specified hart.</p>
<p>Provide a two-stage address mapping table for IMSIC:</p>
<pre><code class="language-rs">        let paddr = 0x2800_0000 as HostPhysAddr;
        let size = PAGE_SIZE;
        self.gpm.insert(MemoryRegion::new_with_offset_mapper(
            paddr as GuestPhysAddr,
            paddr + PAGE_SIZE * 1,
            size,
            MemFlags::READ | MemFlags::WRITE,
        ))?;
        ...
</code></pre>
<h2 id="aplic"><a class="header" href="#aplic">APLIC</a></h2>
<h3 id="structure"><a class="header" href="#structure">Structure</a></h3>
<p>There is only one global APLIC.</p>
<p>When a wired interrupt arrives, it first reaches the root interrupt domain in machine mode (OpenSBI), and then the interrupt is routed to the sub-interrupt domain (hvisor). Hvisor sends the interrupt signal to the virtual machine's corresponding CPU in MSI mode according to the target registers configured in APLIC.</p>
<img src="chap04/subchap02/../img/riscv_aia_aplicdomain.png"  style="zoom: 70%;" />
<p>The APLIC specification manual specifies the byte offsets for various fields of APLIC. Define the APLIC structure as follows, and implement reading and writing of APLIC fields through the following methods:</p>
<pre><code class="language-rs">#[repr(C)]
pub struct Aplic {
    pub base: usize,
    pub size: usize,
}
impl Aplic {
    pub fn new(base: usize, size: usize) -&gt; Self {
        Self {
            base,
            size,
        }
    }
    pub fn read_domaincfg(&amp;self) -&gt; u32{
        let addr = self.base + APLIC_DOMAINCFG_BASE;
        unsafe { core::ptr::read_volatile(addr as *const u32) }
    }
    pub fn set_domaincfg(&amp;self, bigendian: bool, msimode: bool, enabled: bool){
        ...
        let addr = self.base + APLIC_DOMAINCFG_BASE;
        let src = (enabled &lt;&lt; 8) | (msimode &lt;&lt; 2) | bigendian;
        unsafe {
            core::ptr::write_volatile(addr as *mut u32, src);
        }
    }
    ...
}
</code></pre>
<h3 id="initialization-1"><a class="header" href="#initialization-1">Initialization</a></h3>
<p>Initialize APLIC based on the base address and size in the device tree:</p>
<pre><code class="language-rs">pub fn primary_init_early(host_fdt: &amp;Fdt) {
    let aplic_info = host_fdt.find_node("/soc/aplic").unwrap();
    init_aplic(
        aplic_info.reg().unwrap().next().unwrap().starting_address as usize,
        aplic_info.reg().unwrap().next().unwrap().size.unwrap(),
    );
}
pub fn init_aplic(aplic_base: usize, aplic_size: usize) {
    let aplic = Aplic::new(aplic_base, aplic_size);
    APLIC.call_once(|| RwLock::new(aplic));
}
pub static APLIC: Once&lt;RwLock&lt;Aplic&gt;&gt; = Once::new();
pub fn host_aplic&lt;'a&gt;() -&gt; &amp;'a RwLock&lt;Aplic&gt; {
    APLIC.get().expect("Uninitialized hypervisor aplic!")
}
</code></pre>
<p>Since there is only one global APLIC, locking is used to avoid read-write conflicts, and the host_aplic() method is used for access.</p>
<p>When the virtual machine starts, the address space accessing APLIC is initialized, which is unmapped. Therefore, a page fault is triggered, falling into hvisor for handling:</p>
<pre><code class="language-rs">pub fn guest_page_fault_handler(current_cpu: &amp;mut ArchCpu) {
    ...
    if addr &gt;= host_aplic_base &amp;&amp; addr &lt; host_aplic_base + host_aplic_size {
        let mut inst: u32 = read_csr!(CSR_HTINST) as u32;
        ...
        if let Some(inst) = inst {
                vaplic_emul_handler(current_cpu, addr, inst);
                current_cpu.sepc += ins_size;
            }
        ...
    }
}
</code></pre>
<p>Determine if the accessed address space belongs to APLIC, parse the access instruction, and enter vaplic_emul_handler to simulate APLIC in the virtual machine.</p>
<pre><code class="language-rs">pub fn vaplic_emul_handler(
    current_cpu: &amp;mut ArchCpu,
    addr: GuestPhysAddr,
    inst: Instruction,
) {
    let host_aplic = host_aplic();
    let offset = addr.wrapping_sub(host_aplic.read().base);
    if offset &gt;= APLIC_DOMAINCFG_BASE &amp;&amp; offset &lt; APLIC_SOURCECFG_BASE {
        match inst {
            Instruction::Sw(i) =&gt; {
                ...
                host_aplic.write().set_domaincfg(bigendian, msimode, enabled);
            }
            Instruction::Lw(i) =&gt; {
                let value = host_aplic.read().read_domaincfg();
                current_cpu.x[i.rd() as usize] = value as usize;
            }
            _ =&gt; panic!("Unexpected instruction {:?}", inst),
        }
    }
    ...
}
</code></pre>
<h2 id="interrupt-process"><a class="header" href="#interrupt-process">Interrupt Process</a></h2>
<p>After hvisor completes the simulation of APLIC initialization through a page fault, it enters the virtual machine. Taking the interrupt generated by a keyboard press as an example: the interrupt signal first arrives at OpenSBI, then is routed to hvisor, and according to the configuration of the target register, it writes to the virtual interrupt file to trigger the external interrupt of the virtual machine.</p>
<h1 id="software-interrupt-1"><a class="header" href="#software-interrupt-1">Software Interrupt</a></h1>
<p>After enabling the AIA specification, the Linux kernel of the virtual machine sends IPIs via MSI mode, eliminating the need to use the ecall instruction to fall into hvisor.</p>
<img src="chap04/subchap02/../img/riscv_aia_ipi.jpg"  style="zoom:40%;" />
<p>As shown in the figure, in hvisor, writing to the specified hart's interrupt file can trigger an IPI.</p>
<p>In the virtual machine, writing to the specified virtual interrupt file can implement IPIs within the virtual machine without the need for hvisor's simulation support.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loongarch-interrupt-control"><a class="header" href="#loongarch-interrupt-control">LoongArch Interrupt Control</a></h1>
<p>Due to the different designs of interrupt controllers for different Loongson processors/development boards (embedded processors like 2K1000 have their own interrupt controller designs, and the 3-series processors have the 7A1000 and 7A2000 bridge chips responsible for external interrupt control), this article mainly introduces the interrupt controller inside the latest <strong>Loongson 7A2000 bridge chip</strong>[1].</p>
<h2 id="cpu-interrupts"><a class="header" href="#cpu-interrupts">CPU Interrupts</a></h2>
<p>The interrupt configuration of LoongArch is controlled by <code>CSR.ECFG</code>. The interrupts in the Loongson architecture are line interrupts, and each processor core can record 13 line interrupts. These interrupts include: 1 inter-core interrupt (IPI), 1 timer interrupt (TI), 1 performance monitoring counter overflow interrupt (PMI), 8 hardware interrupts (HWI0~HWI7), and 2 software interrupts (SWI0~SWI1). All line interrupts are level interrupts and are active high[3].</p>
<ul>
<li><strong>Inter-core Interrupt</strong>: Comes from an external interrupt controller and is recorded at <code>CSR.ESTAT.IS[12]</code>.</li>
<li><strong>Timer Interrupt</strong>: Originates from an in-core constant frequency timer, triggered when the timer counts down to zero, and is recorded at <code>CSR.ESTAT.IS[11]</code>. It is cleared by writing 1 to the <code>TI</code> bit of the <code>CSR.TICLR</code> register.</li>
<li><strong>Performance Counter Overflow Interrupt</strong>: Comes from an in-core performance counter, triggered when any performance counter with interrupt enable has its 63rd bit set to 1, and is recorded at <code>CSR.ESTAT.IS[10]</code>. It is cleared by resetting the 63rd bit of the performance counter causing the interrupt or by disabling the interrupt enable of that performance counter.</li>
<li><strong>Hardware Interrupts</strong>: Come from an external interrupt controller outside the processor core, 8 hardware interrupts <code>HWI[7:0]</code> are recorded at <code>CSR.ESTAT.IS[9:2]</code>.</li>
<li><strong>Software Interrupts</strong>: Originates from within the processor core, set by writing 1 to <code>CSR.ESTAT.IS[1:0]</code> through software instruction, cleared by writing 0.</li>
</ul>
<p>The index value recorded in the <code>CSR.ESTAT.IS</code> domain is also known as the interrupt number (Int Number). For example, the interrupt number for <code>SWI0</code> is 0, for <code>SWI1</code> is 1, and so on, with <code>IPI</code> being 12.</p>
<h2 id="traditional-io-interrupts"><a class="header" href="#traditional-io-interrupts">Traditional IO Interrupts</a></h2>
<p>The diagram above shows the interrupt system of the 3A series processor + 7A series bridge chip. It shows two types of interrupt processes, the upper part shows the interruption through the interrupt line <code>INTn0</code>, and the lower part shows the interruption through the HT message packet.</p>
<p>Interrupts <code>intX</code> issued by devices (except for PCIe devices operating in MSI mode) are sent to the internal interrupt controller of 7A, routed to the bridge chip pins or converted into HT message packets sent to the 3A's HT controller. The 3A's interrupt controller receives the interrupt through external interrupt pins or HT controller interrupts, and routes it to interrupt a specific processor core[1].</p>
<p>The Loongson 3A5000 chip's <strong>traditional IO interrupts</strong> support 32 interrupt sources, managed in a unified manner as shown below.
Any IO interrupt source can be configured to enable, trigger mode, and the target processor core interrupt pin to be routed. Traditional interrupts do not support cross-chip distribution of interrupts; they can only interrupt processor cores within the same processor chip[2].</p>
<h2 id="extended-io-interrupts"><a class="header" href="#extended-io-interrupts">Extended IO Interrupts</a></h2>
<p>In addition to the compatibility with the original traditional IO interrupt mode, starting with 3A5000, <strong>extended I/O interrupts</strong> are supported, which distribute the 256-bit interrupts on the HT bus directly to each processor core, without forwarding through the HT interrupt line, enhancing the flexibility of IO interrupt usage[2].</p>
<h2 id="references-3"><a class="header" href="#references-3">References</a></h2>
<p>[1] Loongson Technology Corporation Limited. Loongson 7A2000 Bridge Chip User Manual. V1.0. Chapter 5.</p>
<p>[2] Loongson Technology Corporation Limited. Loongson 3A5000/3B5000 Processor Register Usage Manual - Multi-core Processor Architecture, Register Description and System Software Programming Guide. V1.3. Chapter 11.</p>
<p>[3] Loongson Technology Corporation Limited. Loongson Architecture Reference Manual. Volume One: Basic Architecture.</p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="arm-smmu-technical-documentation"><a class="header" href="#arm-smmu-technical-documentation">ARM-SMMU Technical Documentation</a></h1>
<p>Abstract: Introduction to the development process of ARM-SMMU.</p>
<h2 id="background-knowledge"><a class="header" href="#background-knowledge">Background Knowledge</a></h2>
<p>A brief introduction to the principle and function of SMMU.</p>
<h3 id="what-is-dma-why-do-we-need-iommu"><a class="header" href="#what-is-dma-why-do-we-need-iommu">What is DMA? Why do we need IOMMU?</a></h3>
<p>Virtual machines running on top of the hypervisor need to interact with devices, but if they wait for the CPU to host such tasks every time, it will reduce processing efficiency, hence the emergence of the DMA mechanism. <strong>DMA is a mechanism that allows devices to exchange data directly with memory without CPU involvement.</strong></p>
<p>We can roughly outline the process of virtual machines interacting with devices through DMA. First, the virtual machine issues a DMA request, telling the target device where to write the data, and then the device writes into the memory according to the address.</p>
<p>However, some issues need to be considered in the above process:</p>
<ul>
<li>The hypervisor has virtualized memory for each virtual machine, so the target memory address of the DMA request issued by the virtual machine is GPA, also called IOVA here, which needs to be converted to the real PA to be written to the correct position in physical memory.</li>
<li>Moreover, if the range of IOVA is not restricted, it means that any memory address can be accessed through the DMA mechanism, causing unforeseeable serious consequences.</li>
</ul>
<p>Therefore, we need an institution that can help us with address conversion and ensure the legality of the operation address, just like the MMU memory management unit. This institution is called <strong>IOMMU</strong>, and in the Arm architecture, it has another name called <strong>SMMU</strong> (hereinafter referred to as SMMU).</p>
<p>Now you know that SMMU can convert virtual addresses to physical addresses, thus ensuring the legality of devices directly accessing memory.</p>
<h3 id="specific-work-of-smmu"><a class="header" href="#specific-work-of-smmu">Specific Work of SMMU</a></h3>
<p>As mentioned above, the function of SMMU is similar to MMU, whose target is virtual machines or applications, while SMMU targets each device, each identified by a sid, corresponding to a table called <strong>stream table</strong>. The table uses the device's sid as an index, and the sid of PCI devices can be obtained from the BDF number: sid = (B &lt;&lt; 5) | (D &lt;&lt; 3) | F.</p>
<h2 id="development-work"><a class="header" href="#development-work">Development Work</a></h2>
<p>Currently, we have implemented support for stage-2 address translation of SMMUv3 in Qemu, created a simple linear table, and conducted simple verification using PCI devices.</p>
<p>The work of IOMMU has not yet been merged into the mainline, you can switch to the IOMMU branch to check.</p>
<h3 id="overall-idea"><a class="header" href="#overall-idea">Overall Idea</a></h3>
<p>We pass through PCI HOST to zone0, that is, add a PCI node to the device tree provided to zone0, map the corresponding memory address in the second-stage page table of zone0, and ensure normal interrupt injection. Then zone0 will detect and configure the PCI device by itself, and we only need to do the configuration work of SMMU in the hypervisor.</p>
<h3 id="qemu-parameters"><a class="header" href="#qemu-parameters">Qemu Parameters</a></h3>
<p>Add <code>iommu=smmuv3</code> in <code>machine</code> to enable SMMUv3 support, and add <code>arm-smmuv3.stage=2</code> in <code>global</code> to enable the second-stage address translation.</p>
<p>Note that nested translation is not yet supported in Qemu. If <code>stage=2</code> is not specified, it defaults to supporting only the first-stage address translation. Please use Qemu version 8.1 or above, as lower versions do not support enabling the second-stage address translation.</p>
<p>When adding a PCI device, please enable <code>iommu_platform=on</code>.</p>
<p>addr can specify the bdf number of the device.</p>
<p><strong>In the PCI bus simulated by Qemu, in addition to the PCI HOST, there is a default network card device, so the addr parameter of other added devices must start from 2.0.</strong></p>
<pre><code>// scripts/qemu-aarch64.mk

QEMU_ARGS := -machine virt,secure=on,gic-version=3,virtualization=on,iommu=smmuv3
QEMU_ARGS += -global arm-smmuv3.stage=2

QEMU_ARGS += -device virtio-blk-pci,drive=Xa003e000,disable-legacy=on,disable-modern=off,iommu_platform=on,addr=2.0
</code></pre>
<h3 id="mapping-smmu-related-memory-in-the-hypervisors-page-table"><a class="header" href="#mapping-smmu-related-memory-in-the-hypervisors-page-table">Mapping SMMU Related Memory in the Hypervisor's Page Table</a></h3>
<p>Consulting the source code of Qemu, it is known that the memory region corresponding to VIRT_SMMU starts at 0x09050000 and is 0x20000 in size. We need to access this area, so it must be mapped in the hypervisor's page table.</p>
<pre><code>// src/arch/aarch64/mm.rs

pub fn init_hv_page_table(fdt: &amp;fdt::Fdt) -&gt; HvResult {
    hv_pt.insert(MemoryRegion::new_with_offset_mapper(
        smmuv3_base(),
        smmuv3_base(),
        smmuv3_size(),
        MemFlags::READ | MemFlags::WRITE,
    ))?;
}
</code></pre>
<h3 id="smmuv3-data-structure"><a class="header" href="#smmuv3-data-structure">SMMUv3 Data Structure</a></h3>
<p>This structure contains a reference to the memory region of SMMUv3 that will be accessed, whether it supports a second-level table, the maximum number of bits of sid, and the base address and allocated page frames of the stream table.</p>
<p>The rp is a reference to <code>RegisterPage</code> defined, and <code>RegisterPage</code> is set according to the offsets in Chapter 6 of the SMMUv3 manual. Readers can refer to it themselves.</p>
<pre><code>// src/arch/aarch64/iommu.rs

pub struct Smmuv3{
    rp:&amp;'static RegisterPage,

    strtab_2lvl:bool,
    sid_max_bits:usize,

    frames:Vec&lt;Frame&gt;,

    // strtab
    strtab_base:usize,

    // about queues...
}
</code></pre>
<h3 id="new"><a class="header" href="#new">new()</a></h3>
<p>After completing the mapping work, we can refer to the corresponding register area.</p>
<pre><code>impl Smmuv3{
    fn new() -&gt; Self{
        let rp = unsafe {
            &amp;*(SMMU_BASE_ADDR as *const RegisterPage)
        };

        let mut r = Self{
            ...
        };

        r.check_env();

        r.init_structures();

        r.device_reset();

        r
    }
}
</code></pre>
<h3 id="check_env"><a class="header" href="#check_env">check_env()</a></h3>
<p>Check which stage of address translation the current environment supports, what type of stream table it supports, how many bits of sid it supports, etc.</p>
<p>Taking the check of which table format the environment supports as an example, the supported table type is in the <code>IDR0</code> register, obtained by <code>self.rp.IDR0.get() as usize</code>, and the value of <code>IDR0</code> is obtained by <code>extract_bit</code>, obtaining the value of the <code>ST_LEVEL</code> field. According to the manual, 0b00 represents support for a linear table, 0b01 represents support for a linear table and a second-level table, and 0b1x is a reserved bit. We can choose what type of stream table to create based on this information.</p>
<pre><code>impl Smmuv3{
    fn check_env(&amp;mut self){
        let idr0 = self.rp.IDR0.get() as usize;

        info!("Smmuv3 IDR0:{:b}",idr0);

        // supported types of stream tables.
        let stb_support = extract_bits(idr0, IDR0_ST_LEVEL_OFF, IDR0_ST_LEVEL_LEN);
        match stb_support{
            0 =&gt; info!("Smmuv3 Linear Stream Table Supported."),
            1 =&gt; {info!("Smmuv3 2-level Stream Table Supported.");
                self.strtab_2lvl = true;
            }
            _ =&gt; info!("Smmuv3 don't support any stream table."),
        }

	...
    }
}
</code></pre>
<h3 id="init_linear_strtab"><a class="header" href="#init_linear_strtab">init_linear_strtab()</a></h3>
<p>We need to support the second-stage address translation, and there are not many devices in the system, so we choose to use a linear table.</p>
<p>When applying for the space needed for the linear table, we should determine the number of entries according to the current maximum number of bits of sid, multiplied by the space required for each entry <code>STRTAB_STE_SIZE</code>, and then know how many page frames need to be applied for. However, SMMUv3 has strict requirements for the starting address of the stream table, the low (5+sid_max_bits) bits of the starting address must be 0.</p>
<p>Since the current hypervisor does not yet support such space application, we apply for a space under the premise of ensuring safety, and select an address that meets the conditions within this space as the table base address, although this will cause some space waste.</p>
<p>After applying for the space, we can fill in this table's base address into the <code>STRTAB_BASE</code> register:</p>
<pre><code>	let mut base = extract_bits(self.strtab_base, STRTAB_BASE_OFF, STRTAB_BASE_LEN);
	base = base &lt;&lt; STRTAB_BASE_OFF;
	base |= STRTAB_BASE_RA;
	self.rp.STRTAB_BASE.set(base as _);
</code></pre>
<p>Then we also need to set the <code>STRTAB_BASE_CFG</code> register to indicate the format of the table we are using, whether it is a linear table or a second-level table, and the number of entries (represented in LOG2 form, i.e., the maximum number of bits of SID):</p>
<pre><code>        // format : linear table
        cfg |= STRTAB_BASE_CFG_FMT_LINEAR &lt;&lt; STRTAB_BASE_CFG_FMT_OFF;

        // table size : log2(entries)
        // entry_num = 2^(sid_bits)
        // log2(size) = sid_bits
        cfg |= self.sid_max_bits &lt;&lt; STRTAB_BASE_CFG_LOG2SIZE_OFF;

        // linear table -&gt; ignore SPLIT field
        self.rp.STRTAB_BASE_CFG.set(cfg as _);
</code></pre>
<h3 id="init_bypass"><a class="header" href="#init_bypass">init_bypass</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h3 id="4612-implementation-of-risc-v-iommu-standard"><a class="header" href="#4612-implementation-of-risc-v-iommu-standard">4.6.1.2 Implementation of RISC-V IOMMU Standard</a></h3>
<h4 id="risc-v-iommu-workflow"><a class="header" href="#risc-v-iommu-workflow">RISC-V IOMMU Workflow</a></h4>
<p>For virtualized systems with DMA devices, there is a possibility that the system's stability could be compromised due to malicious DMA configurations by virtual machines. The introduction of IOMMU can further enhance the isolation between Zones, ensuring the system's security.</p>
<p>IOMMU supports two-stage address translation, providing DMA remapping functionality. On one hand, it offers memory protection for DMA operations, limiting the physical memory areas that devices can access, making DMA operations safer. On the other hand, device DMA operations only require continuous IOVA, not continuous PA, allowing efficient use of scattered pages in physical memory.</p>
<p>To perform address translation and memory protection, RISC-V IOMMU uses the same page table format as the CPU's MMU in both the first and second stages. Using the same page table format as the CPU MMU simplifies some complexities in memory management for DMA and allows the CPU MMU and IOMMU to use the same page tables.</p>
<p>The second-stage address translation process supported in hvisor, i.e., the translation from device-side IOVA (GPA) to HPA, is shown in the following diagrams, and the second-stage page tables are shared between the CPU MMU and IOMMU:</p>
<p>IOMMU needs to first locate the device context (DC) in the device directory table using the device identifier (device_id) before translation. Each device has a unique device_id, which is specified during hardware implementation for platform devices, and for PCI/PCIe devices, the BDF number of the PCI/PCIe device is used as the device_id. The DC contains information such as the base address of the two-stage address translation page tables and some translation control information. For example, in two-stage address translation, the I/O device's IOVA is first translated into GPA in the Stage-1 page table pointed to by the fsc field, then into HPA in the Stage-2 page table pointed to by the iohgatp field, and then accesses memory accordingly. In hvisor, only the second-stage translation using the iohgatp field is supported, as shown below:</p>
<p>RISC-V IOMMU, as a physical hardware, can be accessed via MMIO, and its various fields' byte offsets are specified in the IOMMU specification manual. Implementation needs to access these fields correctly according to the specified offsets and sizes. The IommuHw structure is defined to simplify access to the physical IOMMU, as shown below:</p>
<p>The Capabilities of the IOMMU is a read-only register that reports the supported functions of the IOMMU. When initializing the IOMMU, it is necessary to first check this register to determine if the hardware supports IOMMU functions.</p>
<p>During initialization, the IOMMU must first check if the current IOMMU matches the driver. The rv_iommu_check_features function is defined to check for hardware support for features like Sv39x4, WSI, etc., as shown below:</p>
<p>The fctl of the IOMMU is a functional control register, providing some functional controls of the IOMMU, including whether the IOMMU accesses memory data in big-endian or little-endian, whether the interrupts generated by the IOMMU are WSI or MSI interrupts, and controls for the Guest address translation scheme.</p>
<p>The ddtp of the IOMMU is a device directory table pointer register, which contains the root page's PPN of the device directory table and the IOMMU Mode. It can be configured to Off, Bare, 1LVL, 2LVL, or 3LVL, where Off means the IOMMU does not allow device access to memory, Bare means the IOMMU allows all memory access by devices without translation and protection, and 1LVL, 2LVL, 3LVL indicate the number of levels in the device directory table used by the IOMMU.</p>
<p>The rv_iommu_init function is defined for checking and controlling the functions of the physical IOMMU, such as configuring interrupts as WSI, configuring the device directory table, etc., as shown below:</p>
<p>The entry format of the device directory table is provided in the specification manual. To make the hardware work, it needs to be implemented in conjunction with the specification. The DdtEntry structure is defined to represent an entry in the device directory table, representing a DMA device. The iohgatp saves the PPN of the second-stage page table, the Guest Software Context ID (GSCID), and the Mode field used to select the second-stage address translation scheme. The tc contains many transformation control bits, most of which are not used in hvisor, and the valid bits need to be set to 1 for subsequent more advanced feature extensions. The structure of the device directory table entry is as follows:</p>
<p>Currently, hvisor only supports a single-level device directory table. The Lvl1DdtHw structure is defined to facilitate access to the device directory table entries. A single-level device directory table can support 64 DMA devices, occupying one physical page, as shown below:</p>
<p>The Iommu structure is defined as a higher-level abstraction of the IOMMU, where base is the base address of the IommuHw, i.e., the physical address of the IOMMU, which can be used to access the physical IOMMU, and ddt is the device directory table, which needs to be allocated physical pages during IOMMU initialization. Since it supports only a single-level device directory table, only one physical page is needed, as shown below:</p>
<p>The device directory table and translation page tables of the IOMMU are stored in memory and need to be allocated according to actual needs, i.e., the device directory table's memory needs to be allocated during new. In addition, adding device entries to the device directory table is a very important task because DMA devices perform DMA operations, the first step is to find the translation page tables and other information from the device directory table, and then the IOMMU performs the translation based on the page table-related information. The contents such as tc, iohgatp, etc., need to be filled in, as shown below:</p>
<p>Since hvisor supports RISC-V's IOMMU and Arm's SMMUv3, two interfaces for external calls, iommu_init and iommu_add_device, are encapsulated during implementation. These two functions have the same function names and parameters as the common call interface functions under the Arm architecture, as shown below:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio"><a class="header" href="#virtio">Virtio</a></h1>
<h2 id="introduction-to-virtio"><a class="header" href="#introduction-to-virtio">Introduction to Virtio</a></h2>
<p>Virtio was proposed by Rusty Russell in 2008 and is a device virtualization standard aimed at improving device performance and unifying various paravirtual device schemes. Currently, Virtio includes over a dozen peripherals such as disks, network cards, consoles, GPUs, etc., and many operating systems including Linux have implemented frontend drivers for various Virtio devices. Therefore, the virtual machine monitor only needs to implement the Virtio backend device, and it can directly allow virtual machines that have implemented Virtio drivers, such as Linux, to use Virtio devices.</p>
<p>The Virtio protocol defines a set of driver interfaces for paravirtual IO devices, stipulating that the operating system of the virtual machine needs to implement the frontend driver, and the Hypervisor needs to implement the backend device. The virtual machine and Hypervisor communicate and interact through the data plane interface and control plane interface.</p>
<h3 id="data-plane-interface"><a class="header" href="#data-plane-interface">Data Plane Interface</a></h3>
<p>The data plane interface refers to the method of IO data transfer between the driver and the device. For Virtio, the data plane interface refers to a shared memory area between the driver and the device called Virtqueue. Virtqueue is an important data structure in the Virtio protocol and is the mechanism and abstract representation for batch data transfer of Virtio devices, used for various data transfer operations between the driver and the device. Virtqueue consists of three main components: descriptor table, available ring, and used ring, which serve the following purposes:</p>
<ol>
<li>
<p>Descriptor Table: An array of descriptors. Each descriptor contains 4 fields: addr, len, flag, next. Descriptors can represent the address (addr), size (len), and properties (flag) of a memory buffer. The memory buffer can contain commands or data of IO requests (filled by the Virtio driver) or the results of completed IO requests (filled by the Virtio device). Descriptors can be linked into a descriptor chain using the next field as needed, where a descriptor chain represents a complete IO request or result.</p>
</li>
<li>
<p>Available Ring: A circular queue, each element in the queue represents the index of an IO request issued by the Virtio driver in the descriptor table, pointing to the starting descriptor of a descriptor chain.</p>
</li>
<li>
<p>Used Ring: A circular queue, each element in the queue represents the index in the descriptor table where the IO result written by the Virtio device after completing the IO request is located.</p>
</li>
</ol>
<p>Using these three data structures, the commands, data, and results of IO data transfer requests between the driver and the device can be completely described. The Virtio driver program is responsible for allocating the memory area where the Virtqueue is located and writing its address into the corresponding MMIO control registers to inform the Virtio device. After the device obtains the addresses of the three, it can perform IO transfer with the driver through the Virtqueue.</p>
<h3 id="control-plane-interface"><a class="header" href="#control-plane-interface">Control Plane Interface</a></h3>
<p>The control plane interface refers to the way the driver discovers, configures, and manages the device. In the Hypervisor, the control plane interface of Virtio mainly refers to the MMIO registers based on memory mapping. The operating system first detects MMIO-based Virtio devices through the device tree and can negotiate, configure, and notify the device by reading and writing these memory-mapped control registers. Some of the more important registers include:</p>
<ul>
<li>
<p>QueueSel: Used to select the current operating Virtqueue. A device may contain multiple Virtqueues, and the driver indicates which queue it is operating by writing this register.</p>
</li>
<li>
<p>QueueDescLow, QueueDescHigh: Used to indicate the intermediate physical address IPA of the descriptor table. The driver writes these two 32-bit registers to inform the device of the 64-bit physical address of the descriptor table, used to establish shared memory.</p>
</li>
<li>
<p>QueueDriverLow, QueueDriverHigh: Used to indicate the intermediate physical address IPA of the available ring.</p>
</li>
<li>
<p>QueueDeviceLow, QueueDeviceHigh: Used to indicate the intermediate physical address IPA of the used ring.</p>
</li>
<li>
<p>QueueNotify: When the driver writes this register, it indicates that there are new IO requests in the Virtqueue that need to be processed.</p>
</li>
</ul>
<p>In addition to the control registers, each device's MMIO memory area also includes a device configuration space. For disk devices, the configuration space indicates the disk's capacity and block size; for network devices, the configuration space indicates the device's MAC address and connection status. For console devices, the configuration space provides console size information.</p>
<p>For the MMIO memory area where the Virtio device is located, the Hypervisor does not map the second-stage address translation for the virtual machine. When the driver reads and writes this area, a page fault exception will occur, causing a VM Exit into the Hypervisor. The Hypervisor can determine the accessed register based on the address causing the page fault exception and take appropriate actions, such as notifying the device to perform IO operations. After processing, the Hypervisor returns to the virtual machine through VM Entry.</p>
<h3 id="io-process-of-virtio-devices"><a class="header" href="#io-process-of-virtio-devices">IO Process of Virtio Devices</a></h3>
<p>The process from when a user process running on a virtual machine initiates an IO operation to when it obtains the IO result can generally be divided into the following four steps:</p>
<ol>
<li>The user process initiates an IO operation, and the Virtio driver program in the operating system kernel receives the IO operation command, writes it into the Virtqueue, and writes to the QueueNotify register to notify the Virtio device.</li>
<li>After receiving the notification, the device parses the available ring and descriptor table, obtains the specific IO request and buffer address, and performs the actual IO operation.</li>
<li>After completing the IO operation, the device writes the result into the used ring. If the driver program uses the polling method to wait for the IO result, the driver can immediately receive the result information; otherwise, it needs to notify the driver program through an interrupt.</li>
<li>The driver program obtains the IO result from the used ring and returns it to the user process.</li>
</ol>
<h2 id="design-and-implementation-of-the-virtio-backend-mechanism"><a class="header" href="#design-and-implementation-of-the-virtio-backend-mechanism">Design and Implementation of the Virtio Backend Mechanism</a></h2>
<p>The Virtio devices in hvisor follow the <a href="https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.pdf">Virtio v1.2</a> protocol for design and implementation. To maintain good device performance while ensuring the lightweight nature of hvisor, the two design points of the Virtio backend are:</p>
<ol>
<li>
<p>Adopting a microkernel design philosophy, moving the implementation of Virtio devices from the Hypervisor layer to the user space of the management virtual machine. The management virtual machine runs the Linux operating system, referred to as Root Linux. Physical devices such as disks and network cards are passed through to Root Linux, while Virtio devices act as daemons on Root Linux, providing device emulation for other virtual machines (Non Root Linux). This ensures the lightweight nature of the Hypervisor layer and facilitates formal verification.</p>
</li>
<li>
<p>The Virtio drivers located on other virtual machines and the Virtio devices located on Root Linux interact directly through shared memory. The shared memory area, which stores interaction information, is called the communication trampoline and adopts a producer-consumer model, shared by the Virtio device backend and Hypervisor. This reduces the interaction overhead between the driver and the device, enhancing the device's performance.</p>
</li>
</ol>
<p>Based on the above two design points, the implementation of the Virtio backend device will be divided into three parts: communication trampoline, Virtio daemon, and kernel service module:</p>
<h3 id="communication-trampoline"><a class="header" href="#communication-trampoline">Communication Trampoline</a></h3>
<p>To achieve efficient interaction between drivers and devices distributed across different virtual machines, this paper designs a communication trampoline as a bridge for passing control plane interaction information between the driver and the device. It is essentially a shared memory area containing two circular queues: the request submission queue and the request result queue, which store interaction requests issued by the driver and results returned by the device, respectively. Both queues are located in the memory area shared by the Hypervisor and the Virtio daemon and adopt a producer-consumer model. The Hypervisor acts as the producer of the request submission queue and the consumer of the request result queue, while the Virtio daemon acts as the consumer of the request submission queue and the producer of the request result queue. This facilitates the passing of Virtio control plane interaction information between Root Linux and other virtual machines. It is important to note that the request submission queue and the request result queue are different from the Virtqueue. The Virtqueue is the data plane interface between the driver and the device, used for data transfer and essentially containing information about the data buffer's address and structure. In contrast, the communication trampoline is used for control plane interactions and communication between the driver and the device.</p>
<ul>
<li>Communication Trampoline Structure</li>
</ul>
<p>The communication trampoline is represented by the virtio_bridge structure, where req_list is the request submission queue, and res_list and cfg_values together form the request result queue. The device_req structure represents interaction requests sent by the driver to the device, and the device_res structure represents interrupt information injected by the device to notify the virtual machine driver that the IO operation has been completed.</p>
<pre><code class="language-c">// Communication trampoline structure:
struct virtio_bridge {
	__u32 req_front;
	__u32 req_rear;
    __u32 res_front;
    __u32 res_rear;
    // Request submission queue
	struct device_req req_list[MAX_REQ]; 
    // res_list, cfg_flags, and cfg_values together form the request result queue
    struct device_res res_list[MAX_REQ];
	__u64 cfg_flags[MAX_CPUS]; 
	__u64 cfg_values[MAX_CPUS];
	__u64 mmio_addrs[MAX_DEVS];
	__u8 mmio_avail;
	__u8 need_wakeup;
};
// Interaction requests sent by the driver to the device
struct device_req {
	__u64 src_cpu;
	__u64 address; // zone's ipa
	__u64 size;
	__u64 value;
	__u32 src_zone;
	__u8 is_write;
	__u8 need_interrupt;
	__u16 padding;
};
// Interrupt information injected by</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio-block"><a class="header" href="#virtio-block">Virtio Block</a></h1>
<p>The implementation of Virtio disk devices follows the Virtio specification, using MMIO device access method for other virtual machines to discover and use. Currently, it supports five features: <code>VIRTIO_BLK_F_SEG_MAX</code>, <code>VIRTIO_BLK_F_SIZE_MAX</code>, <code>VIRTIO_F_VERSION_1</code>, <code>VIRTIO_RING_F_INDIRECT_DESC</code>, and <code>VIRTIO_RING_F_EVENT_IDX</code>.</p>
<h2 id="top-level-description-of-virtio-device---virtiodevice"><a class="header" href="#top-level-description-of-virtio-device---virtiodevice">Top-level description of Virtio device - VirtIODevice</a></h2>
<p>A Virtio device is represented by the VirtIODevice structure, which contains the device ID, the number of Virtqueues vqs_len, the virtual machine ID it belongs to, the device interrupt number irq_id, the starting address of the MMIO area base_addr, the length of the MMIO area len, the device type, some MMIO registers saved by the device regs, an array of Virtqueues vqs, and a pointer dev pointing to specific device information. With this information, a Virtio device can be fully described.</p>
<pre><code class="language-c">// The highest representations of virtio device
struct VirtIODevice
{
    uint32_t id;
    uint32_t vqs_len;
    uint32_t zone_id;
    uint32_t irq_id;
    uint64_t base_addr; // the virtio device's base addr in non root zone's memory
    uint64_t len;       // mmio region's length
    VirtioDeviceType type;
    VirtMmioRegs regs;
    VirtQueue *vqs;
    // according to device type, blk is BlkDev, net is NetDev, console is ConsoleDev.
    void *dev;          
    bool activated;
};

typedef struct VirtMmioRegs {
    uint32_t device_id;
    uint32_t dev_feature_sel;
    uint32_t drv_feature_sel;
    uint32_t queue_sel;
    uint32_t interrupt_status;
    uint32_t interrupt_ack;
    uint32_t status;
    uint32_t generation;
    uint64_t dev_feature;
    uint64_t drv_feature;
} VirtMmioRegs;
</code></pre>
<h2 id="description-of-virtio-block-device"><a class="header" href="#description-of-virtio-block-device">Description of Virtio Block device</a></h2>
<p>For Virtio disk devices, the type field in VirtIODevice is VirtioTBlock, vqs_len is 1, indicating that there is only one Virtqueue, and the dev pointer points to the virtio_blk_dev structure describing specific information about the disk device. virtio_blk_dev's config represents the device's data capacity and the maximum amount of data in a single data transfer, img_fd is the file descriptor of the disk image opened by the device, tid, mtx, cond are used for the worker thread, procq is the work queue, and closing indicates when the worker thread should close. The definitions of virtio_blk_dev and blkp_req structures are shown in Figure 4.6.</p>
<pre><code class="language-c">typedef struct virtio_blk_dev {
    BlkConfig config;
    int img_fd;
	// describe the worker thread that executes read, write and ioctl.
	pthread_t tid;
	pthread_mutex_t mtx;
	pthread_cond_t cond;
	TAILQ_HEAD(, blkp_req) procq;
	int close;
} BlkDev;

// A request needed to process by blk thread.
struct blkp_req {
	TAILQ_ENTRY(blkp_req) link;
    struct iovec *iov;
	int iovcnt;
	uint64_t offset;
	uint32_t type;
	uint16_t idx;
};
</code></pre>
<h2 id="virtio-block-device-worker-thread"><a class="header" href="#virtio-block-device-worker-thread">Virtio Block device worker thread</a></h2>
<p>Each Virtio disk device has a worker thread and a work queue. The thread ID of the worker thread is saved in the tid field of virtio_blk_dev, and the work queue is procq. The worker thread is responsible for data IO operations and calling the interrupt injection system interface. It is created after the Virtio disk device is started and continuously checks whether there are new tasks in the work queue. If the queue is empty, it waits for the condition variable cond; otherwise, it processes tasks.</p>
<p>When the driver writes to the QueueNotify register in the MMIO area of the disk device, it indicates that there are new IO requests in the available ring. After receiving this request, the Virtio disk device (located in the main thread's execution flow) first reads the available ring to get the first descriptor in the descriptor chain. The first descriptor points to a memory buffer that contains the type of IO request (read/write) and the sector number to be read or written. Subsequent descriptors point to data buffers; for read operations, the read data is stored in these data buffers, and for write operations, the data to be written is obtained from these data buffers. The last descriptor's memory buffer (result buffer) is used to describe the completion result of the IO request, with options including success (OK), failure (IOERR), and unsupported operation (UNSUPP). Based on this, the entire descriptor chain can be parsed to obtain all information about the IO request and save it in the blkp_req structure. The fields in this structure, iov, represent all data buffers, offset represents the data offset of the IO operation, type represents the type of IO operation (read/write), and idx is the index of the first descriptor in the descriptor chain, used to update the used ring. Subsequently, the device adds blkp_req to the work queue procq and wakes up the worker thread blocked on the condition variable cond through the signal function. The worker thread can then process the task.</p>
<p>After the worker thread retrieves the task, it reads and writes the disk image corresponding to img_fd using the preadv and pwritev functions according to the IO operation information indicated by blkp_req. After completing the read/write operation, it first updates the last descriptor in the descriptor chain, which is used to describe the completion result of the IO request, such as success, failure, or unsupported operation. Then it updates the used ring, writing the first descriptor of the descriptor chain to a new entry. Subsequently, it performs interrupt injection to notify other virtual machines.</p>
<p>The establishment of the worker thread effectively distributes time-consuming operations to other CPU cores, improving the efficiency and throughput of the main thread's request distribution and enhancing device performance.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio-network-device"><a class="header" href="#virtio-network-device">Virtio Network Device</a></h1>
<p>Virtio network device is essentially a virtual network card. The currently supported features include <code>VIRTIO_NET_F_MAC</code>, <code>VIRTIO_NET_F_STATUS</code>, <code>VIRTIO_F_VERSION_1</code>, <code>VIRTIO-RING_F_INDIRECT_DESC</code>, <code>VIRTIO_RING_F_EVENT_IDX</code>.</p>
<h2 id="description-of-virtio-network-device"><a class="header" href="#description-of-virtio-network-device">Description of Virtio Network Device</a></h2>
<p>For Virtio network devices, the type field in VirtIODevice is VirtioTNet, vqs_len is 2, indicating that there are 2 Virtqueues, which are the Receive Queue and the Transmit Queue, respectively. The dev pointer points to the virtio_net_dev structure that describes specific information about the network device. In Virtio_net_dev, config is used to represent the MAC address and connection status of the network card, tapfd is the file descriptor of the Tap device corresponding to the device, rx_ready indicates whether the receive queue is available, and the event is used for the receive packet thread to monitor the readable events of the Tap device through epoll.</p>
<pre><code class="language-c">typedef struct virtio_net_dev {
    NetConfig config;
    int tapfd;
    int rx_ready;   
    struct hvisor_event *event;
} NetDev;

struct hvisor_event {
    void		(*handler)(int, int, void *);
    void		*param;
    int			fd;
    int 		epoll_type;
};
</code></pre>
<h2 id="tap-devices-and-bridge-devices"><a class="header" href="#tap-devices-and-bridge-devices">Tap Devices and Bridge Devices</a></h2>
<p>The implementation of Virtio network devices is based on two virtual devices provided by the Linux kernel: Tap devices and bridge devices.</p>
<p>A Tap device is an Ethernet device implemented in software by the Linux kernel, and Ethernet frames can be simulated by reading and writing to the Tap device in user space. Specifically, when a process or kernel performs a write operation on a Tap device, it is equivalent to sending a packet to the Tap device. When a read operation is performed on a Tap device, it is equivalent to receiving a packet from the Tap device. Thus, by reading and writing to the Tap device, the transfer of packets between the kernel and the process can be achieved.</p>
<p>The command to create a tap device is: <code>ip tuntap add dev tap0 mode tap</code>. This command creates a tap device named tap0. If a process wants to use this device, it needs to first open the /dev/net/tun device, obtain a file descriptor tun_fd, and call ioctl(TUNSETIFF) on it to link the process to the tap0 device. Afterward, tun_fd actually becomes the file descriptor of the tap0 device, and it can be read, written, and monitored with epoll.</p>
<p>A bridge device is a virtual device provided by the Linux kernel that functions similarly to a switch. When other network devices are connected to the bridge device, those devices become ports of the bridge device, and the bridge device takes over the packet sending and receiving process of all connected devices. When other devices receive packets, they are sent directly to the bridge device, which forwards them to other ports based on the MAC address. Therefore, all devices connected to the bridge can communicate with each other.</p>
<p>The command to create a bridge device is: <code>brctl addbr br0</code>. The command to connect the physical network card eth0 to br0 is: brctl addif br0 eth0. The command to connect the tap0 device to br0 is: brctl addif br0 tap0.</p>
<p>Before the Virtio network device starts, Root Linux needs to create and start the tap and bridge devices in advance on the command line, and connect the tap device and the physical network card on Root Linux to the bridge device, respectively. Each Virtio network device needs to be connected to a tap device, ultimately forming a network topology diagram as shown below. In this way, the Virtio network device can transmit packets to the external network by reading and writing to the tap device.</p>
<p><img src="chap04/subchap03/VirtIO/./img/hvisor-virtio-net.svg" alt="hvisor-virtio-net" /></p>
<h2 id="sending-packets"><a class="header" href="#sending-packets">Sending Packets</a></h2>
<p>The Transmit Virtqueue of the Virtio network device is used to store the send buffer. When the device receives a request from the driver to write to the QueueNotify register, if the QueueSel register points to the Transmit Queue at this time, it indicates that the driver is informing the device that there are new packets to send. The Virtio-net device will take out a descriptor chain from the available ring, each descriptor chain corresponds to a packet, and the memory buffers pointed to by the descriptor chains are all packet data to be sent. The packet data includes 2 parts, the first part is the packet header virtio_net_hdr_v1 structure specified by the Virtio protocol, which contains some descriptive information about the packet, and the second part is the Ethernet frame. When sending packets, only the Ethernet frame part needs to be written into the Tap device through the writev function. After the Tap device receives the frame, it will forward it to the bridge device, and the bridge device will forward it to the external network through the physical network card based on the MAC address.</p>
<h2 id="receiving-packets"><a class="header" href="#receiving-packets">Receiving Packets</a></h2>
<p>When initializing, the Virtio network device adds the file descriptor of the Tap device to the interest list of the event monitor thread's epoll instance. The event monitor thread will loop and call the epoll_wait function to monitor the readable events of the tap device. Once a readable event occurs, indicating that the tap device has received a packet from the kernel, the epoll_wait function returns, and the packet reception processing function is executed. The processing function will take out a descriptor chain from the available ring of the Receive Virtqueue and read the tap device to write data into the memory buffer pointed to by the descriptor chain, and update the used ring. The processing function will repeat this step until reading the tap device returns a negative value and errno is EWOULDBLOCK, indicating that the tap device has no new packets, and then interrupts notify other virtual machines to receive packets.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio-console"><a class="header" href="#virtio-console">Virtio Console</a></h1>
<p>The Virtio Console device is essentially a virtual console device used for input and output of data and can be used as a virtual terminal for other virtual machines. Currently, hvisor supports the <code>VIRTIO_CONSOLE_F_SIZE</code> and <code>VIRTIO_F_VERSION_1</code> features.</p>
<h2 id="description-of-the-virtio-console-device"><a class="header" href="#description-of-the-virtio-console-device">Description of the Virtio Console Device</a></h2>
<p>For the Virtio console device, the type field in the VirtIODevice structure is VirtioTConsole, vqs_len is 2, indicating that there are two Virtqueues, the receive virtqueue and the transmit virtqueue, used for receiving and sending data for port 0. The dev pointer points to the virtio_console_dev structure that describes the specific information of the console device, in which the config represents the number of rows and columns of the console, master_fd is the file descriptor of the pseudo-terminal master device connected to the device, rx_ready indicates whether the receive queue is available, and event is used for the event monitor thread to monitor the readable events of the pseudo-terminal master device through epoll.</p>
<pre><code class="language-c">typedef struct virtio_console_dev {
    ConsoleConfig config;
    int master_fd;
    int rx_ready;
    struct hvisor_event *event;
} ConsoleDev;
</code></pre>
<h2 id="pseudo-terminal"><a class="header" href="#pseudo-terminal">Pseudo Terminal</a></h2>
<p>A terminal is essentially an input-output device. When computers were first developed, terminals were called Teleprinters (TTY). Now, terminals have become a virtual device on computers, connected by terminal emulation programs to the graphics card driver and keyboard driver to implement data input and output. There are two different implementations of terminal emulation programs: the first is as a Linux kernel module, exposed to user programs as <code>/dev/tty[n]</code>; the second is as an application running in Linux user space, known as a pseudo-terminal (PTY).</p>
<p>Pseudo-terminals themselves are not the focus of this article, but the two devices used by pseudo-terminals that can pass data to each other—the PTY master and the PTY slave—are used in this article to implement the Virtio Console device.</p>
<p>Applications can obtain an available PTY master by executing <code>posix_openpt</code>, and can get the corresponding PTY slave for that PTY master using the <code>ptsname</code> function. A TTY driver connecting the PTY master and PTY slave will copy data between the master and slave. Thus, when a program writes data to the master (or slave), the program can read the same data from the slave (or master).</p>
<h2 id="overall-design-of-the-virtio-console"><a class="header" href="#overall-design-of-the-virtio-console">Overall Design of the Virtio Console</a></h2>
<p>The Virtio Console device acts as a daemon on Root Linux and opens a PTY master during device initialization, outputting the path of the corresponding PTY slave <code>/dev/pts/x</code> to the log file for screen session connections. Meanwhile, the event monitor thread in the Virtio daemon monitors the readable events of the PTY slave so that the PTY master can promptly obtain user input data.</p>
<p>When a user executes <code>screen /dev/pts/x</code> on Root Linux, a screen session is created on the current terminal, connecting to the device corresponding to the PTY slave <code>/dev/pts/x</code>, and taking over the input and output of the current terminal. The implementation structure of the Virtio Console device is shown in the figure below.</p>
<p><img src="chap04/subchap03/VirtIO/./img/virtio_console.svg" alt="virtio_console" /></p>
<h3 id="input-commands"><a class="header" href="#input-commands">Input Commands</a></h3>
<p>When a user types commands on the keyboard, the input characters are passed to the Screen session through the terminal device. The Screen session writes the characters to the PTY slave. When the event monitor thread detects through epoll that the PTY slave is readable, it calls the <code>virtio_console_event_handler</code> function. This function reads from the PTY slave and writes the data into the Virtio Console device's Receive Virtqueue, and sends an interrupt to the corresponding virtual machine.</p>
<p>The corresponding virtual machine, after receiving the interrupt, passes the received character data through the TTY subsystem to the Shell for interpretation and execution.</p>
<h3 id="display-information"><a class="header" href="#display-information">Display Information</a></h3>
<p>When a virtual machine using the Virtio Console driver wants to output information through the Virtio Console device, the Virtio Console driver writes the data to be output into the Transmit Virtqueue and writes to the QueueNotify register in the MMIO area to notify the Virtio Console device to handle the IO operation.</p>
<p>The Virtio Console device reads from the Transmit Virtqueue, retrieves the data to be output, and writes it to the PTY master. The Screen session then retrieves the data to be output from the PTY slave and displays the output information on the monitor through the terminal device.</p>
<blockquote>
<p>Since the PTY master and PTY slave are connected by a TTY driver, which includes a line discipline for passing data written to the PTY slave back to the PTY master, we need to disable this functionality by using the <code>cfmakeraw</code> function to turn off the line discipline feature.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>PCI devices primarily have three spaces: Configuration Space, Memory Space, and I/O Space.</p>
<h3 id="1-configuration-space"><a class="header" href="#1-configuration-space">1. Configuration Space</a></h3>
<ul>
<li><strong>Purpose</strong>: Used for device initialization and configuration.</li>
<li><strong>Size</strong>: Each PCI device has 256 bytes of configuration space.</li>
<li><strong>Access Method</strong>: Accessed through bus number, device number, and function number.</li>
<li><strong>Content</strong>:
<ul>
<li>Device identification information (such as Vendor ID, Device ID).</li>
<li>Status and command registers.</li>
<li>Base Address Registers (BARs), used to map the device's memory space and I/O space.</li>
<li>Information on interrupt lines and interrupt pins.</li>
</ul>
</li>
</ul>
<h3 id="2-memory-space"><a class="header" href="#2-memory-space">2. Memory Space</a></h3>
<ul>
<li><strong>Purpose</strong>: Used to access the device's registers and memory, suitable for high bandwidth access.</li>
<li><strong>Size</strong>: Defined by the device manufacturer, mapped into the system memory address space.</li>
<li><strong>Access Method</strong>: Accessed through memory read and write instructions.</li>
<li><strong>Content</strong>:
<ul>
<li>Device registers: Used for control and status reading.</li>
<li>Device-specific memory: such as frame buffers, DMA buffers, etc.</li>
</ul>
</li>
</ul>
<h3 id="3-io-space"><a class="header" href="#3-io-space">3. I/O Space</a></h3>
<ul>
<li><strong>Purpose</strong>: Used to access the device's control registers, suitable for low bandwidth access.</li>
<li><strong>Size</strong>: Defined by the device manufacturer, mapped into the system's I/O address space.</li>
<li><strong>Access Method</strong>: Accessed through special I/O instructions (such as <code>in</code> and <code>out</code>).</li>
<li><strong>Content</strong>:
<ul>
<li>Device control registers: Used to perform specific I/O operations.</li>
</ul>
</li>
</ul>
<h3 id="summary"><a class="header" href="#summary">Summary</a></h3>
<ul>
<li><strong>Configuration Space</strong> is mainly used for device initialization and configuration.</li>
<li><strong>Memory Space</strong> is used for high-speed access to the device's registers and memory.</li>
<li><strong>I/O Space</strong> is used for low-speed access to the device's control registers.</li>
</ul>
<p>The virtualization of PCI mainly involves managing the above three spaces. Considering that most devices do not have multiple PCI buses, and the ownership of the PCI bus generally belongs to zone0, to ensure the access speed of PCI devices in zone0, hvisor does not process the PCI bus and PCI devices in zone0 when there is no need to allocate devices on this bus to other zones.</p>
<p>When allocating PCI devices to a zone, we need to ensure that Linux in zone0 no longer uses them. As long as the devices are allocated to other zones, zone0 should not access these devices. Unfortunately, we cannot simply use PCI hot-plugging to remove/re-add devices at runtime, as Linux may reprogram the BARs and allocate resources to locations we do not expect or allow. Therefore, a driver within the zone0 kernel is needed to intercept access to these PCI devices, and we turn to the hvisor tool.</p>
<p>The hvisor tool registers itself as a PCI virtual driver and claims management of these devices when other zones use them. Before creating a zone, hvisor allows these devices to unbind from their own drivers and bind to the hvisor tool. When a zone is destroyed, these devices are no longer used by any zone, but from the perspective of zone0, the hvisor tool is still a valid virtual driver, so the release of the devices needs to be done manually. The hvisor tool releases the devices bound to these zones, from the perspective of zone0 Linux, these devices are not bound to any drivers, so if these devices are needed, Linux will automatically rebind the correct drivers.</p>
<p>Now we need to allow the zone to correctly access the PCI devices, and to achieve this goal as simply as possible, we directly reuse the structure of the PCI bus, meaning that the content about the PCI bus will appear in the device tree of the zone that needs to use the devices on this bus, but other than the zone that actually owns this bus, other zones can only access the device through mmio by proxy of hvisor. When a zone attempts to access a PCI device, hvisor checks whether it owns the device, and ownership is declared at the time of zone creation. If a zone accesses the configuration space of a device it owns, hvisor will correctly return the information.</p>
<p>Currently, the treatment of I/O space and memory space is the same as that of configuration space. Because of the uniqueness of BARs resources, the configuration space cannot be directly allocated to a zone, and the frequency of access to BAR space is low, which does not significantly affect efficiency. However, the direct allocation of I/O space and memory space is theoretically feasible, and further, the I/O space and memory space will be directly allocated to the corresponding zone to improve access speed.</p>
<p>To facilitate testing PCI virtualization in QEMU, we wrote a PCI device.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hvisor-management-tool"><a class="header" href="#hvisor-management-tool">hvisor Management Tool</a></h1>
<p>hvisor manages the entire system through a Root Linux managing virtual machines. Root Linux provides services for users to start and shut down virtual machines, and to start and shut down Virtio daemons through a set of management tools. The management tools include a command-line tool and a kernel module. The command-line tool is used to parse and execute commands entered by the user, while the kernel module is used for communication between the command-line tool, Virtio daemon, and Hypervisor. The repository for the management tool is: <a href="https://github.com/syswonder/hvisor-tool">hvisor-tool</a>.</p>
<h2 id="starting-a-virtual-machine-1"><a class="header" href="#starting-a-virtual-machine-1">Starting a Virtual Machine</a></h2>
<p>Users can create a new virtual machine on Root Linux for hvisor by entering the following command:</p>
<pre><code>./hvisor zone start [vm_name].json
</code></pre>
<p>The command-line tool first parses the contents of the <code>[vm_name].json</code> file, writes the virtual machine configuration into the <code>zone_config</code> structure, and reads the contents of the image and dtb files specified in the file into temporary memory through the <code>read</code> function. To load the image and dtb files into the specified physical memory address, the <code>hvisor.ko</code> kernel module provides the <code>hvisor_map</code> function, which can map a physical memory region to the user-space virtual address space.</p>
<p>When the command-line tool executes the <code>mmap</code> function on <code>/dev/hvisor</code>, the kernel calls the <code>hvisor_map</code> function to map the user virtual memory to the specified physical memory. Afterwards, the image and dtb file contents can be moved from temporary memory to the user-specified physical memory area through the memory copy function.</p>
<p>After the image is loaded, the command-line tool calls <code>ioctl</code> on <code>/dev/hvisor</code>, specifying the operation code as <code>HVISOR_ZONE_START</code>. The kernel module then notifies the Hypervisor through a Hypercall and passes the address of the <code>zone_config</code> structure object, informing the Hypervisor to start the virtual machine.</p>
<h2 id="shutting-down-a-virtual-machine-1"><a class="header" href="#shutting-down-a-virtual-machine-1">Shutting Down a Virtual Machine</a></h2>
<p>Users can shut down a virtual machine with the ID <code>vm_id</code> by entering the command:</p>
<pre><code>./hvisor shutdown -id [vm_id]
</code></pre>
<p>This command calls <code>ioctl</code> on <code>/dev/hvisor</code>, specifying the operation code as <code>HVISOR_ZONE_SHUTDOWN</code>. The kernel module then notifies the Hypervisor through a Hypercall, passing <code>vm_id</code>, informing the Hypervisor to shut down the virtual machine.</p>
<h2 id="starting-virtio-daemon"><a class="header" href="#starting-virtio-daemon">Starting Virtio Daemon</a></h2>
<p>Users can start a Virtio device according to the Virtio device information specified in <code>virtio_cfg.json</code> by entering the command:</p>
<pre><code>nohup ./hvisor virtio start [virtio_cfg.json] &amp;
</code></pre>
<p>This command will create Virtio devices and initialize related data structures based on the specified Virtio device information. Currently, three types of Virtio devices can be created, including Virtio-net, Virtio-block, and Virtio-console devices.</p>
<p>Since the command-line parameters include <code>nohup</code> and <code>&amp;</code>, the command will exist in the form of a daemon, with all outputs of the daemon redirected to <code>nohup.out</code>. The daemon's output includes six levels, from low to high: <code>LOG_TRACE</code>, <code>LOG_DEBUG</code>, <code>LOG_INFO</code>, <code>LOG_WARN</code>, <code>LOG_ERROR</code>, <code>LOG_FATAL</code>. When compiling the command-line tool, the LOG level can be specified, for example, when LOG is <code>LOG_INFO</code>, outputs equal to or higher than <code>LOG_INFO</code> will be recorded in the log file, while <code>log_trace</code> and <code>log_debug</code> will not be output.</p>
<p>After the Virtio device is created, the Virtio daemon will poll the request submission queue to fetch Virtio requests from other virtual machines. When there are no requests for a long time, it will automatically enter sleep mode.</p>
<h2 id="shutting-down-virtio-daemon"><a class="header" href="#shutting-down-virtio-daemon">Shutting Down Virtio Daemon</a></h2>
<p>Users can shut down the Virtio daemon by entering the command:</p>
<pre><code>pkill hvisor
</code></pre>
<p>The Virtio daemon registers the signal handling function <code>virtio_close</code> for the <code>SIGTERM</code> signal at startup. When executing <code>pkill hvisor</code>, a <code>SIGTERM</code> signal is sent to the process named <code>hvisor</code>, at which point the daemon will execute <code>virtio_close</code>, reclaim resources, shut down various sub-threads, and finally exit.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hypercall-description"><a class="header" href="#hypercall-description">Hypercall Description</a></h1>
<p>hvisor acts as a Hypervisor, providing a hypercall processing mechanism to the upper layer virtual machines.</p>
<h2 id="how-virtual-machines-execute-hypercalls"><a class="header" href="#how-virtual-machines-execute-hypercalls">How Virtual Machines Execute Hypercalls</a></h2>
<p>Virtual machines execute a specified assembly instruction, <code>hvc</code> for Arm64 and <code>ecall</code> for riscv64. When executing the assembly instruction, the parameters passed are:</p>
<ul>
<li>code: hypercall id, its range and meaning are detailed in hvisor's handling of hypercalls</li>
<li>arg0: the first parameter passed by the virtual machine, type u64</li>
<li>arg1: the second parameter passed by the virtual machine, type u64</li>
</ul>
<p>For example, for riscv linux:</p>
<pre><code class="language-c">#ifdef RISCV64

// according to the riscv sbi spec
// SBI return has the following format:
// struct sbiret
//  {
//  long error;
//  long value;
// };

// a0: error, a1: value
static inline __u64 hvisor_call(__u64 code,__u64 arg0, __u64 arg1) {
	register __u64 a0 asm("a0") = code;
	register __u64 a1 asm("a1") = arg0;
	register __u64 a2 asm("a2") = arg1;
	register __u64 a7 asm("a7") = 0x114514;
	asm volatile ("ecall"
	        : "+r" (a0), "+r" (a1)
			: "r" (a2), "r" (a7)
			: "memory");
	return a1;
}
#endif
</code></pre>
<p>For arm64 linux:</p>
<pre><code class="language-c">#ifdef ARM64
static inline __u64 hvisor_call(__u64 code, __u64 arg0, __u64 arg1) {
	register __u64 x0 asm("x0") = code;
	register __u64 x1 asm("x1") = arg0;
	register __u64 x2 asm("x2") = arg1;

	asm volatile ("hvc #0x4856"
	        : "+r" (x0)
			: "r" (x1), "r" (x2)
			: "memory");
	return x0;
}
#endif /* ARM64 */
</code></pre>
<h2 id="hvisors-handling-of-hypercalls"><a class="header" href="#hvisors-handling-of-hypercalls">hvisor's Handling of Hypercalls</a></h2>
<p>After a virtual machine executes a hypercall, the CPU enters the exception handling function specified by hvisor: <code>hypercall</code>. Then, based on the parameters <code>code</code>, <code>arg0</code>, <code>arg1</code> passed by the hypercall, hvisor continues to call different handling functions, which are:</p>
<div class="table-wrapper"><table><thead><tr><th>code</th><th>Function Called</th><th>Parameter Description</th><th>Function Overview</th></tr></thead><tbody>
<tr><td>0</td><td>hv_virtio_init</td><td>arg0: shared memory start address</td><td>Used for root zone to initialize virtio bootstrap mechanism</td></tr>
<tr><td>1</td><td>hv_virtio_inject_irq</td><td>None</td><td>Used for root zone to send virtio device interrupts to other virtual machines</td></tr>
<tr><td>2</td><td>hv_zone_start</td><td>arg0: virtual machine configuration file address; arg1: configuration file size</td><td>Used for root zone to start a virtual machine</td></tr>
<tr><td>3</td><td>hv_zone_shutdown</td><td>arg0: id of the virtual machine to be shut down</td><td>Used for root zone to shut down a virtual machine</td></tr>
<tr><td>4</td><td>hv_zone_list</td><td>arg0: address of data structure representing virtual machine information; arg1: number of virtual machines</td><td>Used for root zone to view information of all virtual machines in the system</td></tr>
<tr><td>5</td><td>hv_ivc_info</td><td>arg0: starting address of ivc information</td><td>Used for a zone to view its communication domain information</td></tr>
</tbody></table>
</div>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="assets/fzf.umd.js"></script>
        <script src="assets/elasticlunr.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
